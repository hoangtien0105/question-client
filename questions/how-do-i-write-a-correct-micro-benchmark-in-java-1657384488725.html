<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/2eccd4d47c856f2b.css" as="style"/><link rel="stylesheet" href="/_next/static/css/2eccd4d47c856f2b.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-0d1b80a048d4787e.js"></script><script src="/_next/static/chunks/webpack-cb7634a8b6194820.js" defer=""></script><script src="/_next/static/chunks/framework-4556c45dd113b893.js" defer=""></script><script src="/_next/static/chunks/main-25e5079ab4bd6ecd.js" defer=""></script><script src="/_next/static/chunks/pages/_app-20edbe0b078add93.js" defer=""></script><script src="/_next/static/chunks/29107295-fbcfe2172188e46f.js" defer=""></script><script src="/_next/static/chunks/613-1e0aa2b2023820bb.js" defer=""></script><script src="/_next/static/chunks/495-bb1d5b202c02d7f2.js" defer=""></script><script src="/_next/static/chunks/471-84c36aa98dd4107c.js" defer=""></script><script src="/_next/static/chunks/81-301f760ac8107464.js" defer=""></script><script src="/_next/static/chunks/pages/questions/%5Bslug%5D-76d2c3e2d98bb08b.js" defer=""></script><script src="/_next/static/8pZkyd0U8-Y2Qf3QK9j7l/_buildManifest.js" defer=""></script><script src="/_next/static/8pZkyd0U8-Y2Qf3QK9j7l/_ssgManifest.js" defer=""></script><style data-styled="" data-styled-version="5.3.5">.eZIPKL code{padding:5px;color:hsl(210deg 8% 15%);background-color:hsl(210deg 8% 90%);border-radius:3px;}/*!sc*/
data-styled.g5[id="sc-c4b0431a-0"]{content:"eZIPKL,"}/*!sc*/
</style></head><body><div id="__next"><div class="sc-9099c029-0 cIPEih"><header><nav class="bg-white border-gray-200 px-4 lg:px-6 py-2.5 dark:bg-gray-800"><div class="flex flex-wrap justify-between items-center mx-auto max-w-screen-xl"><a class="flex items-center" href="/"><img src="https://flowbite.com/docs/images/logo.svg" class="mr-3 h-6 sm:h-9" alt="Flowbite Logo"/><span class="self-center text-xl font-semibold whitespace-nowrap dark:text-white">Solution Hunter</span></a><div class="flex items-center lg:order-2"><button data-collapse-toggle="mobile-menu-2" type="button" class="inline-flex items-center p-2 ml-1 text-sm text-gray-500 rounded-lg lg:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200 dark:text-gray-400 dark:hover:bg-gray-700 dark:focus:ring-gray-600" aria-controls="mobile-menu-2" aria-expanded="false"><span class="sr-only">Open main menu</span><svg class="w-6 h-6" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 10a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 15a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z" clip-rule="evenodd"></path></svg><svg class="hidden w-6 h-6" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M4.293 4.293a1 1 0 011.414 0L10 8.586l4.293-4.293a1 1 0 111.414 1.414L11.414 10l4.293 4.293a1 1 0 01-1.414 1.414L10 11.414l-4.293 4.293a1 1 0 01-1.414-1.414L8.586 10 4.293 5.707a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></button></div><div class="hidden justify-between items-center w-full lg:flex lg:w-auto lg:order-1" id="mobile-menu-2"><ul class="flex flex-col mt-4 font-medium lg:flex-row lg:space-x-8 lg:mt-0"><li><a class="block py-2 pr-4 pl-3 text-gray-700 border-b border-gray-100 hover:bg-gray-50 lg:hover:bg-transparent lg:border-0 lg:hover:text-blue-700 lg:p-0 dark:text-gray-400 lg:dark:hover:text-white dark:hover:bg-gray-700 dark:hover:text-white lg:dark:hover:bg-transparent dark:border-gray-700" aria-current="page" href="/">Home</a></li><li><a class="block py-2 pr-4 pl-3 text-gray-700 border-b border-gray-100 hover:bg-gray-50 lg:hover:bg-transparent lg:border-0 lg:hover:text-blue-700 lg:p-0 dark:text-gray-400 lg:dark:hover:text-white dark:hover:bg-gray-700 dark:hover:text-white lg:dark:hover:bg-transparent dark:border-gray-700" href="/questions?tab=news">Questions</a></li><li><a class="block py-2 pr-4 pl-3 text-gray-700 border-b border-gray-100 hover:bg-gray-50 lg:hover:bg-transparent lg:border-0 lg:hover:text-blue-700 lg:p-0 dark:text-gray-400 lg:dark:hover:text-white dark:hover:bg-gray-700 dark:hover:text-white lg:dark:hover:bg-transparent dark:border-gray-700" href="/post?tab=news">Post</a></li><li><a class="block py-2 pr-4 pl-3 text-gray-700 border-b border-gray-100 hover:bg-gray-50 lg:hover:bg-transparent lg:border-0 lg:hover:text-blue-700 lg:p-0 dark:text-gray-400 lg:dark:hover:text-white dark:hover:bg-gray-700 dark:hover:text-white lg:dark:hover:bg-transparent dark:border-gray-700" href="/questions/how-do-i-write-a-correct-micro-benchmark-in-java-1657384488725#">Coding</a></li></ul></div></div></nav></header><div class="main-content"><div class="sc-c5440139-0 figLul question my-5"><div class="sc-c4b0431a-0 eZIPKL flex question-header items-center justify-center"><div class="rounded-xl border p-5 shadow-md w-9/12 bg-white"><div class="flex w-full items-center justify-between border-b pb-3"><div class="flex items-center space-x-3"><div class="text-lg font-bold text-slate-700"><a href="/questions/how-do-i-write-a-correct-micro-benchmark-in-java-1657384488725">How do I write a correct micro-benchmark in Java?</a></div></div><div class="flex flex-wrap h-auto justify-end items-center space-x-8"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold" href="/questions/tag/jvm">jvm</a><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold" href="/questions/tag/benchmarking">benchmarking</a><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold" href="/questions/tag/jvm-hotspot">jvm-hotspot</a><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold" href="/questions/tag/microbenchmark">microbenchmark</a></div></div><div class="question-content mt-5">
                
<p>How do you write (and run) a correct micro-benchmark in Java?</p>

<p>I'm looking for some code samples and comments illustrating various things to think about.</p>

<p>Example: Should the benchmark measure time/iteration or iterations/time, and why?</p>

<p>Related: <a href="https://stackoverflow.com/questions/410437/is-stopwatch-benchmarking-acceptable">Is stopwatch benchmarking acceptable?</a></p>
    </div></div></div><div class="sc-c4b0431a-2 cRqwQe"><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 1</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Tips about writing micro benchmarks <a href="https://wiki.openjdk.java.net/display/HotSpot/Main" rel="noreferrer">from the creators of Java HotSpot</a>:</p>

<p><strong>Rule 0:</strong> Read a reputable paper on JVMs and micro-benchmarking. A good one is <a href="http://www.ibm.com/developerworks/java/library/j-jtp02225" rel="noreferrer">Brian Goetz, 2005</a>. Do not expect too much from micro-benchmarks; they measure only a limited range of JVM performance characteristics.</p>

<p><strong>Rule 1:</strong> Always include a warmup phase which runs your test kernel all the way through, enough to trigger all initializations and compilations before timing phase(s). (Fewer iterations is OK on the warmup phase. The rule of thumb is several tens of thousands of inner loop iterations.)</p>

<p><strong>Rule 2:</strong> Always run with <code>-XX:+PrintCompilation</code>, <code>-verbose:gc</code>, etc., so you can verify that the compiler and other parts of the JVM are not doing unexpected work during your timing phase.</p>

<p><strong>Rule 2.1:</strong> Print messages at the beginning and end of timing and warmup phases, so you can verify that there is no output from Rule 2 during the timing phase.</p>

<p><strong>Rule 3:</strong> Be aware of the difference between <code>-client</code> and <code>-server</code>, and OSR and regular compilations. The <code>-XX:+PrintCompilation</code> flag reports OSR compilations with an at-sign to denote the non-initial entry point, for example: <code>Trouble$1::run @ 2 (41 bytes)</code>. Prefer server to client, and regular to OSR, if you are after best performance.</p>

<p><strong>Rule 4:</strong> Be aware of initialization effects. Do not print for the first time during your timing phase, since printing loads and initializes classes. Do not load new classes outside of the warmup phase (or final reporting phase), unless you are testing class loading specifically (and in that case load only the test classes). Rule 2 is your first line of defense against such effects.</p>

<p><strong>Rule 5:</strong> Be aware of deoptimization and recompilation effects. Do not take any code path for the first time in the timing phase, because the compiler may junk and recompile the code, based on an earlier optimistic assumption that the path was not going to be used at all. Rule 2 is your first line of defense against such effects.</p>

<p><strong>Rule 6:</strong> Use appropriate tools to read the compiler's mind, and expect to be surprised by the code it produces. Inspect the code yourself before forming theories about what makes something faster or slower.</p>

<p><strong>Rule 7:</strong> Reduce noise in your measurements. Run your benchmark on a quiet machine, and run it several times, discarding outliers. Use <code>-Xbatch</code> to serialize the compiler with the application, and consider setting <code>-XX:CICompilerCount=1</code> to prevent the compiler from running in parallel with itself. Try your best to reduce GC overhead, set <code>Xmx</code>(large enough) equals <code>Xms</code> and use <a href="http://openjdk.java.net/jeps/318" rel="noreferrer"><code>UseEpsilonGC</code></a> if it is available.</p>

<p><strong>Rule 8:</strong> Use a library for your benchmark as it is probably more efficient and was already debugged for this sole purpose. Such as <a href="http://openjdk.java.net/projects/code-tools/jmh/" rel="noreferrer">JMH</a>, <a href="https://github.com/google/caliper" rel="noreferrer">Caliper</a> or <a href="http://cseweb.ucsd.edu/users/wgg/JavaProf/javaprof.html" rel="noreferrer">Bill and Paul's Excellent UCSD Benchmarks for Java</a>.  </p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 2</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>I know this question has been marked as answered but I wanted to mention two libraries that help us to write micro benchmarks</p>
<p><strong><a href="https://github.com/google/caliper" rel="noreferrer">Caliper from Google</a></strong></p>
<p><em>Getting started tutorials</em></p>
<ol>
<li><a href="http://codingjunkie.net/micro-benchmarking-with-caliper/" rel="noreferrer">http://codingjunkie.net/micro-benchmarking-with-caliper/</a></li>
<li><a href="http://vertexlabs.co.uk/blog/caliper" rel="noreferrer">http://vertexlabs.co.uk/blog/caliper</a></li>
</ol>
<p><strong><a href="http://openjdk.java.net/projects/code-tools/jmh/" rel="noreferrer">JMH from OpenJDK</a></strong></p>
<p><em>Getting started tutorials</em></p>
<ol>
<li><a href="http://www.oracle.com/technetwork/articles/java/architect-benchmarking-2266277.html" rel="noreferrer">Avoiding Benchmarking Pitfalls on the JVM</a></li>
<li><a href="http://nitschinger.at/Using-JMH-for-Java-Microbenchmarking" rel="noreferrer">Using JMH for Java Microbenchmarking</a></li>
<li><a href="https://web.archive.org/web/20181018130828/http://java-performance.info:80/jmh" rel="noreferrer">Introduction to JMH</a></li>
</ol>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 3</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Important things for Java benchmarks are:</p>

<ul>
<li>Warm up the JIT first by running the code several times <strong>before timing</strong> it</li>
<li>Make sure you run it for long enough to be able to measure the results in seconds or (better) tens of seconds</li>
<li>While you can't call <code>System.gc()</code> between iterations, it's a good idea to run it between tests, so that each test will hopefully get a "clean" memory space to work with. (Yes, <code>gc()</code> is more of a hint than a guarantee, but it's very <em>likely</em> that it really will garbage collect in my experience.)</li>
<li>I like to display iterations and time, and a score of time/iteration which can be scaled such that the "best" algorithm gets a score of 1.0 and others are scored in a relative fashion. This means you can run <em>all</em> algorithms for a longish time, varying both number of iterations and time, but still getting comparable results.</li>
</ul>

<p>I'm just in the process of blogging about the design of a benchmarking framework in .NET. I've got a <a href="http://msmvps.com/blogs/jon_skeet/archive/2009/01/26/benchmarking-made-easy.aspx" rel="noreferrer">couple</a> of <a href="http://msmvps.com/blogs/jon_skeet/archive/2009/01/29/for-vs-foreach-on-arrays-and-lists.aspx" rel="noreferrer">earlier posts</a> which may be able to give you some ideas - not everything will be appropriate, of course, but some of it may be.</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 4</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p><a href="http://openjdk.java.net/projects/code-tools/jmh/" rel="nofollow noreferrer">jmh</a> is a recent addition to OpenJDK and has been written by some performance engineers from Oracle. Certainly worth a look.</p>

<blockquote>
  <p>The jmh is a Java harness for building, running, and analysing nano/micro/macro benchmarks written in Java and other languages targetting the JVM.</p>
</blockquote>

<p>Very interesting pieces of information buried in <a href="http://hg.openjdk.java.net/code-tools/jmh/file/tip/jmh-samples/src/main/java/org/openjdk/jmh/samples/" rel="nofollow noreferrer">the sample tests comments</a>.</p>

<p>See also:</p>

<ul>
<li><a href="http://www.oracle.com/technetwork/articles/java/architect-benchmarking-2266277.html" rel="nofollow noreferrer">Avoiding Benchmarking Pitfalls on the JVM</a></li>
<li><a href="https://groups.google.com/forum/#!msg/mechanical-sympathy/m4opvy4xq3U/7lY8x8SvHgwJ" rel="nofollow noreferrer">Discussion on the main strengths of jmh</a>.</li>
</ul>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 5</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<blockquote>
  <p>Should the benchmark measure time/iteration or iterations/time, and why?</p>
</blockquote>

<p>It depends on <strong>what</strong> you are trying to test.  </p>

<p>If you are interested in <strong>latency</strong>, use time/iteration and if you are interested in <strong>throughput</strong>, use iterations/time.</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 6</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Make sure you somehow use results which are computed in benchmarked code. Otherwise your code can be optimized away.</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 7</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>If you are trying to compare two algorithms, do at least two benchmarks for each, alternating the order.  i.e.:</p>

<pre class="lang-java s-code-block"><code class="hljs language-java"><span class="hljs-keyword">for</span>(i=<span class="hljs-number">1.</span>.n)
  alg1();
<span class="hljs-keyword">for</span>(i=<span class="hljs-number">1.</span>.n)
  alg2();
<span class="hljs-keyword">for</span>(i=<span class="hljs-number">1.</span>.n)
  alg2();
<span class="hljs-keyword">for</span>(i=<span class="hljs-number">1.</span>.n)
  alg1();
</code></pre>

<p>I have found some noticeable differences (5-10% sometimes) in the runtime of the same algorithm in different passes..</p>

<p>Also, make sure that <em>n</em> is very large, so that the runtime of each loop is at the very least 10 seconds or so.  The more iterations, the more significant figures in your benchmark time and the more reliable that data is.</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 8</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>There are many possible pitfalls for writing micro-benchmarks in Java.</p>

<p>First: You have to calculate with all sorts of events that take time more or less random: Garbage collection, caching effects (of OS for files and of CPU for memory), IO etc.</p>

<p>Second: You cannot trust the accuracy of the measured times for very short intervals.</p>

<p>Third: The JVM optimizes your code while executing. So different runs in the same JVM-instance will become faster and faster.</p>

<p>My recommendations: Make your benchmark run some seconds, that is more reliable than a runtime over milliseconds. Warm up the JVM (means running the benchmark at least once without measuring, that the JVM can run optimizations). And run your benchmark multiple times (maybe 5 times) and take the median-value. Run every micro-benchmark in a new JVM-instance (call for every benchmark new Java) otherwise optimization effects of the JVM can influence later running tests. Don't execute things, that aren't executed in the warmup-phase (as this could trigger class-load and recompilation).</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 9</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>It should also be noted that it might also be important to analyze the results of the micro benchmark when comparing different implementations. Therefore a <a href="http://en.wikipedia.org/wiki/Significance_testing" rel="noreferrer">significance test</a> should be made.</p>

<p>This is because implementation <code>A</code> might be faster during most of the runs of the benchmark than implementation <code>B</code>. But <code>A</code> might also have a higher spread, so the measured performance benefit of <code>A</code> won't be of any significance when compared with <code>B</code>.</p>

<p>So it is also important to write and run a micro benchmark correctly, but also to analyze it correctly.</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 10</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>To add to the other excellent advice, I'd also be mindful of the following:</p>

<p>For some CPUs (e.g. Intel Core i5 range with TurboBoost), the temperature (and number of cores currently being used, as well as thier utilisation percent) affects the clock speed. Since CPUs are dynamically clocked, this can affect your results. For example, if you have a single-threaded application, the maximum clock speed (with TurboBoost) is higher than for an application using all cores. This can therefore interfere with comparisons of single and multi-threaded performance on some systems. Bear in mind that the temperature and volatages also affect how long Turbo frequency is maintained.</p>

<p>Perhaps a more fundamentally important aspect that you have direct control over: make sure you're measuring the right thing! For example, if you're using <code>System.nanoTime()</code> to benchmark a particular bit of code, put the calls to the assignment in places that make sense to avoid measuring things which you aren't interested in. For example, don't do:</p>

<pre class="lang-java s-code-block"><code class="hljs language-java"><span class="hljs-type">long</span> <span class="hljs-variable">startTime</span> <span class="hljs-operator">=</span> System.nanoTime();
<span class="hljs-comment">//code here...</span>
System.out.println(<span class="hljs-string">"Code took "</span>+(System.nanoTime()-startTime)+<span class="hljs-string">"nano seconds"</span>);
</code></pre>

<p>Problem is you're not immediately getting the end time when the code has finished. Instead, try the following:</p>

<pre class="lang-java s-code-block"><code class="hljs language-java"><span class="hljs-keyword">final</span> <span class="hljs-type">long</span> endTime, startTime = System.nanoTime();
<span class="hljs-comment">//code here...</span>
endTime = System.nanoTime();
System.out.println(<span class="hljs-string">"Code took "</span>+(endTime-startTime)+<span class="hljs-string">"nano seconds"</span>);
</code></pre>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 11</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p><a href="http://opt.sourceforge.net/" rel="noreferrer">http://opt.sourceforge.net/</a> Java Micro Benchmark - control tasks required to determine the comparative performance characteristics of the computer system on different platforms. Can be used to guide optimization decisions and to compare different Java implementations.</p>
    </div></div></div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"data":{"answer":["\n\u0026lt;p\u0026gt;Tips about writing micro benchmarks \u0026lt;a href=\u0026quot;https://wiki.openjdk.java.net/display/HotSpot/Main\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;from the creators of Java HotSpot\u0026lt;/a\u0026gt;:\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Rule 0:\u0026lt;/strong\u0026gt; Read a reputable paper on JVMs and micro-benchmarking. A good one is \u0026lt;a href=\u0026quot;http://www.ibm.com/developerworks/java/library/j-jtp02225\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;Brian Goetz, 2005\u0026lt;/a\u0026gt;. Do not expect too much from micro-benchmarks; they measure only a limited range of JVM performance characteristics.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Rule 1:\u0026lt;/strong\u0026gt; Always include a warmup phase which runs your test kernel all the way through, enough to trigger all initializations and compilations before timing phase(s). (Fewer iterations is OK on the warmup phase. The rule of thumb is several tens of thousands of inner loop iterations.)\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Rule 2:\u0026lt;/strong\u0026gt; Always run with \u0026lt;code\u0026gt;-XX:+PrintCompilation\u0026lt;/code\u0026gt;, \u0026lt;code\u0026gt;-verbose:gc\u0026lt;/code\u0026gt;, etc., so you can verify that the compiler and other parts of the JVM are not doing unexpected work during your timing phase.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Rule 2.1:\u0026lt;/strong\u0026gt; Print messages at the beginning and end of timing and warmup phases, so you can verify that there is no output from Rule 2 during the timing phase.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Rule 3:\u0026lt;/strong\u0026gt; Be aware of the difference between \u0026lt;code\u0026gt;-client\u0026lt;/code\u0026gt; and \u0026lt;code\u0026gt;-server\u0026lt;/code\u0026gt;, and OSR and regular compilations. The \u0026lt;code\u0026gt;-XX:+PrintCompilation\u0026lt;/code\u0026gt; flag reports OSR compilations with an at-sign to denote the non-initial entry point, for example: \u0026lt;code\u0026gt;Trouble$1::run @ 2 (41 bytes)\u0026lt;/code\u0026gt;. Prefer server to client, and regular to OSR, if you are after best performance.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Rule 4:\u0026lt;/strong\u0026gt; Be aware of initialization effects. Do not print for the first time during your timing phase, since printing loads and initializes classes. Do not load new classes outside of the warmup phase (or final reporting phase), unless you are testing class loading specifically (and in that case load only the test classes). Rule 2 is your first line of defense against such effects.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Rule 5:\u0026lt;/strong\u0026gt; Be aware of deoptimization and recompilation effects. Do not take any code path for the first time in the timing phase, because the compiler may junk and recompile the code, based on an earlier optimistic assumption that the path was not going to be used at all. Rule 2 is your first line of defense against such effects.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Rule 6:\u0026lt;/strong\u0026gt; Use appropriate tools to read the compiler\u0026apos;s mind, and expect to be surprised by the code it produces. Inspect the code yourself before forming theories about what makes something faster or slower.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Rule 7:\u0026lt;/strong\u0026gt; Reduce noise in your measurements. Run your benchmark on a quiet machine, and run it several times, discarding outliers. Use \u0026lt;code\u0026gt;-Xbatch\u0026lt;/code\u0026gt; to serialize the compiler with the application, and consider setting \u0026lt;code\u0026gt;-XX:CICompilerCount=1\u0026lt;/code\u0026gt; to prevent the compiler from running in parallel with itself. Try your best to reduce GC overhead, set \u0026lt;code\u0026gt;Xmx\u0026lt;/code\u0026gt;(large enough) equals \u0026lt;code\u0026gt;Xms\u0026lt;/code\u0026gt; and use \u0026lt;a href=\u0026quot;http://openjdk.java.net/jeps/318\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;\u0026lt;code\u0026gt;UseEpsilonGC\u0026lt;/code\u0026gt;\u0026lt;/a\u0026gt; if it is available.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Rule 8:\u0026lt;/strong\u0026gt; Use a library for your benchmark as it is probably more efficient and was already debugged for this sole purpose. Such as \u0026lt;a href=\u0026quot;http://openjdk.java.net/projects/code-tools/jmh/\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;JMH\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026quot;https://github.com/google/caliper\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;Caliper\u0026lt;/a\u0026gt; or \u0026lt;a href=\u0026quot;http://cseweb.ucsd.edu/users/wgg/JavaProf/javaprof.html\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;Bill and Paul\u0026apos;s Excellent UCSD Benchmarks for Java\u0026lt;/a\u0026gt;.  \u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;I know this question has been marked as answered but I wanted to mention two libraries that help us to write micro benchmarks\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;\u0026lt;a href=\u0026quot;https://github.com/google/caliper\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;Caliper from Google\u0026lt;/a\u0026gt;\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Getting started tutorials\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;ol\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;a href=\u0026quot;http://codingjunkie.net/micro-benchmarking-with-caliper/\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;http://codingjunkie.net/micro-benchmarking-with-caliper/\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;a href=\u0026quot;http://vertexlabs.co.uk/blog/caliper\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;http://vertexlabs.co.uk/blog/caliper\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;/ol\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;\u0026lt;a href=\u0026quot;http://openjdk.java.net/projects/code-tools/jmh/\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;JMH from OpenJDK\u0026lt;/a\u0026gt;\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Getting started tutorials\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;ol\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;a href=\u0026quot;http://www.oracle.com/technetwork/articles/java/architect-benchmarking-2266277.html\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;Avoiding Benchmarking Pitfalls on the JVM\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;a href=\u0026quot;http://nitschinger.at/Using-JMH-for-Java-Microbenchmarking\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;Using JMH for Java Microbenchmarking\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;a href=\u0026quot;https://web.archive.org/web/20181018130828/http://java-performance.info:80/jmh\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;Introduction to JMH\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;/ol\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Important things for Java benchmarks are:\u0026lt;/p\u0026gt;\n\n\u0026lt;ul\u0026gt;\n\u0026lt;li\u0026gt;Warm up the JIT first by running the code several times \u0026lt;strong\u0026gt;before timing\u0026lt;/strong\u0026gt; it\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;Make sure you run it for long enough to be able to measure the results in seconds or (better) tens of seconds\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;While you can\u0026apos;t call \u0026lt;code\u0026gt;System.gc()\u0026lt;/code\u0026gt; between iterations, it\u0026apos;s a good idea to run it between tests, so that each test will hopefully get a \u0026quot;clean\u0026quot; memory space to work with. (Yes, \u0026lt;code\u0026gt;gc()\u0026lt;/code\u0026gt; is more of a hint than a guarantee, but it\u0026apos;s very \u0026lt;em\u0026gt;likely\u0026lt;/em\u0026gt; that it really will garbage collect in my experience.)\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;I like to display iterations and time, and a score of time/iteration which can be scaled such that the \u0026quot;best\u0026quot; algorithm gets a score of 1.0 and others are scored in a relative fashion. This means you can run \u0026lt;em\u0026gt;all\u0026lt;/em\u0026gt; algorithms for a longish time, varying both number of iterations and time, but still getting comparable results.\u0026lt;/li\u0026gt;\n\u0026lt;/ul\u0026gt;\n\n\u0026lt;p\u0026gt;I\u0026apos;m just in the process of blogging about the design of a benchmarking framework in .NET. I\u0026apos;ve got a \u0026lt;a href=\u0026quot;http://msmvps.com/blogs/jon_skeet/archive/2009/01/26/benchmarking-made-easy.aspx\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;couple\u0026lt;/a\u0026gt; of \u0026lt;a href=\u0026quot;http://msmvps.com/blogs/jon_skeet/archive/2009/01/29/for-vs-foreach-on-arrays-and-lists.aspx\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;earlier posts\u0026lt;/a\u0026gt; which may be able to give you some ideas - not everything will be appropriate, of course, but some of it may be.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;http://openjdk.java.net/projects/code-tools/jmh/\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;jmh\u0026lt;/a\u0026gt; is a recent addition to OpenJDK and has been written by some performance engineers from Oracle. Certainly worth a look.\u0026lt;/p\u0026gt;\n\n\u0026lt;blockquote\u0026gt;\n  \u0026lt;p\u0026gt;The jmh is a Java harness for building, running, and analysing nano/micro/macro benchmarks written in Java and other languages targetting the JVM.\u0026lt;/p\u0026gt;\n\u0026lt;/blockquote\u0026gt;\n\n\u0026lt;p\u0026gt;Very interesting pieces of information buried in \u0026lt;a href=\u0026quot;http://hg.openjdk.java.net/code-tools/jmh/file/tip/jmh-samples/src/main/java/org/openjdk/jmh/samples/\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;the sample tests comments\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;See also:\u0026lt;/p\u0026gt;\n\n\u0026lt;ul\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;a href=\u0026quot;http://www.oracle.com/technetwork/articles/java/architect-benchmarking-2266277.html\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;Avoiding Benchmarking Pitfalls on the JVM\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;a href=\u0026quot;https://groups.google.com/forum/#!msg/mechanical-sympathy/m4opvy4xq3U/7lY8x8SvHgwJ\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;Discussion on the main strengths of jmh\u0026lt;/a\u0026gt;.\u0026lt;/li\u0026gt;\n\u0026lt;/ul\u0026gt;\n    ","\n\u0026lt;blockquote\u0026gt;\n  \u0026lt;p\u0026gt;Should the benchmark measure time/iteration or iterations/time, and why?\u0026lt;/p\u0026gt;\n\u0026lt;/blockquote\u0026gt;\n\n\u0026lt;p\u0026gt;It depends on \u0026lt;strong\u0026gt;what\u0026lt;/strong\u0026gt; you are trying to test.  \u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;If you are interested in \u0026lt;strong\u0026gt;latency\u0026lt;/strong\u0026gt;, use time/iteration and if you are interested in \u0026lt;strong\u0026gt;throughput\u0026lt;/strong\u0026gt;, use iterations/time.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Make sure you somehow use results which are computed in benchmarked code. Otherwise your code can be optimized away.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;If you are trying to compare two algorithms, do at least two benchmarks for each, alternating the order.  i.e.:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre class=\u0026quot;lang-java s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-java\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;for\u0026lt;/span\u0026gt;(i=\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1.\u0026lt;/span\u0026gt;.n)\n  alg1();\n\u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;for\u0026lt;/span\u0026gt;(i=\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1.\u0026lt;/span\u0026gt;.n)\n  alg2();\n\u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;for\u0026lt;/span\u0026gt;(i=\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1.\u0026lt;/span\u0026gt;.n)\n  alg2();\n\u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;for\u0026lt;/span\u0026gt;(i=\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1.\u0026lt;/span\u0026gt;.n)\n  alg1();\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;I have found some noticeable differences (5-10% sometimes) in the runtime of the same algorithm in different passes..\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Also, make sure that \u0026lt;em\u0026gt;n\u0026lt;/em\u0026gt; is very large, so that the runtime of each loop is at the very least 10 seconds or so.  The more iterations, the more significant figures in your benchmark time and the more reliable that data is.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;There are many possible pitfalls for writing micro-benchmarks in Java.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;First: You have to calculate with all sorts of events that take time more or less random: Garbage collection, caching effects (of OS for files and of CPU for memory), IO etc.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Second: You cannot trust the accuracy of the measured times for very short intervals.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Third: The JVM optimizes your code while executing. So different runs in the same JVM-instance will become faster and faster.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;My recommendations: Make your benchmark run some seconds, that is more reliable than a runtime over milliseconds. Warm up the JVM (means running the benchmark at least once without measuring, that the JVM can run optimizations). And run your benchmark multiple times (maybe 5 times) and take the median-value. Run every micro-benchmark in a new JVM-instance (call for every benchmark new Java) otherwise optimization effects of the JVM can influence later running tests. Don\u0026apos;t execute things, that aren\u0026apos;t executed in the warmup-phase (as this could trigger class-load and recompilation).\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;It should also be noted that it might also be important to analyze the results of the micro benchmark when comparing different implementations. Therefore a \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Significance_testing\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;significance test\u0026lt;/a\u0026gt; should be made.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;This is because implementation \u0026lt;code\u0026gt;A\u0026lt;/code\u0026gt; might be faster during most of the runs of the benchmark than implementation \u0026lt;code\u0026gt;B\u0026lt;/code\u0026gt;. But \u0026lt;code\u0026gt;A\u0026lt;/code\u0026gt; might also have a higher spread, so the measured performance benefit of \u0026lt;code\u0026gt;A\u0026lt;/code\u0026gt; won\u0026apos;t be of any significance when compared with \u0026lt;code\u0026gt;B\u0026lt;/code\u0026gt;.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;So it is also important to write and run a micro benchmark correctly, but also to analyze it correctly.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;To add to the other excellent advice, I\u0026apos;d also be mindful of the following:\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;For some CPUs (e.g. Intel Core i5 range with TurboBoost), the temperature (and number of cores currently being used, as well as thier utilisation percent) affects the clock speed. Since CPUs are dynamically clocked, this can affect your results. For example, if you have a single-threaded application, the maximum clock speed (with TurboBoost) is higher than for an application using all cores. This can therefore interfere with comparisons of single and multi-threaded performance on some systems. Bear in mind that the temperature and volatages also affect how long Turbo frequency is maintained.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Perhaps a more fundamentally important aspect that you have direct control over: make sure you\u0026apos;re measuring the right thing! For example, if you\u0026apos;re using \u0026lt;code\u0026gt;System.nanoTime()\u0026lt;/code\u0026gt; to benchmark a particular bit of code, put the calls to the assignment in places that make sense to avoid measuring things which you aren\u0026apos;t interested in. For example, don\u0026apos;t do:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre class=\u0026quot;lang-java s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-java\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-type\u0026quot;\u0026gt;long\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-variable\u0026quot;\u0026gt;startTime\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-operator\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; System.nanoTime();\n\u0026lt;span class=\u0026quot;hljs-comment\u0026quot;\u0026gt;//code here...\u0026lt;/span\u0026gt;\nSystem.out.println(\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;Code took \u0026quot;\u0026lt;/span\u0026gt;+(System.nanoTime()-startTime)+\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;nano seconds\u0026quot;\u0026lt;/span\u0026gt;);\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;Problem is you\u0026apos;re not immediately getting the end time when the code has finished. Instead, try the following:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre class=\u0026quot;lang-java s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-java\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;final\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-type\u0026quot;\u0026gt;long\u0026lt;/span\u0026gt; endTime, startTime = System.nanoTime();\n\u0026lt;span class=\u0026quot;hljs-comment\u0026quot;\u0026gt;//code here...\u0026lt;/span\u0026gt;\nendTime = System.nanoTime();\nSystem.out.println(\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;Code took \u0026quot;\u0026lt;/span\u0026gt;+(endTime-startTime)+\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;nano seconds\u0026quot;\u0026lt;/span\u0026gt;);\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n    ","\n\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;http://opt.sourceforge.net/\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;http://opt.sourceforge.net/\u0026lt;/a\u0026gt; Java Micro Benchmark - control tasks required to determine the comparative performance characteristics of the computer system on different platforms. Can be used to guide optimization decisions and to compare different Java implementations.\u0026lt;/p\u0026gt;\n    "],"id":73,"title":"How do I write a correct micro-benchmark in Java?","content":"\n                \n\u0026lt;p\u0026gt;How do you write (and run) a correct micro-benchmark in Java?\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;I\u0026apos;m looking for some code samples and comments illustrating various things to think about.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Example: Should the benchmark measure time/iteration or iterations/time, and why?\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Related: \u0026lt;a href=\u0026quot;https://stackoverflow.com/questions/410437/is-stopwatch-benchmarking-acceptable\u0026quot;\u0026gt;Is stopwatch benchmarking acceptable?\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n    ","slug":"how-do-i-write-a-correct-micro-benchmark-in-java-1657384488725","postType":"QUESTION","createdAt":"2022-07-09T16:34:48.000Z","updatedAt":"2022-07-09T16:34:48.000Z","tags":[{"id":265,"name":"jvm","slug":"jvm","createdAt":"2022-07-09T16:34:48.000Z","updatedAt":"2022-07-09T16:34:48.000Z","Questions_Tags":{"questionId":73,"tagId":265}},{"id":266,"name":"benchmarking","slug":"benchmarking","createdAt":"2022-07-09T16:34:48.000Z","updatedAt":"2022-07-09T16:34:48.000Z","Questions_Tags":{"questionId":73,"tagId":266}},{"id":267,"name":"jvm-hotspot","slug":"jvm-hotspot","createdAt":"2022-07-09T16:34:48.000Z","updatedAt":"2022-07-09T16:34:48.000Z","Questions_Tags":{"questionId":73,"tagId":267}},{"id":268,"name":"microbenchmark","slug":"microbenchmark","createdAt":"2022-07-09T16:34:48.000Z","updatedAt":"2022-07-09T16:34:48.000Z","Questions_Tags":{"questionId":73,"tagId":268}}]}},"__N_SSG":true},"page":"/questions/[slug]","query":{"slug":"how-do-i-write-a-correct-micro-benchmark-in-java-1657384488725"},"buildId":"8pZkyd0U8-Y2Qf3QK9j7l","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>