<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@solutionschecker.com"/><meta name="twitter:creator" content="@solutionschecker.com"/><meta property="og:url" content="https://solutionschecker.com"/><meta property="og:type" content="website"/><meta property="og:image" content="https://solutionschecker.com/solutions-checker-banner.png"/><meta property="og:image:alt" content="Find the solution to any question. We focus on finding the fastest possible solution for users. Main topics like coding, learning. - solutionschecker.com"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><script type="application/ld+json">{"@context":"https://schema.org","@type":"Organization","logo":"/logo.svg","url":"https://solutionschecker.com"}</script><link name="keywords" content="solutions checker, solution checker, how to, solution for, check for solution, resolve question, what is, what solution for, find solution"/><script type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"https://solutionschecker.com","name":"Home"}},{"@type":"ListItem","position":2,"item":{"@id":"https://solutionschecker.com/questions","name":"Questions"}},{"@type":"ListItem","position":3,"item":{"@id":"https://solutionschecker.com/questions/how-can-i-find-the-time-complexity-of-an-algorithm-1657388486001","name":"Questions"}}]}</script><title>How can I find the time complexity of an algorithm? | Solution Checker</title><meta name="robots" content="index,follow"/><meta name="description" content="I have gone through Google and Stack Overflow search, but nowhere I was able to find a clear and straightforward explanation for how to calculate time complexity.
What do I know already?
Say for code as simple as the one below:
char h = &#x27;y&#x27;; // This will be executed 1 time
int abc = 0; // This will be executed 1 time

Say for a loop like the one below:
for (int i = 0; i &lt; N; i++) {
    Console.Write(&#x27;Hello, World!!&#x27;);
}


int i=0; This will be executed only once.

The time is actually calculated to i=0 and not the declaration.

i &lt; N; This will be executed N+1 times
i++ This will be executed N times

So the number of operations required by this loop are {1+(N+1)+N} = 2N+2. (But this still may be wrong, as I am not confident about my understanding.)
OK, so these small basic calculations I think I know, but in most cases I have seen the time complexity as O(N), O(n^2), O(log n), O(n!), and many others.
    "/><meta property="og:title" content="How can I find the time complexity of an algorithm? | Solution Checker"/><meta property="og:description" content="I have gone through Google and Stack Overflow search, but nowhere I was able to find a clear and straightforward explanation for how to calculate time complexity.
What do I know already?
Say for code as simple as the one below:
char h = &#x27;y&#x27;; // This will be executed 1 time
int abc = 0; // This will be executed 1 time

Say for a loop like the one below:
for (int i = 0; i &lt; N; i++) {
    Console.Write(&#x27;Hello, World!!&#x27;);
}


int i=0; This will be executed only once.

The time is actually calculated to i=0 and not the declaration.

i &lt; N; This will be executed N+1 times
i++ This will be executed N times

So the number of operations required by this loop are {1+(N+1)+N} = 2N+2. (But this still may be wrong, as I am not confident about my understanding.)
OK, so these small basic calculations I think I know, but in most cases I have seen the time complexity as O(N), O(n^2), O(log n), O(n!), and many others.
    "/><script type="application/ld+json">{"@context":"https://schema.org","@type":"QAPage","mainEntity":{"name":"How can I find the time complexity of an algorithm?","text":"I have gone through Google and Stack Overflow search, but nowhere I was able to find a clear and straightforward explanation for how to calculate time complexity.\nWhat do I know already?\nSay for code as simple as the one below:\nchar h = &apos;y&apos;; // This will be executed 1 time\nint abc = 0; // This will be executed 1 time\n\nSay for a loop like the one below:\nfor (int i = 0; i &lt; N; i++) {\n    Console.Write(&apos;Hello, World!!&apos;);\n}\n\n\nint i=0; This will be executed only once.\n\nThe time is actually calculated to i=0 and not the declaration.\n\ni &lt; N; This will be executed N+1 times\ni++ This will be executed N times\n\nSo the number of operations required by this loop are {1+(N+1)+N} = 2N+2. (But this still may be wrong, as I am not confident about my understanding.)\nOK, so these small basic calculations I think I know, but in most cases I have seen the time complexity as O(N), O(n^2), O(log n), O(n!), and many others.\n    ","answerCount":9,"upVoteCount":500,"suggestedAnswer":[{"text":"\n  How to find time complexity of an algorithm\n\n\nYou add up how many machine instructions it will execute as a function of the size of its input, and then simplify the expression to the largest (when N is very large) term  and can include any simplifying constant factor.\n\nFor example, lets see how we simplify 2N + 2 machine instructions to describe this as just O(N).\n\nWhy do we remove the two 2s ?\n\nWe are interested in the performance of the algorithm as N becomes large.\n\nConsider the two terms 2N and 2. \n\nWhat is the relative influence of these two terms as N becomes large? Suppose N is a million.\n\nThen the first term is 2 million and the second term is only 2.\n\nFor this reason, we drop all but the largest terms for large N.\n\nSo, now we have gone from 2N + 2 to 2N.\n\nTraditionally, we are only interested in performance up to constant factors. \n\nThis means that we don&apos;t really care if there is some constant multiple of difference in performance when N is large.  The unit of 2N is not well-defined in the first place anyway.  So we can multiply or divide by a constant factor to get to the simplest expression.\n\nSo 2N becomes just N.\n    ","url":"/questions/[slug]#solution1","@type":"Answer","upvoteCount":0},{"text":"This is an excellent article: Time complexity of algorithm\nThe below answer is copied from above (in case the excellent link goes bust)\nThe most common metric for calculating time complexity is Big O notation. This removes all constant factors so that the running time can be estimated in relation to N as N approaches infinity. In general you can think of it like this:\nstatement;\n\nIs constant. The running time of the statement will not change in relation to N.\nfor ( i = 0; i &lt; N; i++ )\n     statement;\n\nIs linear. The running time of the loop is directly proportional to N. When N doubles, so does the running time.\nfor ( i = 0; i &lt; N; i++ ) {\n  for ( j = 0; j &lt; N; j++ )\n    statement;\n}\n\nIs quadratic. The running time of the two loops is proportional to the square of N. When N doubles, the running time increases by N * N.\nwhile ( low &lt;= high ) {\n  mid = ( low + high ) / 2;\n  if ( target &lt; list[mid] )\n    high = mid - 1;\n  else if ( target &gt; list[mid] )\n    low = mid + 1;\n  else break;\n}\n\nIs logarithmic. The running time of the algorithm is proportional to the number of times N can be divided by 2. This is because the algorithm divides the working area in half with each iteration.\nvoid quicksort (int list[], int left, int right)\n{\n  int pivot = partition (list, left, right);\n  quicksort(list, left, pivot - 1);\n  quicksort(list, pivot + 1, right);\n}\n\nIs N * log (N). The running time consists of N loops (iterative or recursive) that are logarithmic, thus the algorithm is a combination of linear and logarithmic.\nIn general, doing something with every item in one dimension is linear, doing something with every item in two dimensions is quadratic, and dividing the working area in half is logarithmic. There are other Big O measures such as cubic, exponential, and square root, but they&apos;re not nearly as common. Big O notation is described as O ( &lt;type&gt; ) where &lt;type&gt; is the measure. The quicksort algorithm would be described as O (N * log(N )).\nNote that none of this has taken into account best, average, and worst case measures. Each would have its own Big O notation. Also note that this is a VERY simplistic explanation. Big O is the most common, but it&apos;s also more complex that I&apos;ve shown. There are also other notations such as big omega, little o, and big theta. You probably won&apos;t encounter them outside of an algorithm analysis course. ;)\n    ","url":"/questions/[slug]#solution2","@type":"Answer","upvoteCount":0},{"text":"Taken from here - Introduction to Time Complexity of an Algorithm\n1. Introduction\nIn computer science, the time complexity of an algorithm quantifies the amount of time taken by an algorithm to run as a function of the length of the string representing the input.\n2. Big O notation\nThe time complexity of an algorithm is commonly expressed using big O notation, which excludes coefficients and lower order terms. When expressed this way, the time complexity is said to be described asymptotically, i.e., as the input size goes to infinity.\nFor example, if the time required by an algorithm on all inputs of size n is at most 5n3 + 3n, the asymptotic time complexity is O(n3). More on that later.\nA few more examples:\n\n1 = O(n)\nn = O(n2)\nlog(n) = O(n)\n2 n + 1 = O(n)\n\n3. O(1) constant time:\nAn algorithm is said to run in constant time if it requires the same amount of time regardless of the input size.\nExamples:\n\narray: accessing any element\nfixed-size stack: push and pop methods\nfixed-size queue: enqueue and dequeue methods\n\n4. O(n) linear time\nAn algorithm is said to run in linear time if its time execution is directly proportional to the input size, i.e. time grows linearly as input size increases.\nConsider the following examples. Below I am linearly searching for an element, and this has a time complexity of O(n).\nint find = 66;\nvar numbers = new int[] { 33, 435, 36, 37, 43, 45, 66, 656, 2232 };\nfor (int i = 0; i &lt; numbers.Length - 1; i++)\n{\n    if(find == numbers[i])\n    {\n        return;\n    }\n}\n\nMore Examples:\n\nArray: Linear Search, Traversing, Find minimum etc\nArrayList: contains method\nQueue: contains method\n\n5. O(log n) logarithmic time:\nAn algorithm is said to run in logarithmic time if its time execution is proportional to the logarithm of the input size.\nExample: Binary Search\nRecall the &quot;twenty questions&quot; game - the task is to guess the value of a hidden number in an interval. Each time you make a guess, you are told whether your guess is too high or too low. Twenty questions game implies a strategy that uses your guess number to halve the interval size. This is an example of the general problem-solving method known as binary search.\n6. O(n2) quadratic time\nAn algorithm is said to run in quadratic time if its time execution is proportional to the square of the input size.\nExamples:\n\nBubble Sort\nSelection Sort\nInsertion Sort\n\n7. Some useful links\n\nBig-O Misconceptions\nDetermining The Complexity Of Algorithm\nBig O Cheat Sheet\n\n    ","url":"/questions/[slug]#solution3","@type":"Answer","upvoteCount":0},{"text":"Several examples of loop.\n\nO(n) time complexity of a loop is considered as O(n) if the loop variables is incremented / decremented by a constant amount. For example following functions have O(n) time complexity.\n  // Here c is a positive integer constant\n  for (int i = 1; i &lt;= n; i += c) {\n      // some O(1) expressions\n  }\n\n  for (int i = n; i &gt; 0; i -= c) {\n      // some O(1) expressions\n  }\n\n\nO(nc) time complexity of nested loops is equal to the number of times the innermost statement is executed. For example, the following sample loops have O(n2) time complexity\n  for (int i = 1; i &lt;=n; i += c) {\n     for (int j = 1; j &lt;=n; j += c) {\n        // some O(1) expressions\n     }\n  }\n\n  for (int i = n; i &gt; 0; i += c) {\n     for (int j = i+1; j &lt;=n; j += c) {\n        // some O(1) expressions\n  }\n\nFor example, selection sort and insertion sort have O(n2) time complexity.\n\nO(log n) time complexity of a loop is considered as O(log n) if the loop variables is divided / multiplied by a constant amount.\n  for (int i = 1; i &lt;=n; i *= c) {\n     // some O(1) expressions\n  }\n  for (int i = n; i &gt; 0; i /= c) {\n     // some O(1) expressions\n  }\n\nFor example, [binary search][3] has _O(log&amp;nbsp;n)_ time complexity.\n\n\nO(log log n) time complexity of a loop is considered as O(log log n) if the loop variables is reduced / increased exponentially by a constant amount.\n  // Here c is a constant greater than 1\n  for (int i = 2; i &lt;=n; i = pow(i, c)) {\n     // some O(1) expressions\n  }\n  //Here fun is sqrt or cuberoot or any other constant root\n  for (int i = n; i &gt; 0; i = fun(i)) {\n     // some O(1) expressions\n  }\n\n\n\n\nOne example of time complexity analysis\nint fun(int n)\n{\n    for (int i = 1; i &lt;= n; i++)\n    {\n        for (int j = 1; j &lt; n; j += i)\n        {\n            // Some O(1) task\n        }\n    }\n}\n\nAnalysis:\nFor i = 1, the inner loop is executed n times.\nFor i = 2, the inner loop is executed approximately n/2 times.\nFor i = 3, the inner loop is executed approximately n/3 times.\nFor i = 4, the inner loop is executed approximately n/4 times.\n.\nFor i = n, the inner loop is executed approximately n/n times.\n\nSo the total time complexity of the above algorithm is (n + n/2 + n/3 +  + n/n), which becomes n * (1/1 + 1/2 + 1/3 +  + 1/n)\nThe important thing about series (1/1 + 1/2 + 1/3 +  + 1/n) is around to O(log n). So the time complexity of the above code is O(n·log n).\n\nReferences:\n1\n2\n3\n    ","url":"/questions/[slug]#solution4","@type":"Answer","upvoteCount":0},{"text":"Time complexity with examples\n1 - Basic operations (arithmetic, comparisons, accessing arrays elements, assignment): The running time is always constant O(1)\nExample:\nread(x)                               // O(1)\na = 10;                               // O(1)\na = 1,000,000,000,000,000,000         // O(1)\n\n2 - If then else statement: Only taking the maximum running time from two or more possible statements.\nExample:\nage = read(x)                               // (1+1) = 2\nif age &lt; 17 then begin                      // 1\n      status = &quot;Not allowed!&quot;;              // 1\nend else begin\n      status = &quot;Welcome! Please come in&quot;;   // 1\n      visitors = visitors + 1;              // 1+1 = 2\nend;\n\nSo, the complexity of the above pseudo code is T(n) = 2 + 1 + max(1, 1+2) = 6. Thus, its big oh is still constant T(n) = O(1).\n3 - Looping (for, while, repeat): Running time for this statement is the number of loops multiplied by the number of operations inside that looping.\nExample:\ntotal = 0;                                  // 1\nfor i = 1 to n do begin                     // (1+1)*n = 2n\n      total = total + i;                    // (1+1)*n = 2n\nend;\nwriteln(total);                             // 1\n\nSo, its complexity is T(n) = 1+4n+1 = 4n + 2. Thus, T(n) = O(n).\n4 - Nested loop (looping inside looping): Since there is at least one looping inside the main looping, running time of this statement used O(n^2) or O(n^3).\nExample:\nfor i = 1 to n do begin                     // (1+1)*n  = 2n\n   for j = 1 to n do begin                  // (1+1)n*n = 2n^2\n       x = x + 1;                           // (1+1)n*n = 2n^2\n       print(x);                            // (n*n)    = n^2\n   end;\nend;\n\nCommon running time\nThere are some common running times when analyzing an algorithm:\n\nO(1)  Constant time\nConstant time means the running time is constant, its not affected by the input size.\n\nO(n)  Linear time\nWhen an algorithm accepts n input size, it would perform n operations as well.\n\nO(log n)  Logarithmic time\nAlgorithm that has running time O(log n) is slight faster than O(n). Commonly, algorithm divides the problem into sub problems with the same size. Example: binary search algorithm, binary conversion algorithm.\n\nO(n log n)  Linearithmic time\nThis running time is often found in &quot;divide &amp; conquer algorithms&quot; which divide the problem into sub problems recursively and then merge them in n time. Example: Merge Sort algorithm.\n\nO(n2)  Quadratic time\nLook Bubble Sort algorithm!\n\nO(n3)  Cubic time\nIt has the same principle with O(n2).\n\nO(2n)  Exponential time\nIt is very slow as input get larger, if n = 1,000,000, T(n) would be 21,000,000. Brute Force algorithm has this running time.\n\nO(n!)  Factorial time\nThe slowest!!! Example: Travelling salesman problem (TSP)\n\n\nIt is taken from this article. It is very well explained and you should give it a read.\n    ","url":"/questions/[slug]#solution5","@type":"Answer","upvoteCount":0},{"text":"When you&apos;re analyzing  code, you have to analyse it line by line, counting every operation/recognizing time complexity. In the end, you have to sum it to get whole picture.\nFor example, you can have one simple loop with linear complexity, but later in that same program you can have a triple loop that has cubic complexity, so your program will have cubic complexity. Function order of growth comes into play right here.\nLet&apos;s look at what are possibilities for time complexity of an algorithm, you can see order of growth I mentioned above:\n\nConstant time has an order of growth 1, for example: a = b + c.\n\nLogarithmic time has an order of growth log N. It usually occurs when you&apos;re dividing something in half (binary search, trees, and even loops), or multiplying something in same way.\n\nLinear. The order of growth is N, for example\n int p = 0;\n for (int i = 1; i &lt; N; i++)\n   p = p + 2;\n\n\nLinearithmic. The order of growth is n·log N. It usually occurs in divide-and-conquer algorithms.\n\nCubic. The order of growth is N3. A classic example is a triple loop where you check all triplets:\n int x = 0;\n for (int i = 0; i &lt; N; i++)\n    for (int j = 0; j &lt; N; j++)\n       for (int k = 0; k &lt; N; k++)\n           x = x + 2\n\n\nExponential. The order of growth is 2N. It usually occurs when you do exhaustive search, for example, check subsets of some set.\n\n\n    ","url":"/questions/[slug]#solution6","@type":"Answer","upvoteCount":0},{"text":"Loosely speaking, time complexity is a way of summarising how the number of operations or run-time of an algorithm grows as the input size increases.\n\nLike most things in life, a cocktail party can help us understand.\n\nO(N)\n\nWhen you arrive at the party, you have to shake everyone&apos;s hand (do an operation on every item). As the number of attendees N increases, the time/work it will take you to shake everyone&apos;s hand increases as O(N).\n\nWhy O(N) and not cN?\n\nThere&apos;s variation in the amount of time it takes to shake hands with people. You could average this out and capture it in a constant c. But the fundamental operation here --- shaking hands with everyone --- would always be proportional to O(N), no matter what c was. When debating whether we should go to a cocktail party, we&apos;re often more interested in the fact that we&apos;ll have to meet everyone than in the minute details of what those meetings look like.\n\nO(N^2)\n\nThe host of the cocktail party wants you to play a silly game where everyone meets everyone else. Therefore, you must meet N-1 other people and, because the next person has already met you, they must meet N-2 people, and so on. The sum of this series is x^2/2+x/2. As the number of attendees grows, the x^2 term gets big fast, so we just drop everything else.\n\nO(N^3)\n\nYou have to meet everyone else and, during each meeting, you must talk about everyone else in the room.\n\nO(1)\n\nThe host wants to announce something. They ding a wineglass and speak loudly. Everyone hears them. It turns out it doesn&apos;t matter how many attendees there are, this operation always takes the same amount of time.\n\nO(log N)\n\nThe host has laid everyone out at the table in alphabetical order. Where is Dan? You reason that he must be somewhere between Adam and Mandy (certainly not between Mandy and Zach!). Given that, is he between George and Mandy? No. He must be between Adam and Fred, and between Cindy and Fred. And so on... we can efficiently locate Dan by looking at half the set and then half of that set. Ultimately, we look at O(log_2 N) individuals.\n\nO(N log N)\n\nYou could find where to sit down at the table using the algorithm above. If a large number of people came to the table, one at a time, and all did this, that would take O(N log N) time. This turns out to be how long it takes to sort any collection of items when they must be compared.\n\nBest/Worst Case\n\nYou arrive at the party and need to find Inigo - how long will it take? It depends on when you arrive. If everyone is milling around you&apos;ve hit the worst-case: it will take O(N) time. However, if everyone is sitting down at the table, it will take only O(log N) time. Or maybe you can leverage the host&apos;s wineglass-shouting power and it will take only O(1) time.\n\nAssuming the host is unavailable, we can say that the Inigo-finding algorithm has a lower-bound of O(log N) and an upper-bound of O(N), depending on the state of the party when you arrive.\n\nSpace &amp; Communication\n\nThe same ideas can be applied to understanding how algorithms use space or communication.\n\nKnuth has written a nice paper about the former entitled &quot;The Complexity of Songs&quot;.\n\n\n  Theorem 2: There exist arbitrarily long songs of complexity O(1).\n  \n  PROOF: (due to Casey and the Sunshine Band). Consider the songs Sk defined by (15), but with\n\n\nV_k = &apos;That&apos;s the way,&apos; U &apos;I like it, &apos; U\nU   = &apos;uh huh,&apos; &apos;uh huh&apos;\n\n\n\n  for all k. \n\n    ","url":"/questions/[slug]#solution7","@type":"Answer","upvoteCount":0},{"text":"For the mathematically-minded people: The master theorem is another useful thing to know when studying complexity.\n    ","url":"/questions/[slug]#solution8","@type":"Answer","upvoteCount":0},{"text":"O(n) is big O notation used for writing time complexity of an algorithm. When you add up the number of executions in an algorithm, you&apos;ll get an expression in result like 2N+2. In this expression, N is the dominating term (the term having largest effect on expression if its value increases or decreases). Now O(N) is the time complexity while N is dominating term.\nExample\nFor i = 1 to n;\n  j = 0;\nwhile(j &lt;= n);\n  j = j + 1;\n\nHere the total number of executions for the inner loop are n+1 and the total number of executions for the outer loop are n(n+1)/2, so the total number of executions for the whole algorithm are n + 1 + n(n+1/2) = (n2 + 3n)/2.\nHere n^2 is the dominating term so the time complexity for this algorithm is O(n2).\n    ","url":"/questions/[slug]#solution9","@type":"Answer","upvoteCount":0}],"@type":"Question"}}</script><meta name="next-head-count" content="22"/><link rel="preload" href="/_next/static/css/4f7b2847abfe82c6.css" as="style"/><link rel="stylesheet" href="/_next/static/css/4f7b2847abfe82c6.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-0d1b80a048d4787e.js"></script><script src="/_next/static/chunks/webpack-42cdea76c8170223.js" defer=""></script><script src="/_next/static/chunks/framework-4556c45dd113b893.js" defer=""></script><script src="/_next/static/chunks/main-ccfab947c79712f4.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c0d2dcb5e85faf18.js" defer=""></script><script src="/_next/static/chunks/294-106ef8570fa99deb.js" defer=""></script><script src="/_next/static/chunks/490-7f0418bb4354ac73.js" defer=""></script><script src="/_next/static/chunks/pages/questions/%5Bslug%5D-50e201fdaa1e0fd1.js" defer=""></script><script src="/_next/static/eGqQHYKGZKE-nXShFn8wX/_buildManifest.js" defer=""></script><script src="/_next/static/eGqQHYKGZKE-nXShFn8wX/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="wrapper"><header><nav class="bg-white border-gray-200 px-4 lg:px-6 py-2.5 dark:bg-gray-800"><div class="flex flex-wrap justify-between items-center mx-auto max-w-screen-xl"><a class="flex items-center" href="/"><img src="/logo-second.png" class="mr-3 h-6 sm:h-9" alt="Solution Checker Logo"/><h1 class="self-center text-xl font-semibold whitespace-nowrap dark:text-white">Solution Checker</h1></a><div class="flex items-center lg:order-2"><button data-collapse-toggle="mobile-menu-2" type="button" class="inline-flex items-center p-2 ml-1 text-sm text-gray-500 rounded-lg lg:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200 dark:text-gray-400 dark:hover:bg-gray-700 dark:focus:ring-gray-600" aria-controls="mobile-menu-2" aria-expanded="false"><span class="sr-only">Open main menu</span><svg class="w-6 h-6" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 10a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 15a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z" clip-rule="evenodd"></path></svg><svg class="hidden w-6 h-6" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M4.293 4.293a1 1 0 011.414 0L10 8.586l4.293-4.293a1 1 0 111.414 1.414L11.414 10l4.293 4.293a1 1 0 01-1.414 1.414L10 11.414l-4.293 4.293a1 1 0 01-1.414-1.414L8.586 10 4.293 5.707a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></button></div><div class="hidden justify-between items-center w-full lg:flex lg:w-auto lg:order-1" id="mobile-menu-2"><ul class="flex flex-col mt-4 font-medium lg:flex-row lg:space-x-8 lg:mt-0"><li><a class="block py-2 pr-4 pl-3 text-gray-700 border-b border-gray-100 hover:bg-gray-50 lg:hover:bg-transparent lg:border-0 lg:hover:text-blue-700 lg:p-0 dark:text-gray-400 lg:dark:hover:text-white dark:hover:bg-gray-700 dark:hover:text-white lg:dark:hover:bg-transparent dark:border-gray-700" aria-current="page" href="/">Home</a></li><li><a class="block py-2 pr-4 pl-3 text-gray-700 border-b border-gray-100 hover:bg-gray-50 lg:hover:bg-transparent lg:border-0 lg:hover:text-blue-700 lg:p-0 dark:text-gray-400 lg:dark:hover:text-white dark:hover:bg-gray-700 dark:hover:text-white lg:dark:hover:bg-transparent dark:border-gray-700" href="/questions?tab=news">Questions</a></li><li><a class="block py-2 pr-4 pl-3 text-gray-700 border-b border-gray-100 hover:bg-gray-50 lg:hover:bg-transparent lg:border-0 lg:hover:text-blue-700 lg:p-0 dark:text-gray-400 lg:dark:hover:text-white dark:hover:bg-gray-700 dark:hover:text-white lg:dark:hover:bg-transparent dark:border-gray-700" href="/post?tab=news">Post</a></li><li><a class="block py-2 pr-4 pl-3 text-gray-700 border-b border-gray-100 hover:bg-gray-50 lg:hover:bg-transparent lg:border-0 lg:hover:text-blue-700 lg:p-0 dark:text-gray-400 lg:dark:hover:text-white dark:hover:bg-gray-700 dark:hover:text-white lg:dark:hover:bg-transparent dark:border-gray-700" href="/questions/how-can-i-find-the-time-complexity-of-an-algorithm-1657388486001#">Coding</a></li></ul></div></div></nav></header><div class="main-content"><div class="question my-5"><div class="flex question-header items-center m-auto justify-center"><div class="rounded-xl w-full border p-5 shadow-md bg-white"><div class="flex w-full items-center justify-between border-b pb-3"><div class="flex items-center space-x-3"><div class="text-lg font-bold text-slate-700"><a href="/questions/how-can-i-find-the-time-complexity-of-an-algorithm-1657388486001"><h1>How can I find the time complexity of an algorithm?</h1></a></div></div><div class="flex flex-wrap h-auto justify-end items-center space-x-8"></div></div><div class="question-content mt-5">
                
<p>I have gone through <a href="https://www.google.co.in/search?aq=f&amp;sugexp=chrome,mod=19&amp;sourceid=chrome&amp;ie=UTF-8&amp;q=How%20to%20find%20time%20complexity%20of%20an%20algorithm" rel="noreferrer">Google</a> and <a href="https://stackoverflow.com/search?q=how%20to%20calculate%20time%20complexity%20of%20an%20algorithm">Stack Overflow</a> search, but nowhere I was able to find a clear and straightforward explanation for how to calculate time complexity.</p>
<h3>What do I know already?</h3>
<p>Say for code as simple as the one below:</p>
<pre class="lang-cs s-code-block"><code class="hljs language-csharp"><span class="hljs-built_in">char</span> h = <span class="hljs-string">'y'</span>; <span class="hljs-comment">// This will be executed 1 time</span>
<span class="hljs-built_in">int</span> abc = <span class="hljs-number">0</span>; <span class="hljs-comment">// This will be executed 1 time</span>
</code></pre>
<p>Say for a loop like the one below:</p>
<pre class="lang-cs s-code-block"><code class="hljs language-csharp"><span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> i = <span class="hljs-number">0</span>; i &lt; N; i++) {
    Console.Write(<span class="hljs-string">'Hello, World!!'</span>);
}
</code></pre>
<ul>
<li><code>int i=0;</code> This will be executed only <strong>once</strong>.</li>
</ul>
<p>The time is actually calculated to <code>i=0</code> and not the declaration.</p>
<ul>
<li><code>i &lt; N;</code> This will be executed <strong>N+1</strong> times</li>
<li><code>i++</code> This will be executed <strong>N</strong> times</li>
</ul>
<p>So the number of operations required by this loop are <strong>{1+(N+1)+N} = 2N+2</strong>. (But this still may be wrong, as I am not confident about my understanding.)</p>
<p>OK, so these small basic calculations I think I know, but in most cases I have seen the time complexity as <strong>O(N), O(n^2), O(log n), O(n!)</strong>, and <a href="http://en.wikipedia.org/wiki/Time_complexity" rel="noreferrer">many others</a>.</p>
    </div></div></div><div class="solution-section"><nav class="flex pagination-solution flex-col justify-end"><h1 class="text-lg font-semibold mb-5">Navigate to solutions: </h1><ul class="inline-flex -space-x-px overflow-auto"><li class="pagination-solution-item"><span data-id="#solution1" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">1</span></li><li class="pagination-solution-item"><span data-id="#solution2" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">2</span></li><li class="pagination-solution-item"><span data-id="#solution3" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">3</span></li><li class="pagination-solution-item"><span data-id="#solution4" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">4</span></li><li class="pagination-solution-item"><span data-id="#solution5" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">5</span></li><li class="pagination-solution-item"><span data-id="#solution6" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">6</span></li><li class="pagination-solution-item"><span data-id="#solution7" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">7</span></li><li class="pagination-solution-item"><span data-id="#solution8" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">8</span></li><li class="pagination-solution-item"><span data-id="#solution9" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">9</span></li></ul></nav><div id="solution1" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h1 class="text-4xl font-semibold mb-5">Solution 1</h1><div class="tags-wrap h-max space-x-8"><div class="tags"></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<blockquote>
  <p>How to find time complexity of an algorithm</p>
</blockquote>

<p>You add up how many machine instructions it will execute as a function of the size of its input, and then simplify the expression to the largest (when N is very large) term  and can include any simplifying constant factor.</p>

<p>For example, lets see how we simplify <code>2N + 2</code> machine instructions to describe this as just <code>O(N)</code>.</p>

<p><strong>Why do we remove the two <code>2</code>s ?</strong></p>

<p>We are interested in the performance of the algorithm as N becomes large.</p>

<p>Consider the two terms 2N and 2. </p>

<p>What is the relative influence of these two terms as N becomes large? Suppose N is a million.</p>

<p>Then the first term is 2 million and the second term is only 2.</p>

<p>For this reason, we drop all but the largest terms for large N.</p>

<p>So, now we have gone from <code>2N + 2</code> to <code>2N</code>.</p>

<p>Traditionally, we are only interested in performance <em>up to constant factors</em>. </p>

<p>This means that we don't really care if there is some constant multiple of difference in performance when N is large.  The unit of 2N is not well-defined in the first place anyway.  So we can multiply or divide by a constant factor to get to the simplest expression.</p>

<p>So <code>2N</code> becomes just <code>N</code>.</p>
    </div></div></div></div><div id="solution2" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h1 class="text-4xl font-semibold mb-5">Solution 2</h1><div class="tags-wrap h-max space-x-8"><div class="tags"></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>This is an excellent article: <em><a href="http://www.daniweb.com/software-development/computer-science/threads/13488/time-complexity-of-algorithm" rel="nofollow noreferrer">Time complexity of algorithm</a></em></p>
<p><strong>The below answer is copied from above (in case the excellent link goes bust)</strong></p>
<p>The most common metric for calculating time complexity is Big O notation. This removes all constant factors so that the running time can be estimated in relation to N as N approaches infinity. In general you can think of it like this:</p>
<pre><code>statement;
</code></pre>
<p>Is constant. The running time of the statement will not change in relation to N.</p>
<pre><code>for ( i = 0; i &lt; N; i++ )
     statement;
</code></pre>
<p>Is linear. The running time of the loop is directly proportional to N. When N doubles, so does the running time.</p>
<pre><code>for ( i = 0; i &lt; N; i++ ) {
  for ( j = 0; j &lt; N; j++ )
    statement;
}
</code></pre>
<p>Is quadratic. The running time of the two loops is proportional to the square of N. When N doubles, the running time increases by N * N.</p>
<pre><code>while ( low &lt;= high ) {
  mid = ( low + high ) / 2;
  if ( target &lt; list[mid] )
    high = mid - 1;
  else if ( target &gt; list[mid] )
    low = mid + 1;
  else break;
}
</code></pre>
<p>Is logarithmic. The running time of the algorithm is proportional to the number of times N can be divided by 2. This is because the algorithm divides the working area in half with each iteration.</p>
<pre><code>void quicksort (int list[], int left, int right)
{
  int pivot = partition (list, left, right);
  quicksort(list, left, pivot - 1);
  quicksort(list, pivot + 1, right);
}
</code></pre>
<p>Is N * log (N). The running time consists of N loops (iterative or recursive) that are logarithmic, thus the algorithm is a combination of linear and logarithmic.</p>
<p>In general, doing something with every item in one dimension is linear, doing something with every item in two dimensions is quadratic, and dividing the working area in half is logarithmic. There are other Big O measures such as cubic, exponential, and square root, but they're not nearly as common. Big O notation is described as <code>O ( &lt;type&gt; )</code> where <code>&lt;type&gt;</code> is the measure. The quicksort algorithm would be described as <code>O (N * log(N ))</code>.</p>
<p>Note that none of this has taken into account best, average, and worst case measures. Each would have its own Big O notation. Also note that this is a VERY simplistic explanation. Big O is the most common, but it's also more complex that I've shown. There are also other notations such as big omega, little o, and big theta. You probably won't encounter them outside of an algorithm analysis course. ;)</p>
    </div></div></div></div><div id="solution3" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h1 class="text-4xl font-semibold mb-5">Solution 3</h1><div class="tags-wrap h-max space-x-8"><div class="tags"></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Taken from here - <a href="https://web.archive.org/web/20140929181245/http://careerbaba.in/2014/01/introduction-to-time-complexity-of-an-algorithm/" rel="nofollow noreferrer">Introduction to Time Complexity of an Algorithm</a></p>
<h2>1. Introduction</h2>
<p>In computer science, the time complexity of an algorithm quantifies the amount of time taken by an algorithm to run as a function of the length of the string representing the input.</p>
<h2>2. Big O notation</h2>
<p>The time complexity of an algorithm is commonly expressed using big O notation, which excludes coefficients and lower order terms. When expressed this way, the time complexity is said to be described asymptotically, i.e., as the input size goes to infinity.</p>
<p>For example, if the time required by an algorithm on all inputs of size n is at most 5n<sup>3</sup> + 3n, the asymptotic time complexity is O(n<sup>3</sup>). More on that later.</p>
<p>A few more examples:</p>
<ul>
<li>1 = O(n)</li>
<li>n = O(n<sup>2</sup>)</li>
<li>log(n) = O(n)</li>
<li>2 n + 1 = O(n)</li>
</ul>
<h2>3. O(1) constant time:</h2>
<p>An algorithm is said to run in constant time if it requires the same amount of time regardless of the input size.</p>
<p>Examples:</p>
<ul>
<li>array: accessing any element</li>
<li>fixed-size stack: push and pop methods</li>
<li>fixed-size queue: enqueue and dequeue methods</li>
</ul>
<h2>4. O(n) linear time</h2>
<p>An algorithm is said to run in linear time if its time execution is directly proportional to the input size, i.e. time grows linearly as input size increases.</p>
<p>Consider the following examples. Below I am linearly searching for an element, and this has a time complexity of O(n).</p>
<pre><code>int find = 66;
var numbers = new int[] { 33, 435, 36, 37, 43, 45, 66, 656, 2232 };
for (int i = 0; i &lt; numbers.Length - 1; i++)
{
    if(find == numbers[i])
    {
        return;
    }
}
</code></pre>
<p>More Examples:</p>
<ul>
<li>Array: Linear Search, Traversing, Find minimum etc</li>
<li>ArrayList: contains method</li>
<li>Queue: contains method</li>
</ul>
<h2>5. O(log n) logarithmic time:</h2>
<p>An algorithm is said to run in logarithmic time if its time execution is proportional to the logarithm of the input size.</p>
<p>Example: <a href="http://en.wikipedia.org/wiki/Binary_search" rel="nofollow noreferrer">Binary Search</a></p>
<p>Recall the "twenty questions" game - the task is to guess the value of a hidden number in an interval. Each time you make a guess, you are told whether your guess is too high or too low. Twenty questions game implies a strategy that uses your guess number to halve the interval size. This is an example of the general problem-solving method known as binary search.</p>
<h2>6. O(n<sup>2</sup>) quadratic time</h2>
<p>An algorithm is said to run in quadratic time if its time execution is proportional to the square of the input size.</p>
<p>Examples:</p>
<ul>
<li><a href="http://en.wikipedia.org/wiki/Bubble_sort" rel="nofollow noreferrer">Bubble Sort</a></li>
<li><a href="http://en.wikipedia.org/wiki/Selection_sort" rel="nofollow noreferrer">Selection Sort</a></li>
<li><a href="http://en.wikipedia.org/wiki/Insertion_Sort" rel="nofollow noreferrer">Insertion Sort</a></li>
</ul>
<h2>7. Some useful links</h2>
<ul>
<li><a href="http://ssp.impulsetrain.com/big-o.html" rel="nofollow noreferrer">Big-O Misconceptions</a></li>
<li><a href="http://philipstel.wordpress.com/2011/03/07/determining-the-complexity-of-an-algorithm-the-basic-part/" rel="nofollow noreferrer">Determining The Complexity Of Algorithm</a></li>
<li><a href="http://bigocheatsheet.com/" rel="nofollow noreferrer">Big O Cheat Sheet</a></li>
</ul>
    </div></div></div></div><div id="solution4" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h1 class="text-4xl font-semibold mb-5">Solution 4</h1><div class="tags-wrap h-max space-x-8"><div class="tags"></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Several examples of <em>loop</em>.</p>
<ul>
<li><p><strong>O(n)</strong> <em>time complexity</em> of a loop is considered as <em>O(n)</em> if the loop variables is incremented / decremented by a constant amount. For example following functions have <em>O(n)</em> time complexity.</p>
<pre><code>  // Here c is a positive integer constant
  for (int i = 1; i &lt;= n; i += c) {
      // some O(1) expressions
  }

  for (int i = n; i &gt; 0; i -= c) {
      // some O(1) expressions
  }
</code></pre>
</li>
<li><p><strong>O(n<sup>c</sup>)</strong> time complexity of nested loops is equal to the number of times the innermost statement is executed. For example, the following sample loops have <em>O(n<sup>2</sup>)</em> time complexity</p>
<pre><code>  for (int i = 1; i &lt;=n; i += c) {
     for (int j = 1; j &lt;=n; j += c) {
        // some O(1) expressions
     }
  }

  for (int i = n; i &gt; 0; i += c) {
     for (int j = i+1; j &lt;=n; j += c) {
        // some O(1) expressions
  }
</code></pre>
<p>For example, <a href="https://en.wikipedia.org/wiki/Selection_sort" rel="nofollow noreferrer">selection sort</a> and <a href="https://en.wikipedia.org/wiki/Insertion_sort" rel="nofollow noreferrer">insertion sort</a> have <em>O(n<sup>2</sup>)</em> time complexity.</p>
</li>
<li><p><strong>O(log n)</strong> time complexity of a loop is considered as <em>O(log&nbsp;n)</em> if the loop variables is divided / multiplied by a constant amount.</p>
<pre><code>  for (int i = 1; i &lt;=n; i *= c) {
     // some O(1) expressions
  }
  for (int i = n; i &gt; 0; i /= c) {
     // some O(1) expressions
  }

For example, [binary search][3] has _O(log&amp;nbsp;n)_ time complexity.
</code></pre>
</li>
<li><p><strong>O(log log n)</strong> time complexity of a loop is considered as <em>O(log&nbsp;log&nbsp;n)</em> if the loop variables is reduced / increased exponentially by a constant amount.</p>
<pre><code>  // Here c is a constant greater than 1
  for (int i = 2; i &lt;=n; i = pow(i, c)) {
     // some O(1) expressions
  }
  //Here fun is sqrt or cuberoot or any other constant root
  for (int i = n; i &gt; 0; i = fun(i)) {
     // some O(1) expressions
  }
</code></pre>
</li>
</ul>
<hr>
<p>One example of time complexity analysis</p>
<pre><code>int fun(int n)
{
    for (int i = 1; i &lt;= n; i++)
    {
        for (int j = 1; j &lt; n; j += i)
        {
            // Some O(1) task
        }
    }
}
</code></pre>
<p><strong>Analysis</strong>:</p>
<pre><code>For i = 1, the inner loop is executed n times.
For i = 2, the inner loop is executed approximately n/2 times.
For i = 3, the inner loop is executed approximately n/3 times.
For i = 4, the inner loop is executed approximately n/4 times.
.
For i = n, the inner loop is executed approximately n/n times.
</code></pre>
<p>So the total time complexity of the above algorithm is <code>(n + n/2 + n/3 +  + n/n)</code>, which becomes <code>n * (1/1 + 1/2 + 1/3 +  + 1/n)</code></p>
<p>The important thing about series <code>(1/1 + 1/2 + 1/3 +  + 1/n)</code> is around to <em>O(log&nbsp;n)</em>. So the time complexity of the above code is <em>O(n·log&nbsp;n)</em>.</p>
<hr>
<p>References:</p>
<p><a href="http://discrete.gr/complexity/" rel="nofollow noreferrer">1</a>
<a href="http://www.geeksforgeeks.org/analysis-of-algorithms-set-4-analysis-of-loops/" rel="nofollow noreferrer">2</a>
<a href="http://www.geeksforgeeks.org/interesting-time-complexity-question/" rel="nofollow noreferrer">3</a></p>
    </div></div></div></div><div id="solution5" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h1 class="text-4xl font-semibold mb-5">Solution 5</h1><div class="tags-wrap h-max space-x-8"><div class="tags"></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<h2>Time complexity with examples</h2>
<p>1 - Basic operations (arithmetic, comparisons, accessing arrays elements, assignment): The running time is always constant O(1)</p>
<p>Example:</p>
<pre><code>read(x)                               // O(1)
a = 10;                               // O(1)
a = 1,000,000,000,000,000,000         // O(1)
</code></pre>
<p>2 - If then else statement: Only taking the maximum running time from two or more possible statements.</p>
<p>Example:</p>
<pre><code>age = read(x)                               // (1+1) = 2
if age &lt; 17 then begin                      // 1
      status = "Not allowed!";              // 1
end else begin
      status = "Welcome! Please come in";   // 1
      visitors = visitors + 1;              // 1+1 = 2
end;
</code></pre>
<p>So, the complexity of the above pseudo code is T(n) = 2 + 1 + max(1, 1+2) = 6. Thus, its big oh is still constant T(n) = O(1).</p>
<p>3 - Looping (<em>for</em>, <em>while</em>, <em>repeat</em>): Running time for this statement is the number of loops multiplied by the number of operations inside that looping.</p>
<p>Example:</p>
<pre><code>total = 0;                                  // 1
for i = 1 to n do begin                     // (1+1)*n = 2n
      total = total + i;                    // (1+1)*n = 2n
end;
writeln(total);                             // 1
</code></pre>
<p>So, its complexity is T(n) = 1+4n+1 = 4n + 2. Thus, T(n) = O(n).</p>
<p>4 - Nested loop (looping inside looping): Since there is at least one looping inside the main looping, running time of this statement used O(n^2) or O(n^3).</p>
<p>Example:</p>
<pre><code>for i = 1 to n do begin                     // (1+1)*n  = 2n
   for j = 1 to n do begin                  // (1+1)n*n = 2n^2
       x = x + 1;                           // (1+1)n*n = 2n^2
       print(x);                            // (n*n)    = n^2
   end;
end;
</code></pre>
<h2>Common running time</h2>
<p>There are some common running times when analyzing an algorithm:</p>
<ol>
<li><p>O(1)  Constant time</p>
<p>Constant time means the running time is constant, its <em>not affected by the input size</em>.</p>
</li>
<li><p>O(n)  Linear time</p>
<p>When an algorithm accepts n input size, it would perform n operations as well.</p>
</li>
<li><p>O(log n)  Logarithmic time</p>
<p>Algorithm that has running time O(log n) is slight faster than O(n). Commonly, algorithm divides the problem into sub problems with the same size. Example: binary search algorithm, binary conversion algorithm.</p>
</li>
<li><p>O(n log n)  <a href="https://en.wiktionary.org/wiki/linearithmic#Adjective" rel="nofollow noreferrer">Linearithmic</a> time</p>
<p>This running time is often found in "divide &amp; conquer algorithms" which divide the problem into sub problems recursively and then merge them in n time. Example: Merge Sort algorithm.</p>
</li>
<li><p>O(n<sup>2</sup>)  Quadratic time</p>
<p>Look Bubble Sort algorithm!</p>
</li>
<li><p>O(n<sup>3</sup>)  Cubic time</p>
<p>It has the same principle with O(n<sup>2</sup>).</p>
</li>
<li><p>O(2<sup>n</sup>)  Exponential time</p>
<p>It is very slow as input get larger, if n = 1,000,000, T(n) would be 21,000,000. Brute Force algorithm has this running time.</p>
</li>
<li><p>O(n!)  Factorial time</p>
<p><em>The slowest</em>!!! Example: <a href="https://en.wikipedia.org/wiki/Travelling_salesman_problem" rel="nofollow noreferrer">Travelling salesman problem</a> (TSP)</p>
</li>
</ol>
<p>It is taken from <a href="http://philipstel.wordpress.com/2011/03/07/determining-the-complexity-of-an-algorithm-the-basic-part/" rel="nofollow noreferrer">this article</a>. It is very well explained and you should give it a read.</p>
    </div></div></div></div><div id="solution6" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h1 class="text-4xl font-semibold mb-5">Solution 6</h1><div class="tags-wrap h-max space-x-8"><div class="tags"></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>When you're analyzing  code, you have to analyse it line by line, counting every operation/recognizing time complexity. In the end, you have to sum it to get whole picture.</p>
<p>For example, you can have one simple loop with <em>linear complexity</em>, but later in that same program you can have a triple loop that has <em>cubic complexity</em>, so your program will have <strong>cubic complexity</strong>. Function order of growth comes into play right here.</p>
<p>Let's look at what are possibilities for time complexity of an algorithm, you can see order of growth I mentioned above:</p>
<ul>
<li><p><em><strong>Constant time</strong></em> has an order of growth <em>1</em>, for example: <em>a = b + c</em>.</p>
</li>
<li><p><em><strong>Logarithmic time</strong></em> has an order of growth <em>log N</em>. It usually occurs when you're dividing something in half (<a href="https://en.wikipedia.org/wiki/Binary_search_algorithm" rel="nofollow noreferrer">binary search</a>, trees, and even loops), or multiplying something in same way.</p>
</li>
<li><p><em><strong>Linear</strong></em>. The order of growth is <em>N</em>, for example</p>
<pre><code> int p = 0;
 for (int i = 1; i &lt; N; i++)
   p = p + 2;
</code></pre>
</li>
<li><p><em><strong><a href="https://en.wiktionary.org/wiki/linearithmic#Adjective" rel="nofollow noreferrer">Linearithmic</a></strong></em>. The order of growth is <em>n·log N</em>. It usually occurs in <a href="https://en.wikipedia.org/wiki/Divide-and-conquer_algorithm" rel="nofollow noreferrer">divide-and-conquer algorithms</a>.</p>
</li>
<li><p><em><strong>Cubic</strong></em>. The order of growth is <em>N<sup>3</sup></em>. A classic example is a triple loop where you check all triplets:</p>
<pre><code> int x = 0;
 for (int i = 0; i &lt; N; i++)
    for (int j = 0; j &lt; N; j++)
       for (int k = 0; k &lt; N; k++)
           x = x + 2
</code></pre>
</li>
<li><p><em><strong>Exponential</strong></em>. The order of growth is 2<sup>N</sup>. It usually occurs when you do exhaustive search, for example, check subsets of some set.</p>
</li>
</ul>
    </div></div></div></div><div id="solution7" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h1 class="text-4xl font-semibold mb-5">Solution 7</h1><div class="tags-wrap h-max space-x-8"><div class="tags"></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Loosely speaking, time complexity is a way of summarising how the number of operations or run-time of an algorithm grows as the input size increases.</p>

<p>Like most things in life, a cocktail party can help us understand.</p>

<p><strong>O(N)</strong></p>

<p>When you arrive at the party, you have to shake everyone's hand (do an operation on every item). As the number of attendees <code>N</code> increases, the time/work it will take you to shake everyone's hand increases as <code>O(N)</code>.</p>

<p><strong>Why <code>O(N)</code> and not <code>cN</code>?</strong></p>

<p>There's variation in the amount of time it takes to shake hands with people. You could average this out and capture it in a constant <code>c</code>. But the fundamental operation here --- shaking hands with everyone --- would always be proportional to <code>O(N)</code>, no matter what <code>c</code> was. When debating whether we should go to a cocktail party, we're often more interested in the fact that we'll have to meet everyone than in the minute details of what those meetings look like.</p>

<p><strong>O(N^2)</strong></p>

<p>The host of the cocktail party wants you to play a silly game where everyone meets everyone else. Therefore, you must meet <code>N-1</code> other people and, because the next person has already met you, they must meet <code>N-2</code> people, and so on. The sum of this series is <code>x^2/2+x/2</code>. As the number of attendees grows, the <code>x^2</code> term gets big <em>fast</em>, so we just drop everything else.</p>

<p><strong>O(N^3)</strong></p>

<p>You have to meet everyone else and, during each meeting, you must talk about everyone else in the room.</p>

<p><strong>O(1)</strong></p>

<p>The host wants to announce something. They ding a wineglass and speak loudly. Everyone hears them. It turns out it doesn't matter how many attendees there are, this operation always takes the same amount of time.</p>

<p><strong>O(log N)</strong></p>

<p>The host has laid everyone out at the table in alphabetical order. Where is Dan? You reason that he must be somewhere between Adam and Mandy (certainly not between Mandy and Zach!). Given that, is he between George and Mandy? No. He must be between Adam and Fred, and between Cindy and Fred. And so on... we can efficiently locate Dan by looking at half the set and then half of that set. Ultimately, we look at <strong>O(log_2 N)</strong> individuals.</p>

<p><strong>O(N log N)</strong></p>

<p>You could find where to sit down at the table using the algorithm above. If a large number of people came to the table, one at a time, and all did this, that would take <strong>O(N log N)</strong> time. This turns out to be how long it takes to sort any collection of items when they must be compared.</p>

<p><strong>Best/Worst Case</strong></p>

<p>You arrive at the party and need to find Inigo - how long will it take? It depends on when you arrive. If everyone is milling around you've hit the worst-case: it will take <code>O(N)</code> time. However, if everyone is sitting down at the table, it will take only <code>O(log N)</code> time. Or maybe you can leverage the host's wineglass-shouting power and it will take only <code>O(1)</code> time.</p>

<p>Assuming the host is unavailable, we can say that the Inigo-finding algorithm has a lower-bound of <code>O(log N)</code> and an upper-bound of <code>O(N)</code>, depending on the state of the party when you arrive.</p>

<p><strong>Space &amp; Communication</strong></p>

<p>The same ideas can be applied to understanding how algorithms use space or communication.</p>

<p>Knuth has written a nice paper about the former entitled <a href="https://www.cs.utexas.edu/users/arvindn/misc/knuth_song_complexity.pdf">"The Complexity of Songs"</a>.</p>

<blockquote>
  <p>Theorem 2: There exist arbitrarily long songs of complexity O(1).</p>
  
  <p>PROOF: (due to Casey and the Sunshine Band). Consider the songs Sk defined by (15), but with</p>
</blockquote>

<pre><code>V_k = 'That's the way,' U 'I like it, ' U
U   = 'uh huh,' 'uh huh'
</code></pre>

<blockquote>
  <p>for all k. </p>
</blockquote>
    </div></div></div></div><div id="solution8" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h1 class="text-4xl font-semibold mb-5">Solution 8</h1><div class="tags-wrap h-max space-x-8"><div class="tags"></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>For the mathematically-minded people: The <a href="https://en.wikipedia.org/wiki/Master_theorem_(analysis_of_algorithms)" rel="nofollow noreferrer">master theorem</a> is another useful thing to know when studying complexity.</p>
    </div></div></div></div><div id="solution9" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h1 class="text-4xl font-semibold mb-5">Solution 9</h1><div class="tags-wrap h-max space-x-8"><div class="tags"></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>O(n) is big O notation used for writing time complexity of an algorithm. When you add up the number of executions in an algorithm, you'll get an expression in result like 2N+2. In this expression, N is the dominating term (the term having largest effect on expression if its value increases or decreases). Now O(N) is the time complexity while N is dominating term.</p>
<h3>Example</h3>
<pre><code>For i = 1 to n;
  j = 0;
while(j &lt;= n);
  j = j + 1;
</code></pre>
<p>Here the total number of executions for the inner loop are n+1 and the total number of executions for the outer loop are n(n+1)/2, so the total number of executions for the whole algorithm are n + 1 + n(n+1/2) = (n<sub>2</sub> + 3n)/2.
Here n^2 is the dominating term so the time complexity for this algorithm is O(n<sub>2</sub>).</p>
    </div></div></div></div></div></div><div class="widget"><a href="/questions/javascript-closure-inside-loops-simple-practical-example-1657384278449">JavaScript closure inside loops – simple practical example</a><a href="/questions/what-is-the-explicit-promise-construction-antipattern-and-how-do-i-avoid-it-1657384436212">What is the explicit promise construction antipattern and how do I avoid it?</a><a href="/questions/how-can-i-tell-if-a-dom-element-is-visible-in-the-current-viewport-1657388316510">How can I tell if a DOM element is visible in the current viewport?</a><a href="/questions/how-do-i-access-previous-promise-results-in-a-.then()-chain-1657387705386">How do I access previous promise results in a .then() chain?</a><a href="/questions/when-does-sqliteopenhelper-oncreate()-onupgrade()-run-1657384883864">When does SQLiteOpenHelper onCreate() / onUpgrade() run?</a><a href="/questions/why-does-my-javascript-code-receive-a-%22no-&#x27;access-control-allow-origin&#x27;-header-is-present-on-the-requested-resource%22-error-while-postman-does-not-1657384636363">Why does my JavaScript code receive a &quot;No &#x27;Access-Control-Allow-Origin&#x27; header is present on the requested resource&quot; error, while Postman does not?</a><a href="/questions/why-should-i-not-include-lessbitsstdc++.hgreater-1657384470484">Why should I not #include &lt;bits/stdc++.h&gt;?</a><a href="/questions/why-is-using-&#x27;eval&#x27;-a-bad-practice-1657387348760">Why is using &#x27;eval&#x27; a bad practice?</a><a href="/questions/how-can-i-access-an-arrayobject-1657387395541">How can I access an array/object?</a><a href="/questions/how-do-i-return-the-response-from-an-asynchronous-call-1657384208012">How do I return the response from an asynchronous call?</a><a href="/questions/how-can-i-convert-ereg-expressions-to-preg-in-php-1657387652855">How can I convert ereg expressions to preg in PHP?</a><a href="/questions/convert-columns-into-rows-with-pandas-1657388374964">Convert columns into rows with Pandas</a><a href="/questions/how-to-return-datasnapshot-value-as-a-result-of-a-method-1657387538165">How to return DataSnapshot value as a result of a method?</a><a href="/questions/how-do-i-add-a-delay-in-a-javascript-loop-1657388544679">How do I add a delay in a JavaScript loop?</a><a href="/questions/how-to-check-whether-a-string-contains-a-substring-in-javascript-1657388080698">How to check whether a string contains a substring in JavaScript?</a><a href="/questions/list-of-lists-changes-reflected-across-sublists-unexpectedly-1657384393720">List of lists changes reflected across sublists unexpectedly</a><a href="/questions/how-do-i-get-php-errors-to-display-1657384570095">How do I get PHP errors to display?</a><a href="/questions/can-you-provide-some-examples-of-why-it-is-hard-to-parse-xml-and-html-with-a-regex-closed-1657388410824">Can you provide some examples of why it is hard to parse XML and HTML with a regex? [closed]</a><a href="/questions/why-do-regex-constructors-need-to-be-double-escaped-1657388004942">Why do regex constructors need to be double escaped?</a><a href="/questions/how-to-use-java.util.scanner-to-correctly-read-user-input-from-system.in-and-act-on-it-1657388546306">How to use java.util.Scanner to correctly read user input from System.in and act on it?</a></div></div><span class="cursor-pointer text-lg p-2" style="position:fixed;bottom:20px;left:20px;background:#000;z-index:2000;color:white">Go go top</span></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"data":{"answer":["\n\u0026lt;blockquote\u0026gt;\n  \u0026lt;p\u0026gt;How to find time complexity of an algorithm\u0026lt;/p\u0026gt;\n\u0026lt;/blockquote\u0026gt;\n\n\u0026lt;p\u0026gt;You add up how many machine instructions it will execute as a function of the size of its input, and then simplify the expression to the largest (when N is very large) term  and can include any simplifying constant factor.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;For example, lets see how we simplify \u0026lt;code\u0026gt;2N + 2\u0026lt;/code\u0026gt; machine instructions to describe this as just \u0026lt;code\u0026gt;O(N)\u0026lt;/code\u0026gt;.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Why do we remove the two \u0026lt;code\u0026gt;2\u0026lt;/code\u0026gt;s ?\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;We are interested in the performance of the algorithm as N becomes large.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Consider the two terms 2N and 2. \u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;What is the relative influence of these two terms as N becomes large? Suppose N is a million.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Then the first term is 2 million and the second term is only 2.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;For this reason, we drop all but the largest terms for large N.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;So, now we have gone from \u0026lt;code\u0026gt;2N + 2\u0026lt;/code\u0026gt; to \u0026lt;code\u0026gt;2N\u0026lt;/code\u0026gt;.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Traditionally, we are only interested in performance \u0026lt;em\u0026gt;up to constant factors\u0026lt;/em\u0026gt;. \u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;This means that we don\u0026apos;t really care if there is some constant multiple of difference in performance when N is large.  The unit of 2N is not well-defined in the first place anyway.  So we can multiply or divide by a constant factor to get to the simplest expression.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;So \u0026lt;code\u0026gt;2N\u0026lt;/code\u0026gt; becomes just \u0026lt;code\u0026gt;N\u0026lt;/code\u0026gt;.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;This is an excellent article: \u0026lt;em\u0026gt;\u0026lt;a href=\u0026quot;http://www.daniweb.com/software-development/computer-science/threads/13488/time-complexity-of-algorithm\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;Time complexity of algorithm\u0026lt;/a\u0026gt;\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;The below answer is copied from above (in case the excellent link goes bust)\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;The most common metric for calculating time complexity is Big O notation. This removes all constant factors so that the running time can be estimated in relation to N as N approaches infinity. In general you can think of it like this:\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;statement;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;Is constant. The running time of the statement will not change in relation to N.\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;for ( i = 0; i \u0026amp;lt; N; i++ )\n     statement;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;Is linear. The running time of the loop is directly proportional to N. When N doubles, so does the running time.\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;for ( i = 0; i \u0026amp;lt; N; i++ ) {\n  for ( j = 0; j \u0026amp;lt; N; j++ )\n    statement;\n}\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;Is quadratic. The running time of the two loops is proportional to the square of N. When N doubles, the running time increases by N * N.\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;while ( low \u0026amp;lt;= high ) {\n  mid = ( low + high ) / 2;\n  if ( target \u0026amp;lt; list[mid] )\n    high = mid - 1;\n  else if ( target \u0026amp;gt; list[mid] )\n    low = mid + 1;\n  else break;\n}\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;Is logarithmic. The running time of the algorithm is proportional to the number of times N can be divided by 2. This is because the algorithm divides the working area in half with each iteration.\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;void quicksort (int list[], int left, int right)\n{\n  int pivot = partition (list, left, right);\n  quicksort(list, left, pivot - 1);\n  quicksort(list, pivot + 1, right);\n}\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;Is N * log (N). The running time consists of N loops (iterative or recursive) that are logarithmic, thus the algorithm is a combination of linear and logarithmic.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;In general, doing something with every item in one dimension is linear, doing something with every item in two dimensions is quadratic, and dividing the working area in half is logarithmic. There are other Big O measures such as cubic, exponential, and square root, but they\u0026apos;re not nearly as common. Big O notation is described as \u0026lt;code\u0026gt;O ( \u0026amp;lt;type\u0026amp;gt; )\u0026lt;/code\u0026gt; where \u0026lt;code\u0026gt;\u0026amp;lt;type\u0026amp;gt;\u0026lt;/code\u0026gt; is the measure. The quicksort algorithm would be described as \u0026lt;code\u0026gt;O (N * log(N ))\u0026lt;/code\u0026gt;.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Note that none of this has taken into account best, average, and worst case measures. Each would have its own Big O notation. Also note that this is a VERY simplistic explanation. Big O is the most common, but it\u0026apos;s also more complex that I\u0026apos;ve shown. There are also other notations such as big omega, little o, and big theta. You probably won\u0026apos;t encounter them outside of an algorithm analysis course. ;)\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Taken from here - \u0026lt;a href=\u0026quot;https://web.archive.org/web/20140929181245/http://careerbaba.in/2014/01/introduction-to-time-complexity-of-an-algorithm/\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;Introduction to Time Complexity of an Algorithm\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;h2\u0026gt;1. Introduction\u0026lt;/h2\u0026gt;\n\u0026lt;p\u0026gt;In computer science, the time complexity of an algorithm quantifies the amount of time taken by an algorithm to run as a function of the length of the string representing the input.\u0026lt;/p\u0026gt;\n\u0026lt;h2\u0026gt;2. Big O notation\u0026lt;/h2\u0026gt;\n\u0026lt;p\u0026gt;The time complexity of an algorithm is commonly expressed using big O notation, which excludes coefficients and lower order terms. When expressed this way, the time complexity is said to be described asymptotically, i.e., as the input size goes to infinity.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;For example, if the time required by an algorithm on all inputs of size n is at most 5n\u0026lt;sup\u0026gt;3\u0026lt;/sup\u0026gt; + 3n, the asymptotic time complexity is O(n\u0026lt;sup\u0026gt;3\u0026lt;/sup\u0026gt;). More on that later.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;A few more examples:\u0026lt;/p\u0026gt;\n\u0026lt;ul\u0026gt;\n\u0026lt;li\u0026gt;1 = O(n)\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;n = O(n\u0026lt;sup\u0026gt;2\u0026lt;/sup\u0026gt;)\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;log(n) = O(n)\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;2 n + 1 = O(n)\u0026lt;/li\u0026gt;\n\u0026lt;/ul\u0026gt;\n\u0026lt;h2\u0026gt;3. O(1) constant time:\u0026lt;/h2\u0026gt;\n\u0026lt;p\u0026gt;An algorithm is said to run in constant time if it requires the same amount of time regardless of the input size.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Examples:\u0026lt;/p\u0026gt;\n\u0026lt;ul\u0026gt;\n\u0026lt;li\u0026gt;array: accessing any element\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;fixed-size stack: push and pop methods\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;fixed-size queue: enqueue and dequeue methods\u0026lt;/li\u0026gt;\n\u0026lt;/ul\u0026gt;\n\u0026lt;h2\u0026gt;4. O(n) linear time\u0026lt;/h2\u0026gt;\n\u0026lt;p\u0026gt;An algorithm is said to run in linear time if its time execution is directly proportional to the input size, i.e. time grows linearly as input size increases.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Consider the following examples. Below I am linearly searching for an element, and this has a time complexity of O(n).\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;int find = 66;\nvar numbers = new int[] { 33, 435, 36, 37, 43, 45, 66, 656, 2232 };\nfor (int i = 0; i \u0026amp;lt; numbers.Length - 1; i++)\n{\n    if(find == numbers[i])\n    {\n        return;\n    }\n}\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;More Examples:\u0026lt;/p\u0026gt;\n\u0026lt;ul\u0026gt;\n\u0026lt;li\u0026gt;Array: Linear Search, Traversing, Find minimum etc\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;ArrayList: contains method\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;Queue: contains method\u0026lt;/li\u0026gt;\n\u0026lt;/ul\u0026gt;\n\u0026lt;h2\u0026gt;5. O(log n) logarithmic time:\u0026lt;/h2\u0026gt;\n\u0026lt;p\u0026gt;An algorithm is said to run in logarithmic time if its time execution is proportional to the logarithm of the input size.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Example: \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Binary_search\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;Binary Search\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Recall the \u0026quot;twenty questions\u0026quot; game - the task is to guess the value of a hidden number in an interval. Each time you make a guess, you are told whether your guess is too high or too low. Twenty questions game implies a strategy that uses your guess number to halve the interval size. This is an example of the general problem-solving method known as binary search.\u0026lt;/p\u0026gt;\n\u0026lt;h2\u0026gt;6. O(n\u0026lt;sup\u0026gt;2\u0026lt;/sup\u0026gt;) quadratic time\u0026lt;/h2\u0026gt;\n\u0026lt;p\u0026gt;An algorithm is said to run in quadratic time if its time execution is proportional to the square of the input size.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Examples:\u0026lt;/p\u0026gt;\n\u0026lt;ul\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Bubble_sort\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;Bubble Sort\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Selection_sort\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;Selection Sort\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Insertion_Sort\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;Insertion Sort\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;/ul\u0026gt;\n\u0026lt;h2\u0026gt;7. Some useful links\u0026lt;/h2\u0026gt;\n\u0026lt;ul\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;a href=\u0026quot;http://ssp.impulsetrain.com/big-o.html\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;Big-O Misconceptions\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;a href=\u0026quot;http://philipstel.wordpress.com/2011/03/07/determining-the-complexity-of-an-algorithm-the-basic-part/\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;Determining The Complexity Of Algorithm\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;a href=\u0026quot;http://bigocheatsheet.com/\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;Big O Cheat Sheet\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;/ul\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Several examples of \u0026lt;em\u0026gt;loop\u0026lt;/em\u0026gt;.\u0026lt;/p\u0026gt;\n\u0026lt;ul\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;O(n)\u0026lt;/strong\u0026gt; \u0026lt;em\u0026gt;time complexity\u0026lt;/em\u0026gt; of a loop is considered as \u0026lt;em\u0026gt;O(n)\u0026lt;/em\u0026gt; if the loop variables is incremented / decremented by a constant amount. For example following functions have \u0026lt;em\u0026gt;O(n)\u0026lt;/em\u0026gt; time complexity.\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;  // Here c is a positive integer constant\n  for (int i = 1; i \u0026amp;lt;= n; i += c) {\n      // some O(1) expressions\n  }\n\n  for (int i = n; i \u0026amp;gt; 0; i -= c) {\n      // some O(1) expressions\n  }\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;O(n\u0026lt;sup\u0026gt;c\u0026lt;/sup\u0026gt;)\u0026lt;/strong\u0026gt; time complexity of nested loops is equal to the number of times the innermost statement is executed. For example, the following sample loops have \u0026lt;em\u0026gt;O(n\u0026lt;sup\u0026gt;2\u0026lt;/sup\u0026gt;)\u0026lt;/em\u0026gt; time complexity\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;  for (int i = 1; i \u0026amp;lt;=n; i += c) {\n     for (int j = 1; j \u0026amp;lt;=n; j += c) {\n        // some O(1) expressions\n     }\n  }\n\n  for (int i = n; i \u0026amp;gt; 0; i += c) {\n     for (int j = i+1; j \u0026amp;lt;=n; j += c) {\n        // some O(1) expressions\n  }\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;For example, \u0026lt;a href=\u0026quot;https://en.wikipedia.org/wiki/Selection_sort\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;selection sort\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026quot;https://en.wikipedia.org/wiki/Insertion_sort\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;insertion sort\u0026lt;/a\u0026gt; have \u0026lt;em\u0026gt;O(n\u0026lt;sup\u0026gt;2\u0026lt;/sup\u0026gt;)\u0026lt;/em\u0026gt; time complexity.\u0026lt;/p\u0026gt;\n\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;O(log n)\u0026lt;/strong\u0026gt; time complexity of a loop is considered as \u0026lt;em\u0026gt;O(log\u0026amp;nbsp;n)\u0026lt;/em\u0026gt; if the loop variables is divided / multiplied by a constant amount.\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;  for (int i = 1; i \u0026amp;lt;=n; i *= c) {\n     // some O(1) expressions\n  }\n  for (int i = n; i \u0026amp;gt; 0; i /= c) {\n     // some O(1) expressions\n  }\n\nFor example, [binary search][3] has _O(log\u0026amp;amp;nbsp;n)_ time complexity.\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;O(log log n)\u0026lt;/strong\u0026gt; time complexity of a loop is considered as \u0026lt;em\u0026gt;O(log\u0026amp;nbsp;log\u0026amp;nbsp;n)\u0026lt;/em\u0026gt; if the loop variables is reduced / increased exponentially by a constant amount.\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;  // Here c is a constant greater than 1\n  for (int i = 2; i \u0026amp;lt;=n; i = pow(i, c)) {\n     // some O(1) expressions\n  }\n  //Here fun is sqrt or cuberoot or any other constant root\n  for (int i = n; i \u0026amp;gt; 0; i = fun(i)) {\n     // some O(1) expressions\n  }\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;/li\u0026gt;\n\u0026lt;/ul\u0026gt;\n\u0026lt;hr\u0026gt;\n\u0026lt;p\u0026gt;One example of time complexity analysis\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;int fun(int n)\n{\n    for (int i = 1; i \u0026amp;lt;= n; i++)\n    {\n        for (int j = 1; j \u0026amp;lt; n; j += i)\n        {\n            // Some O(1) task\n        }\n    }\n}\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Analysis\u0026lt;/strong\u0026gt;:\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;For i = 1, the inner loop is executed n times.\nFor i = 2, the inner loop is executed approximately n/2 times.\nFor i = 3, the inner loop is executed approximately n/3 times.\nFor i = 4, the inner loop is executed approximately n/4 times.\n.\nFor i = n, the inner loop is executed approximately n/n times.\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;So the total time complexity of the above algorithm is \u0026lt;code\u0026gt;(n + n/2 + n/3 +  + n/n)\u0026lt;/code\u0026gt;, which becomes \u0026lt;code\u0026gt;n * (1/1 + 1/2 + 1/3 +  + 1/n)\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;The important thing about series \u0026lt;code\u0026gt;(1/1 + 1/2 + 1/3 +  + 1/n)\u0026lt;/code\u0026gt; is around to \u0026lt;em\u0026gt;O(log\u0026amp;nbsp;n)\u0026lt;/em\u0026gt;. So the time complexity of the above code is \u0026lt;em\u0026gt;O(n·log\u0026amp;nbsp;n)\u0026lt;/em\u0026gt;.\u0026lt;/p\u0026gt;\n\u0026lt;hr\u0026gt;\n\u0026lt;p\u0026gt;References:\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;http://discrete.gr/complexity/\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;1\u0026lt;/a\u0026gt;\n\u0026lt;a href=\u0026quot;http://www.geeksforgeeks.org/analysis-of-algorithms-set-4-analysis-of-loops/\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;2\u0026lt;/a\u0026gt;\n\u0026lt;a href=\u0026quot;http://www.geeksforgeeks.org/interesting-time-complexity-question/\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;3\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;h2\u0026gt;Time complexity with examples\u0026lt;/h2\u0026gt;\n\u0026lt;p\u0026gt;1 - Basic operations (arithmetic, comparisons, accessing arrays elements, assignment): The running time is always constant O(1)\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Example:\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;read(x)                               // O(1)\na = 10;                               // O(1)\na = 1,000,000,000,000,000,000         // O(1)\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;2 - If then else statement: Only taking the maximum running time from two or more possible statements.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Example:\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;age = read(x)                               // (1+1) = 2\nif age \u0026amp;lt; 17 then begin                      // 1\n      status = \u0026quot;Not allowed!\u0026quot;;              // 1\nend else begin\n      status = \u0026quot;Welcome! Please come in\u0026quot;;   // 1\n      visitors = visitors + 1;              // 1+1 = 2\nend;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;So, the complexity of the above pseudo code is T(n) = 2 + 1 + max(1, 1+2) = 6. Thus, its big oh is still constant T(n) = O(1).\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;3 - Looping (\u0026lt;em\u0026gt;for\u0026lt;/em\u0026gt;, \u0026lt;em\u0026gt;while\u0026lt;/em\u0026gt;, \u0026lt;em\u0026gt;repeat\u0026lt;/em\u0026gt;): Running time for this statement is the number of loops multiplied by the number of operations inside that looping.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Example:\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;total = 0;                                  // 1\nfor i = 1 to n do begin                     // (1+1)*n = 2n\n      total = total + i;                    // (1+1)*n = 2n\nend;\nwriteln(total);                             // 1\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;So, its complexity is T(n) = 1+4n+1 = 4n + 2. Thus, T(n) = O(n).\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;4 - Nested loop (looping inside looping): Since there is at least one looping inside the main looping, running time of this statement used O(n^2) or O(n^3).\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Example:\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;for i = 1 to n do begin                     // (1+1)*n  = 2n\n   for j = 1 to n do begin                  // (1+1)n*n = 2n^2\n       x = x + 1;                           // (1+1)n*n = 2n^2\n       print(x);                            // (n*n)    = n^2\n   end;\nend;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;h2\u0026gt;Common running time\u0026lt;/h2\u0026gt;\n\u0026lt;p\u0026gt;There are some common running times when analyzing an algorithm:\u0026lt;/p\u0026gt;\n\u0026lt;ol\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;O(1)  Constant time\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Constant time means the running time is constant, its \u0026lt;em\u0026gt;not affected by the input size\u0026lt;/em\u0026gt;.\u0026lt;/p\u0026gt;\n\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;O(n)  Linear time\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;When an algorithm accepts n input size, it would perform n operations as well.\u0026lt;/p\u0026gt;\n\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;O(log n)  Logarithmic time\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Algorithm that has running time O(log n) is slight faster than O(n). Commonly, algorithm divides the problem into sub problems with the same size. Example: binary search algorithm, binary conversion algorithm.\u0026lt;/p\u0026gt;\n\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;O(n log n)  \u0026lt;a href=\u0026quot;https://en.wiktionary.org/wiki/linearithmic#Adjective\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;Linearithmic\u0026lt;/a\u0026gt; time\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;This running time is often found in \u0026quot;divide \u0026amp;amp; conquer algorithms\u0026quot; which divide the problem into sub problems recursively and then merge them in n time. Example: Merge Sort algorithm.\u0026lt;/p\u0026gt;\n\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;O(n\u0026lt;sup\u0026gt;2\u0026lt;/sup\u0026gt;)  Quadratic time\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Look Bubble Sort algorithm!\u0026lt;/p\u0026gt;\n\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;O(n\u0026lt;sup\u0026gt;3\u0026lt;/sup\u0026gt;)  Cubic time\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;It has the same principle with O(n\u0026lt;sup\u0026gt;2\u0026lt;/sup\u0026gt;).\u0026lt;/p\u0026gt;\n\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;O(2\u0026lt;sup\u0026gt;n\u0026lt;/sup\u0026gt;)  Exponential time\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;It is very slow as input get larger, if n = 1,000,000, T(n) would be 21,000,000. Brute Force algorithm has this running time.\u0026lt;/p\u0026gt;\n\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;O(n!)  Factorial time\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;The slowest\u0026lt;/em\u0026gt;!!! Example: \u0026lt;a href=\u0026quot;https://en.wikipedia.org/wiki/Travelling_salesman_problem\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;Travelling salesman problem\u0026lt;/a\u0026gt; (TSP)\u0026lt;/p\u0026gt;\n\u0026lt;/li\u0026gt;\n\u0026lt;/ol\u0026gt;\n\u0026lt;p\u0026gt;It is taken from \u0026lt;a href=\u0026quot;http://philipstel.wordpress.com/2011/03/07/determining-the-complexity-of-an-algorithm-the-basic-part/\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;this article\u0026lt;/a\u0026gt;. It is very well explained and you should give it a read.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;When you\u0026apos;re analyzing  code, you have to analyse it line by line, counting every operation/recognizing time complexity. In the end, you have to sum it to get whole picture.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;For example, you can have one simple loop with \u0026lt;em\u0026gt;linear complexity\u0026lt;/em\u0026gt;, but later in that same program you can have a triple loop that has \u0026lt;em\u0026gt;cubic complexity\u0026lt;/em\u0026gt;, so your program will have \u0026lt;strong\u0026gt;cubic complexity\u0026lt;/strong\u0026gt;. Function order of growth comes into play right here.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Let\u0026apos;s look at what are possibilities for time complexity of an algorithm, you can see order of growth I mentioned above:\u0026lt;/p\u0026gt;\n\u0026lt;ul\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;\u0026lt;strong\u0026gt;Constant time\u0026lt;/strong\u0026gt;\u0026lt;/em\u0026gt; has an order of growth \u0026lt;em\u0026gt;1\u0026lt;/em\u0026gt;, for example: \u0026lt;em\u0026gt;a = b + c\u0026lt;/em\u0026gt;.\u0026lt;/p\u0026gt;\n\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;\u0026lt;strong\u0026gt;Logarithmic time\u0026lt;/strong\u0026gt;\u0026lt;/em\u0026gt; has an order of growth \u0026lt;em\u0026gt;log N\u0026lt;/em\u0026gt;. It usually occurs when you\u0026apos;re dividing something in half (\u0026lt;a href=\u0026quot;https://en.wikipedia.org/wiki/Binary_search_algorithm\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;binary search\u0026lt;/a\u0026gt;, trees, and even loops), or multiplying something in same way.\u0026lt;/p\u0026gt;\n\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;\u0026lt;strong\u0026gt;Linear\u0026lt;/strong\u0026gt;\u0026lt;/em\u0026gt;. The order of growth is \u0026lt;em\u0026gt;N\u0026lt;/em\u0026gt;, for example\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt; int p = 0;\n for (int i = 1; i \u0026amp;lt; N; i++)\n   p = p + 2;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;\u0026lt;strong\u0026gt;\u0026lt;a href=\u0026quot;https://en.wiktionary.org/wiki/linearithmic#Adjective\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;Linearithmic\u0026lt;/a\u0026gt;\u0026lt;/strong\u0026gt;\u0026lt;/em\u0026gt;. The order of growth is \u0026lt;em\u0026gt;n·log N\u0026lt;/em\u0026gt;. It usually occurs in \u0026lt;a href=\u0026quot;https://en.wikipedia.org/wiki/Divide-and-conquer_algorithm\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;divide-and-conquer algorithms\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\n\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;\u0026lt;strong\u0026gt;Cubic\u0026lt;/strong\u0026gt;\u0026lt;/em\u0026gt;. The order of growth is \u0026lt;em\u0026gt;N\u0026lt;sup\u0026gt;3\u0026lt;/sup\u0026gt;\u0026lt;/em\u0026gt;. A classic example is a triple loop where you check all triplets:\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt; int x = 0;\n for (int i = 0; i \u0026amp;lt; N; i++)\n    for (int j = 0; j \u0026amp;lt; N; j++)\n       for (int k = 0; k \u0026amp;lt; N; k++)\n           x = x + 2\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;\u0026lt;strong\u0026gt;Exponential\u0026lt;/strong\u0026gt;\u0026lt;/em\u0026gt;. The order of growth is 2\u0026lt;sup\u0026gt;N\u0026lt;/sup\u0026gt;. It usually occurs when you do exhaustive search, for example, check subsets of some set.\u0026lt;/p\u0026gt;\n\u0026lt;/li\u0026gt;\n\u0026lt;/ul\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Loosely speaking, time complexity is a way of summarising how the number of operations or run-time of an algorithm grows as the input size increases.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Like most things in life, a cocktail party can help us understand.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;O(N)\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;When you arrive at the party, you have to shake everyone\u0026apos;s hand (do an operation on every item). As the number of attendees \u0026lt;code\u0026gt;N\u0026lt;/code\u0026gt; increases, the time/work it will take you to shake everyone\u0026apos;s hand increases as \u0026lt;code\u0026gt;O(N)\u0026lt;/code\u0026gt;.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Why \u0026lt;code\u0026gt;O(N)\u0026lt;/code\u0026gt; and not \u0026lt;code\u0026gt;cN\u0026lt;/code\u0026gt;?\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;There\u0026apos;s variation in the amount of time it takes to shake hands with people. You could average this out and capture it in a constant \u0026lt;code\u0026gt;c\u0026lt;/code\u0026gt;. But the fundamental operation here --- shaking hands with everyone --- would always be proportional to \u0026lt;code\u0026gt;O(N)\u0026lt;/code\u0026gt;, no matter what \u0026lt;code\u0026gt;c\u0026lt;/code\u0026gt; was. When debating whether we should go to a cocktail party, we\u0026apos;re often more interested in the fact that we\u0026apos;ll have to meet everyone than in the minute details of what those meetings look like.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;O(N^2)\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;The host of the cocktail party wants you to play a silly game where everyone meets everyone else. Therefore, you must meet \u0026lt;code\u0026gt;N-1\u0026lt;/code\u0026gt; other people and, because the next person has already met you, they must meet \u0026lt;code\u0026gt;N-2\u0026lt;/code\u0026gt; people, and so on. The sum of this series is \u0026lt;code\u0026gt;x^2/2+x/2\u0026lt;/code\u0026gt;. As the number of attendees grows, the \u0026lt;code\u0026gt;x^2\u0026lt;/code\u0026gt; term gets big \u0026lt;em\u0026gt;fast\u0026lt;/em\u0026gt;, so we just drop everything else.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;O(N^3)\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;You have to meet everyone else and, during each meeting, you must talk about everyone else in the room.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;O(1)\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;The host wants to announce something. They ding a wineglass and speak loudly. Everyone hears them. It turns out it doesn\u0026apos;t matter how many attendees there are, this operation always takes the same amount of time.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;O(log N)\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;The host has laid everyone out at the table in alphabetical order. Where is Dan? You reason that he must be somewhere between Adam and Mandy (certainly not between Mandy and Zach!). Given that, is he between George and Mandy? No. He must be between Adam and Fred, and between Cindy and Fred. And so on... we can efficiently locate Dan by looking at half the set and then half of that set. Ultimately, we look at \u0026lt;strong\u0026gt;O(log_2 N)\u0026lt;/strong\u0026gt; individuals.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;O(N log N)\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;You could find where to sit down at the table using the algorithm above. If a large number of people came to the table, one at a time, and all did this, that would take \u0026lt;strong\u0026gt;O(N log N)\u0026lt;/strong\u0026gt; time. This turns out to be how long it takes to sort any collection of items when they must be compared.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Best/Worst Case\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;You arrive at the party and need to find Inigo - how long will it take? It depends on when you arrive. If everyone is milling around you\u0026apos;ve hit the worst-case: it will take \u0026lt;code\u0026gt;O(N)\u0026lt;/code\u0026gt; time. However, if everyone is sitting down at the table, it will take only \u0026lt;code\u0026gt;O(log N)\u0026lt;/code\u0026gt; time. Or maybe you can leverage the host\u0026apos;s wineglass-shouting power and it will take only \u0026lt;code\u0026gt;O(1)\u0026lt;/code\u0026gt; time.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Assuming the host is unavailable, we can say that the Inigo-finding algorithm has a lower-bound of \u0026lt;code\u0026gt;O(log N)\u0026lt;/code\u0026gt; and an upper-bound of \u0026lt;code\u0026gt;O(N)\u0026lt;/code\u0026gt;, depending on the state of the party when you arrive.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Space \u0026amp;amp; Communication\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;The same ideas can be applied to understanding how algorithms use space or communication.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Knuth has written a nice paper about the former entitled \u0026lt;a href=\u0026quot;https://www.cs.utexas.edu/users/arvindn/misc/knuth_song_complexity.pdf\u0026quot;\u0026gt;\u0026quot;The Complexity of Songs\u0026quot;\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\n\n\u0026lt;blockquote\u0026gt;\n  \u0026lt;p\u0026gt;Theorem 2: There exist arbitrarily long songs of complexity O(1).\u0026lt;/p\u0026gt;\n  \n  \u0026lt;p\u0026gt;PROOF: (due to Casey and the Sunshine Band). Consider the songs Sk defined by (15), but with\u0026lt;/p\u0026gt;\n\u0026lt;/blockquote\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;V_k = \u0026apos;That\u0026apos;s the way,\u0026apos; U \u0026apos;I like it, \u0026apos; U\nU   = \u0026apos;uh huh,\u0026apos; \u0026apos;uh huh\u0026apos;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;blockquote\u0026gt;\n  \u0026lt;p\u0026gt;for all k. \u0026lt;/p\u0026gt;\n\u0026lt;/blockquote\u0026gt;\n    ","\n\u0026lt;p\u0026gt;For the mathematically-minded people: The \u0026lt;a href=\u0026quot;https://en.wikipedia.org/wiki/Master_theorem_(analysis_of_algorithms)\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;master theorem\u0026lt;/a\u0026gt; is another useful thing to know when studying complexity.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;O(n) is big O notation used for writing time complexity of an algorithm. When you add up the number of executions in an algorithm, you\u0026apos;ll get an expression in result like 2N+2. In this expression, N is the dominating term (the term having largest effect on expression if its value increases or decreases). Now O(N) is the time complexity while N is dominating term.\u0026lt;/p\u0026gt;\n\u0026lt;h3\u0026gt;Example\u0026lt;/h3\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;For i = 1 to n;\n  j = 0;\nwhile(j \u0026amp;lt;= n);\n  j = j + 1;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;Here the total number of executions for the inner loop are n+1 and the total number of executions for the outer loop are n(n+1)/2, so the total number of executions for the whole algorithm are n + 1 + n(n+1/2) = (n\u0026lt;sub\u0026gt;2\u0026lt;/sub\u0026gt; + 3n)/2.\nHere n^2 is the dominating term so the time complexity for this algorithm is O(n\u0026lt;sub\u0026gt;2\u0026lt;/sub\u0026gt;).\u0026lt;/p\u0026gt;\n    "],"id":584,"title":"How can I find the time complexity of an algorithm?","content":"\n                \n\u0026lt;p\u0026gt;I have gone through \u0026lt;a href=\u0026quot;https://www.google.co.in/search?aq=f\u0026amp;amp;sugexp=chrome,mod=19\u0026amp;amp;sourceid=chrome\u0026amp;amp;ie=UTF-8\u0026amp;amp;q=How%20to%20find%20time%20complexity%20of%20an%20algorithm\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;Google\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026quot;https://stackoverflow.com/search?q=how%20to%20calculate%20time%20complexity%20of%20an%20algorithm\u0026quot;\u0026gt;Stack Overflow\u0026lt;/a\u0026gt; search, but nowhere I was able to find a clear and straightforward explanation for how to calculate time complexity.\u0026lt;/p\u0026gt;\n\u0026lt;h3\u0026gt;What do I know already?\u0026lt;/h3\u0026gt;\n\u0026lt;p\u0026gt;Say for code as simple as the one below:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cs s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-csharp\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;char\u0026lt;/span\u0026gt; h = \u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026apos;y\u0026apos;\u0026lt;/span\u0026gt;; \u0026lt;span class=\u0026quot;hljs-comment\u0026quot;\u0026gt;// This will be executed 1 time\u0026lt;/span\u0026gt;\n\u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;int\u0026lt;/span\u0026gt; abc = \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0\u0026lt;/span\u0026gt;; \u0026lt;span class=\u0026quot;hljs-comment\u0026quot;\u0026gt;// This will be executed 1 time\u0026lt;/span\u0026gt;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;Say for a loop like the one below:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cs s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-csharp\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;for\u0026lt;/span\u0026gt; (\u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;int\u0026lt;/span\u0026gt; i = \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0\u0026lt;/span\u0026gt;; i \u0026amp;lt; N; i++) {\n    Console.Write(\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026apos;Hello, World!!\u0026apos;\u0026lt;/span\u0026gt;);\n}\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;ul\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;code\u0026gt;int i=0;\u0026lt;/code\u0026gt; This will be executed only \u0026lt;strong\u0026gt;once\u0026lt;/strong\u0026gt;.\u0026lt;/li\u0026gt;\n\u0026lt;/ul\u0026gt;\n\u0026lt;p\u0026gt;The time is actually calculated to \u0026lt;code\u0026gt;i=0\u0026lt;/code\u0026gt; and not the declaration.\u0026lt;/p\u0026gt;\n\u0026lt;ul\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;code\u0026gt;i \u0026amp;lt; N;\u0026lt;/code\u0026gt; This will be executed \u0026lt;strong\u0026gt;N+1\u0026lt;/strong\u0026gt; times\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;code\u0026gt;i++\u0026lt;/code\u0026gt; This will be executed \u0026lt;strong\u0026gt;N\u0026lt;/strong\u0026gt; times\u0026lt;/li\u0026gt;\n\u0026lt;/ul\u0026gt;\n\u0026lt;p\u0026gt;So the number of operations required by this loop are \u0026lt;strong\u0026gt;{1+(N+1)+N} = 2N+2\u0026lt;/strong\u0026gt;. (But this still may be wrong, as I am not confident about my understanding.)\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;OK, so these small basic calculations I think I know, but in most cases I have seen the time complexity as \u0026lt;strong\u0026gt;O(N), O(n^2), O(log n), O(n!)\u0026lt;/strong\u0026gt;, and \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Time_complexity\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;many others\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\n    ","slug":"how-can-i-find-the-time-complexity-of-an-algorithm-1657388486001","postType":"QUESTION","createdAt":"2022-07-09T17:41:26.000Z","updatedAt":"2022-07-09T17:41:26.000Z","tags":[],"relatedQuestions":[]},"randomQuestions":[{"title":"JavaScript closure inside loops – simple practical example","slug":"javascript-closure-inside-loops-simple-practical-example-1657384278449"},{"title":"What is the explicit promise construction antipattern and how do I avoid it?","slug":"what-is-the-explicit-promise-construction-antipattern-and-how-do-i-avoid-it-1657384436212"},{"title":"How can I tell if a DOM element is visible in the current viewport?","slug":"how-can-i-tell-if-a-dom-element-is-visible-in-the-current-viewport-1657388316510"},{"title":"How do I access previous promise results in a .then() chain?","slug":"how-do-i-access-previous-promise-results-in-a-.then()-chain-1657387705386"},{"title":"When does SQLiteOpenHelper onCreate() / onUpgrade() run?","slug":"when-does-sqliteopenhelper-oncreate()-onupgrade()-run-1657384883864"},{"title":"Why does my JavaScript code receive a \"No 'Access-Control-Allow-Origin' header is present on the requested resource\" error, while Postman does not?","slug":"why-does-my-javascript-code-receive-a-\"no-'access-control-allow-origin'-header-is-present-on-the-requested-resource\"-error-while-postman-does-not-1657384636363"},{"title":"Why should I not #include \u003cbits/stdc++.h\u003e?","slug":"why-should-i-not-include-lessbitsstdc++.hgreater-1657384470484"},{"title":"Why is using 'eval' a bad practice?","slug":"why-is-using-'eval'-a-bad-practice-1657387348760"},{"title":"How can I access an array/object?","slug":"how-can-i-access-an-arrayobject-1657387395541"},{"title":"How do I return the response from an asynchronous call?","slug":"how-do-i-return-the-response-from-an-asynchronous-call-1657384208012"},{"title":"How can I convert ereg expressions to preg in PHP?","slug":"how-can-i-convert-ereg-expressions-to-preg-in-php-1657387652855"},{"title":"Convert columns into rows with Pandas","slug":"convert-columns-into-rows-with-pandas-1657388374964"},{"title":"How to return DataSnapshot value as a result of a method?","slug":"how-to-return-datasnapshot-value-as-a-result-of-a-method-1657387538165"},{"title":"How do I add a delay in a JavaScript loop?","slug":"how-do-i-add-a-delay-in-a-javascript-loop-1657388544679"},{"title":"How to check whether a string contains a substring in JavaScript?","slug":"how-to-check-whether-a-string-contains-a-substring-in-javascript-1657388080698"},{"title":"List of lists changes reflected across sublists unexpectedly","slug":"list-of-lists-changes-reflected-across-sublists-unexpectedly-1657384393720"},{"title":"How do I get PHP errors to display?","slug":"how-do-i-get-php-errors-to-display-1657384570095"},{"title":"Can you provide some examples of why it is hard to parse XML and HTML with a regex? [closed]","slug":"can-you-provide-some-examples-of-why-it-is-hard-to-parse-xml-and-html-with-a-regex-closed-1657388410824"},{"title":"Why do regex constructors need to be double escaped?","slug":"why-do-regex-constructors-need-to-be-double-escaped-1657388004942"},{"title":"How to use java.util.Scanner to correctly read user input from System.in and act on it?","slug":"how-to-use-java.util.scanner-to-correctly-read-user-input-from-system.in-and-act-on-it-1657388546306"}]},"__N_SSG":true},"page":"/questions/[slug]","query":{"slug":"how-can-i-find-the-time-complexity-of-an-algorithm-1657388486001"},"buildId":"eGqQHYKGZKE-nXShFn8wX","isFallback":false,"gsp":true,"locale":"en","locales":["en"],"defaultLocale":"en","scriptLoader":[]}</script></body></html>