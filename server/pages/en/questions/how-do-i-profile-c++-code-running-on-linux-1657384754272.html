<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@solutionschecker.com"/><meta name="twitter:creator" content="@solutionschecker.com"/><meta property="og:url" content="https://solutionschecker.com"/><meta property="og:type" content="website"/><meta property="og:image" content="https://solutionschecker.com/solutions-checker-banner.png"/><meta property="og:image:alt" content="Find solution for coding, HTML, CSS, JAVASCRIPT, MYSQL, PHP, PYTHON,... quickly. - solutionschecker.com"/><script type="application/ld+json">{"@context":"https://schema.org","@type":"Organization","logo":"/logo.svg","url":"https://solutionschecker.com"}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"https://solutionschecker.com","name":"Home"}},{"@type":"ListItem","position":2,"item":{"@id":"https://solutionschecker.com/questions","name":"Questions"}},{"@type":"ListItem","position":3,"item":{"@id":"https://solutionschecker.com/questions/how-do-i-profile-c++-code-running-on-linux-1657384754272","name":"Questions"}}]}</script><title>How do I profile C++ code running on Linux? | Solutions Checker</title><meta name="robots" content="index,follow"/><meta name="description" content="How do I find areas of my code that run slowly in a C++ application running on Linux?
    "/><meta property="og:title" content="How do I profile C++ code running on Linux? | Solutions Checker"/><meta property="og:description" content="How do I find areas of my code that run slowly in a C++ application running on Linux?
    "/><script type="application/ld+json">{"@context":"https://schema.org","@type":"QAPage","mainEntity":{"name":"How do I profile C++ code running on Linux?","text":"How do I find areas of my code that run slowly in a C++ application running on Linux?\n    ","answerCount":19,"upVoteCount":500,"suggestedAnswer":[{"text":"If your goal is to use a profiler, use one of the suggested ones.\n\nHowever, if you&apos;re in a hurry and you can manually interrupt your program under the debugger while it&apos;s being subjectively slow, there&apos;s a simple way to find performance problems.\n\nJust halt it several times, and each time look at the call stack. If there is some code that is wasting some percentage of the time, 20% or 50% or whatever, that is the probability that you will catch it in the act on each sample. So, that is roughly the percentage of samples on which you will see it. There is no educated guesswork required. If you do have a guess as to what the problem is, this will prove or disprove it.\n\nYou may have multiple performance problems of different sizes. If you clean out any one of them, the remaining ones will take a larger percentage, and be easier to spot, on subsequent passes. This magnification effect, when compounded over multiple problems, can lead to truly massive speedup factors.\n\nCaveat: Programmers tend to be skeptical of this technique unless they&apos;ve used it themselves. They will say that profilers give you this information, but that is only true if they sample the entire call stack, and then let you examine a random set of samples. (The summaries are where the insight is lost.) Call graphs don&apos;t give you the same information, because \n\n\nThey don&apos;t summarize at the instruction level, and\nThey give confusing summaries in the presence of recursion.\n\n\nThey will also say it only works on toy programs, when actually it works on any program, and it seems to work better on bigger programs, because they tend to have more problems to find. They will say it sometimes finds things that aren&apos;t problems, but that is only true if you see something once. If you see a problem on more than one sample, it is real.\n\nP.S. This can also be done on multi-thread programs if there is a way to collect call-stack samples of the thread pool at a point in time, as there is in Java.\n\nP.P.S As a rough generality, the more layers of abstraction you have in your software, the more likely you are to find that that is the cause of performance problems (and the opportunity to get speedup).\n\nAdded: It might not be obvious, but the stack sampling technique works equally well in the presence of recursion. The reason is that the time that would be saved by removal of an instruction is approximated by the fraction of samples containing it, regardless of the number of times it may occur within a sample.\n\nAnother objection I often hear is: &quot;It will stop someplace random, and it will miss the real problem&quot;.\nThis comes from having a prior concept of what the real problem is.\nA key property of performance problems is that they defy expectations.\nSampling tells you something is a problem, and your first reaction is disbelief.\nThat is natural, but you can be sure if it finds a problem it is real, and vice-versa.\n\nAdded: Let me make a Bayesian explanation of how it works.  Suppose there is some instruction I (call or otherwise) which is on the call stack some fraction f of the time (and thus costs that much). For simplicity, suppose we don&apos;t know what f is, but assume it is either 0.1, 0.2, 0.3, ... 0.9, 1.0, and the prior probability of each of these possibilities is 0.1, so all of these costs are equally likely a-priori.\n\nThen suppose we take just 2 stack samples, and we see instruction I on both samples, designated observation o=2/2. This gives us new estimates of the frequency f of I, according to this:\n\nPrior                                    \nP(f=x) x  P(o=2/2|f=x) P(o=2/2&amp;&amp;f=x)  P(o=2/2&amp;&amp;f &gt;= x)  P(f &gt;= x | o=2/2)\n\n0.1    1     1             0.1          0.1            0.25974026\n0.1    0.9   0.81          0.081        0.181          0.47012987\n0.1    0.8   0.64          0.064        0.245          0.636363636\n0.1    0.7   0.49          0.049        0.294          0.763636364\n0.1    0.6   0.36          0.036        0.33           0.857142857\n0.1    0.5   0.25          0.025        0.355          0.922077922\n0.1    0.4   0.16          0.016        0.371          0.963636364\n0.1    0.3   0.09          0.009        0.38           0.987012987\n0.1    0.2   0.04          0.004        0.384          0.997402597\n0.1    0.1   0.01          0.001        0.385          1\n\n                  P(o=2/2) 0.385                \n\n\nThe last column says that, for example, the probability that f &gt;= 0.5 is 92%, up from the prior assumption of 60%.\n\nSuppose the prior assumptions are different. Suppose we assume P(f=0.1) is .991 (nearly certain), and all the other possibilities are almost impossible (0.001). In other words, our prior certainty is that I is cheap. Then we get:\n\nPrior                                    \nP(f=x) x  P(o=2/2|f=x) P(o=2/2&amp;&amp; f=x)  P(o=2/2&amp;&amp;f &gt;= x)  P(f &gt;= x | o=2/2)\n\n0.001  1    1              0.001        0.001          0.072727273\n0.001  0.9  0.81           0.00081      0.00181        0.131636364\n0.001  0.8  0.64           0.00064      0.00245        0.178181818\n0.001  0.7  0.49           0.00049      0.00294        0.213818182\n0.001  0.6  0.36           0.00036      0.0033         0.24\n0.001  0.5  0.25           0.00025      0.00355        0.258181818\n0.001  0.4  0.16           0.00016      0.00371        0.269818182\n0.001  0.3  0.09           0.00009      0.0038         0.276363636\n0.001  0.2  0.04           0.00004      0.00384        0.279272727\n0.991  0.1  0.01           0.00991      0.01375        1\n\n                  P(o=2/2) 0.01375                \n\n\nNow it says P(f &gt;= 0.5) is 26%, up from the prior assumption of 0.6%. So Bayes allows us to update our estimate of the probable cost of I. If the amount of data is small, it doesn&apos;t tell us accurately what the cost is, only that it is big enough to be worth fixing.\n\nYet another way to look at it is called the Rule Of Succession.\nIf you flip a coin 2 times, and it comes up heads both times, what does that tell you about the probable weighting of the coin?\nThe respected way to answer is to say that it&apos;s a Beta distribution, with average value (number of hits + 1) / (number of tries + 2) = (2+1)/(2+2) = 75%.\n\n(The key is that we see I more than once. If we only see it once, that doesn&apos;t tell us much except that f &gt; 0.)\n\nSo, even a very small number of samples can tell us a lot about the cost of instructions that it sees. (And it will see them with a frequency, on average, proportional to their cost. If n samples are taken, and f is the cost, then I will appear on nf+/-sqrt(nf(1-f)) samples. Example, n=10, f=0.3, that is 3+/-1.4 samples.)\n\n\n\nAdded: To give an intuitive feel for the difference between measuring and random stack sampling:\nThere are profilers now that sample the stack, even on wall-clock time, but what comes out is measurements (or hot path, or hot spot, from which a &quot;bottleneck&quot; can easily hide). What they don&apos;t show you (and they easily could) is the actual samples themselves. And if your goal is to find the bottleneck, the number of them you need to see is, on average, 2 divided by the fraction of time it takes.\nSo if it takes 30% of time, 2/.3 = 6.7 samples, on average, will show it, and the chance that 20 samples will show it is 99.2%.\n\nHere is an off-the-cuff illustration of the difference between examining measurements and examining stack samples.\nThe bottleneck could be one big blob like this, or numerous small ones, it makes no difference.\n\n\n\nMeasurement is horizontal; it tells you what fraction of time specific routines take.\nSampling is vertical.\nIf there is any way to avoid what the whole program is doing at that moment, and if you see it on a second sample, you&apos;ve found the bottleneck.\nThat&apos;s what makes the difference - seeing the whole reason for the time being spent, not just how much.\n    ","url":"/questions/[slug]#solution1","@type":"Answer","upvoteCount":0},{"text":"Use Valgrind with the following options:\nvalgrind --tool=callgrind ./(Your binary)\n\nThis generates a file called callgrind.out.x. Use the kcachegrind tool to read this file. It will give you a graphical analysis of things with results like which lines cost how much.\n    ","url":"/questions/[slug]#solution2","@type":"Answer","upvoteCount":0},{"text":"I assume you&apos;re using GCC. The standard solution would be to profile with gprof.\n\nBe sure to add -pg to compilation before profiling:\n\ncc -o myprog myprog.c utils.c -g -pg\n\n\nI haven&apos;t tried it yet but I&apos;ve heard good things about google-perftools. It is definitely worth a try.\n\nRelated question here.\n\nA few other buzzwords if gprof does not do the job for you: Valgrind, Intel VTune, Sun DTrace.\n    ","url":"/questions/[slug]#solution3","@type":"Answer","upvoteCount":0},{"text":"Newer kernels (e.g. the latest Ubuntu kernels) come with the new &apos;perf&apos; tools (apt-get install linux-tools) AKA perf_events.\n\nThese come with classic sampling profilers (man-page) as well as the awesome timechart!\n\nThe important thing is that these tools can be system profiling and not just process profiling - they can show the interaction between threads, processes and the kernel and let you understand the scheduling and I/O dependencies between processes.\n\n\n    ","url":"/questions/[slug]#solution4","@type":"Answer","upvoteCount":0},{"text":"The answer to run valgrind --tool=callgrind is not quite complete without some options. We usually do not want to profile 10 minutes of slow startup time under Valgrind and want to profile our program when it is doing some task.\n\nSo this is what I recommend. Run program first:\n\nvalgrind --tool=callgrind --dump-instr=yes -v --instr-atstart=no ./binary &gt; tmp\n\n\nNow when it works and we want to start profiling we should run in another window:\n\ncallgrind_control -i on\n\n\nThis turns profiling on. To turn it off and stop whole task we might use:\n\ncallgrind_control -k\n\n\nNow we have some files named callgrind.out.* in current directory. To see profiling results use:\n\nkcachegrind callgrind.out.*\n\n\nI recommend in next window to click on &quot;Self&quot; column header, otherwise it shows that &quot;main()&quot; is most time consuming task. &quot;Self&quot; shows how much each function itself took time, not together with dependents. \n    ","url":"/questions/[slug]#solution5","@type":"Answer","upvoteCount":0},{"text":"I would use Valgrind and Callgrind as a base for my profiling tool suite. What is important to know is that Valgrind is basically a Virtual Machine:\n\n\n  (wikipedia) Valgrind is in essence a virtual\n  machine using just-in-time (JIT)\n  compilation techniques, including\n  dynamic recompilation. Nothing from\n  the original program ever gets run\n  directly on the host processor.\n  Instead, Valgrind first translates the\n  program into a temporary, simpler form\n  called Intermediate Representation\n  (IR), which is a processor-neutral,\n  SSA-based form. After the conversion,\n  a tool (see below) is free to do\n  whatever transformations it would like\n  on the IR, before Valgrind translates\n  the IR back into machine code and lets\n  the host processor run it. \n\n\nCallgrind is a profiler build upon that. Main benefit is that you don&apos;t have to run your aplication for hours to get reliable result. Even one second run is sufficient to get rock-solid, reliable results, because Callgrind is a non-probing profiler. \n\nAnother tool build upon Valgrind is Massif. I use it to profile heap memory usage. It works great. What it does is that it gives you snapshots of memory usage -- detailed information WHAT holds WHAT percentage of memory, and WHO had put it there. Such information is available at different points of time of application run.\n    ","url":"/questions/[slug]#solution6","@type":"Answer","upvoteCount":0},{"text":"This is a response to Nazgob&apos;s Gprof answer.\n\nI&apos;ve been using Gprof the last couple of days and have already found three significant limitations, one of which I&apos;ve not seen documented anywhere else (yet):\n\n\nIt doesn&apos;t work properly on multi-threaded code, unless you use a workaround\nThe call graph gets confused by function pointers. Example: I have a function called multithread() which enables me to multi-thread a specified function over a specified array (both passed as arguments). Gprof however, views all calls to multithread() as equivalent for the purposes of computing time spent in children. Since some functions I pass to multithread() take much longer than others my call graphs are mostly useless. (To those wondering if threading is the issue here: no, multithread() can optionally, and did in this case, run everything sequentially on the calling thread only).\nIt says here that &quot;... the number-of-calls figures are derived by counting, not sampling. They are completely accurate...&quot;. Yet I find my call graph giving me 5345859132+784984078 as call stats to my most-called function, where the first number is supposed to be direct calls, and the second recursive calls (which are all from itself). Since this implied I had a bug, I put in long (64-bit) counters into the code and did the same run again. My counts: 5345859132 direct, and 78094395406 self-recursive calls.  There are a lot of digits there, so I&apos;ll point out the recursive calls I measure are 78bn, versus 784m from Gprof: a factor of 100 different. Both runs were single threaded and unoptimised code, one compiled -g and the other -pg.\n\n\nThis was GNU Gprof (GNU Binutils for Debian) 2.18.0.20080103 running under 64-bit Debian Lenny, if that helps anyone.\n    ","url":"/questions/[slug]#solution7","@type":"Answer","upvoteCount":0},{"text":"Survey of C++ profiling techniques: gprof vs valgrind vs perf vs gperftools\nIn this answer, I will use several different tools to a analyze a few very simple test programs, in order to concretely compare how those tools work.\nThe following test program is very simple and does the following:\n\nmain calls fast and maybe_slow 3 times, one of the maybe_slow calls being slow\nThe slow call of maybe_slow is 10x longer, and dominates runtime if we consider calls to the child function common. Ideally, the profiling tool will be able to point us to the specific slow call.\n\nboth fast and maybe_slow call common, which accounts for the bulk of the program execution\n\nThe program interface is:\n./main.out [n [seed]]\n\nand the program does O(n^2) loops in total. seed is just to get different output without affecting runtime.\n\n\nmain.c\n#include &lt;inttypes.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nuint64_t __attribute__ ((noinline)) common(uint64_t n, uint64_t seed) {\n    for (uint64_t i = 0; i &lt; n; ++i) {\n        seed = (seed * seed) - (3 * seed) + 1;\n    }\n    return seed;\n}\n\nuint64_t __attribute__ ((noinline)) fast(uint64_t n, uint64_t seed) {\n    uint64_t max = (n / 10) + 1;\n    for (uint64_t i = 0; i &lt; max; ++i) {\n        seed = common(n, (seed * seed) - (3 * seed) + 1);\n    }\n    return seed;\n}\n\nuint64_t __attribute__ ((noinline)) maybe_slow(uint64_t n, uint64_t seed, int is_slow) {\n    uint64_t max = n;\n    if (is_slow) {\n        max *= 10;\n    }\n    for (uint64_t i = 0; i &lt; max; ++i) {\n        seed = common(n, (seed * seed) - (3 * seed) + 1);\n    }\n    return seed;\n}\n\nint main(int argc, char **argv) {\n    uint64_t n, seed;\n    if (argc &gt; 1) {\n        n = strtoll(argv[1], NULL, 0);\n    } else {\n        n = 1;\n    }\n    if (argc &gt; 2) {\n        seed = strtoll(argv[2], NULL, 0);\n    } else {\n        seed = 0;\n    }\n    seed += maybe_slow(n, seed, 0);\n    seed += fast(n, seed);\n    seed += maybe_slow(n, seed, 1);\n    seed += fast(n, seed);\n    seed += maybe_slow(n, seed, 0);\n    seed += fast(n, seed);\n    printf(&quot;%&quot; PRIX64 &quot;\\n&quot;, seed);\n    return EXIT_SUCCESS;\n}\n\ngprof\ngprof requires recompiling the software with instrumentation, and it also uses a sampling approach together with that instrumentation. It therefore strikes a balance between accuracy (sampling is not always fully accurate and can skip functions) and execution slowdown (instrumentation and sampling are relatively fast techniques that don&apos;t slow down execution very much).\ngprof is built-into GCC/binutils, so all we have to do is to compile with the -pg option to enable gprof. We then run the program normally with a size CLI parameter that produces a run of reasonable duration of a few seconds (10000):\ngcc -pg -ggdb3 -O3 -std=c99 -Wall -Wextra -pedantic -o main.out main.c\ntime ./main.out 10000\n\nFor educational reasons, we will also do a run without optimizations enabled. Note that this is useless in practice, as you normally only care about optimizing the performance of the optimized program:\ngcc -pg -ggdb3 -O0 -std=c99 -Wall -Wextra -pedantic -o main.out main.c\n./main.out 10000\n\nFirst, time tells us that the execution time with and without -pg were the same, which is great: no slowdown! I have however seen accounts of 2x - 3x slowdowns on complex software, e.g. as shown in this ticket.\nBecause we compiled with -pg, running the program produces a file gmon.out file containing the profiling data.\nWe can observe that file graphically with gprof2dot as asked at: Is it possible to get a graphical representation of gprof results?\nsudo apt install graphviz\npython3 -m pip install --user gprof2dot\ngprof main.out &gt; main.gprof\ngprof2dot &lt; main.gprof | dot -Tsvg -o output.svg\n\nHere, the gprof tool reads the gmon.out trace information, and generates a human readable report in main.gprof, which gprof2dot then reads to generate a graph.\nThe source for gprof2dot is at: https://github.com/jrfonseca/gprof2dot\nWe observe the following for the -O0 run:\n\nand for the -O3 run:\n\nThe -O0 output is pretty much self-explanatory. For example, it shows that the 3 maybe_slow calls and their child calls take up 97.56% of the total runtime, although execution of maybe_slow itself without children accounts for 0.00% of the total execution time, i.e. almost all the time spent in that function was spent on child calls.\nTODO: why is main missing from the -O3 output, even though I can see it on a bt in GDB? Missing function from GProf output I think it is because gprof is also sampling based in addition to its compiled instrumentation, and the -O3 main is just too fast and got no samples.\nI choose SVG output instead of PNG because the SVG is searchable with Ctrl + F and the file size can be about 10x smaller. Also, the width and height of the generated image can be humoungous with tens of thousands of pixels for complex software, and GNOME eog 3.28.1 bugs out in that case for PNGs, while SVGs get opened by my browser automatically. gimp 2.8 worked well though, see also:\n\nhttps://askubuntu.com/questions/1112641/how-to-view-extremely-large-images\nhttps://unix.stackexchange.com/questions/77968/viewing-large-image-on-linux\nhttps://superuser.com/questions/356038/viewer-for-huge-images-under-linux-100-mp-color-images\n\nbut even then, you will be dragging the image around a lot to find what you want, see e.g. this image from a &quot;real&quot; software example taken from this ticket:\n\nCan you find the most critical call stack easily with all those tiny unsorted spaghetti lines going over one another? There might be better dot options I&apos;m sure, but I don&apos;t want to go there now. What we really need is a proper dedicated viewer for it, but I haven&apos;t found one yet:\n\nView gprof output in kcachegrind\nWhich is the best replacement for KProf?\n\nYou can however use the color map to mitigate those problems a bit. For example, on the previous huge image, I finally managed to find the critical path on the left when I made the brilliant deduction that green comes after red, followed finally by darker and darker blue.\nAlternatively, we can also observe the text output of the gprof built-in binutils tool which we previously saved at:\ncat main.gprof\n\nBy default, this produces an extremely verbose output that explains what the output data means. Since I can&apos;t explain better than that, I&apos;ll let you read it yourself.\nOnce you have understood the data output format, you can reduce verbosity to show just the data without the tutorial with the -b option:\ngprof -b main.out\n\nIn our example, outputs were for -O0:\nFlat profile:\n\nEach sample counts as 0.01 seconds.\n  %   cumulative   self              self     total           \n time   seconds   seconds    calls   s/call   s/call  name    \n100.35      3.67     3.67   123003     0.00     0.00  common\n  0.00      3.67     0.00        3     0.00     0.03  fast\n  0.00      3.67     0.00        3     0.00     1.19  maybe_slow\n\n            Call graph\n\n\ngranularity: each sample hit covers 2 byte(s) for 0.27% of 3.67 seconds\n\nindex % time    self  children    called     name\n                0.09    0.00    3003/123003      fast [4]\n                3.58    0.00  120000/123003      maybe_slow [3]\n[1]    100.0    3.67    0.00  123003         common [1]\n-----------------------------------------------\n                                                 &lt;spontaneous&gt;\n[2]    100.0    0.00    3.67                 main [2]\n                0.00    3.58       3/3           maybe_slow [3]\n                0.00    0.09       3/3           fast [4]\n-----------------------------------------------\n                0.00    3.58       3/3           main [2]\n[3]     97.6    0.00    3.58       3         maybe_slow [3]\n                3.58    0.00  120000/123003      common [1]\n-----------------------------------------------\n                0.00    0.09       3/3           main [2]\n[4]      2.4    0.00    0.09       3         fast [4]\n                0.09    0.00    3003/123003      common [1]\n-----------------------------------------------\n\nIndex by function name\n\n   [1] common                  [4] fast                    [3] maybe_slow\n\nand for -O3:\nFlat profile:\n\nEach sample counts as 0.01 seconds.\n  %   cumulative   self              self     total           \n time   seconds   seconds    calls  us/call  us/call  name    \n100.52      1.84     1.84   123003    14.96    14.96  common\n\n            Call graph\n\n\ngranularity: each sample hit covers 2 byte(s) for 0.54% of 1.84 seconds\n\nindex % time    self  children    called     name\n                0.04    0.00    3003/123003      fast [3]\n                1.79    0.00  120000/123003      maybe_slow [2]\n[1]    100.0    1.84    0.00  123003         common [1]\n-----------------------------------------------\n                                                 &lt;spontaneous&gt;\n[2]     97.6    0.00    1.79                 maybe_slow [2]\n                1.79    0.00  120000/123003      common [1]\n-----------------------------------------------\n                                                 &lt;spontaneous&gt;\n[3]      2.4    0.00    0.04                 fast [3]\n                0.04    0.00    3003/123003      common [1]\n-----------------------------------------------\n\nIndex by function name\n\n   [1] common\n\nAs a very quick summary for each section e.g.:\n                0.00    3.58       3/3           main [2]\n[3]     97.6    0.00    3.58       3         maybe_slow [3]\n                3.58    0.00  120000/123003      common [1]\n\ncenters around the function that is left indented (maybe_flow). [3] is the ID of that function. Above the function, are its callers, and below it the callees.\nFor -O3, see here like in the graphical output that maybe_slow and fast don&apos;t have a known parent, which is what the documentation says that &lt;spontaneous&gt; means.\nI&apos;m not sure if there is a nice way to do line-by-line profiling with gprof: `gprof` time spent in particular lines of code\nvalgrind callgrind\nvalgrind runs the program through the valgrind virtual machine. This makes the profiling very accurate, but it also produces a very large slowdown of the program. I have also mentioned kcachegrind previously at: Tools to get a pictorial function call graph of code\ncallgrind is the valgrind&apos;s tool to profile code and kcachegrind is a KDE program that can visualize cachegrind output.\nFirst we have to remove the -pg flag to go back to normal compilation, otherwise the run actually fails with Profiling timer expired, and yes, this is so common that I did and there was a Stack Overflow question for it.\nSo we compile and run as:\nsudo apt install kcachegrind valgrind\ngcc -ggdb3 -O3 -std=c99 -Wall -Wextra -pedantic -o main.out main.c\ntime valgrind --tool=callgrind valgrind --dump-instr=yes \\\n  --collect-jumps=yes ./main.out 10000\n\nI enable --dump-instr=yes --collect-jumps=yes because this also dumps information that enables us to view a per assembly line breakdown of performance, at a relatively small added overhead cost.\nOff the bat, time tells us that the program took 29.5 seconds to execute, so we had a slowdown of about 15x on this example. Clearly, this slowdown is going to be a serious limitation for larger workloads. On the &quot;real world software example&quot; mentioned here, I observed a slowdown of 80x.\nThe run generates a profile data file named callgrind.out.&lt;pid&gt; e.g. callgrind.out.8554 in my case. We view that file with:\nkcachegrind callgrind.out.8554\n\nwhich shows a GUI that contains data similar to the textual gprof output:\n\nAlso, if we go on the bottom right &quot;Call Graph&quot; tab, we see a call graph which we can export by right clicking it to obtain the following image with unreasonable amounts of white border :-)\n\nI think fast is not showing on that graph because kcachegrind must have simplified the visualization because that call takes up too little time, this will likely be the behavior you want on a real program. The right click menu has some settings to control when to cull such nodes, but I couldn&apos;t get it to show such a short call after a quick attempt. If I click on fast on the left window, it does show a call graph with fast, so that stack was actually captured. No one had yet found a way to show the complete graph call graph: Make callgrind show all function calls in the kcachegrind callgraph\nTODO on complex C++ software, I see some entries of type &lt;cycle N&gt;, e.g. &lt;cycle 11&gt; where I&apos;d expect function names, what does that mean? I noticed there is a &quot;Cycle Detection&quot; button to toggle that on and off, but what does it mean?\nperf from linux-tools\nperf seems to use exclusively Linux kernel sampling mechanisms. This makes it very simple to setup, but also not fully accurate.\nsudo apt install linux-tools\ntime perf record -g ./main.out 10000\n\nThis added 0.2s to execution, so we are fine time-wise, but I still don&apos;t see much of interest, after expanding the common node with the keyboard right arrow:\nSamples: 7K of event &apos;cycles:uppp&apos;, Event count (approx.): 6228527608     \n  Children      Self  Command   Shared Object     Symbol                  \n-   99.98%    99.88%  main.out  main.out          [.] common              \n     common                                                               \n     0.11%     0.11%  main.out  [kernel]          [k] 0xffffffff8a6009e7  \n     0.01%     0.01%  main.out  [kernel]          [k] 0xffffffff8a600158  \n     0.01%     0.00%  main.out  [unknown]         [k] 0x0000000000000040  \n     0.01%     0.00%  main.out  ld-2.27.so        [.] _dl_sysdep_start    \n     0.01%     0.00%  main.out  ld-2.27.so        [.] dl_main             \n     0.01%     0.00%  main.out  ld-2.27.so        [.] mprotect            \n     0.01%     0.00%  main.out  ld-2.27.so        [.] _dl_map_object      \n     0.01%     0.00%  main.out  ld-2.27.so        [.] _xstat              \n     0.00%     0.00%  main.out  ld-2.27.so        [.] __GI___tunables_init\n     0.00%     0.00%  main.out  [unknown]         [.] 0x2f3d4f4944555453  \n     0.00%     0.00%  main.out  [unknown]         [.] 0x00007fff3cfc57ac  \n     0.00%     0.00%  main.out  ld-2.27.so        [.] _start              \n\nSo then I try to benchmark the -O0 program to see if that shows anything, and only now, at last, do I see a call graph:\nSamples: 15K of event &apos;cycles:uppp&apos;, Event count (approx.): 12438962281   \n  Children      Self  Command   Shared Object     Symbol                  \n+   99.99%     0.00%  main.out  [unknown]         [.] 0x04be258d4c544155  \n+   99.99%     0.00%  main.out  libc-2.27.so      [.] __libc_start_main   \n-   99.99%     0.00%  main.out  main.out          [.] main                \n   - main                                                                 \n      - 97.54% maybe_slow                                                 \n           common                                                         \n      - 2.45% fast                                                        \n           common                                                         \n+   99.96%    99.85%  main.out  main.out          [.] common              \n+   97.54%     0.03%  main.out  main.out          [.] maybe_slow          \n+    2.45%     0.00%  main.out  main.out          [.] fast                \n     0.11%     0.11%  main.out  [kernel]          [k] 0xffffffff8a6009e7  \n     0.00%     0.00%  main.out  [unknown]         [k] 0x0000000000000040  \n     0.00%     0.00%  main.out  ld-2.27.so        [.] _dl_sysdep_start    \n     0.00%     0.00%  main.out  ld-2.27.so        [.] dl_main             \n     0.00%     0.00%  main.out  ld-2.27.so        [.] _dl_lookup_symbol_x \n     0.00%     0.00%  main.out  [kernel]          [k] 0xffffffff8a600158  \n     0.00%     0.00%  main.out  ld-2.27.so        [.] mmap64              \n     0.00%     0.00%  main.out  ld-2.27.so        [.] _dl_map_object      \n     0.00%     0.00%  main.out  ld-2.27.so        [.] __GI___tunables_init\n     0.00%     0.00%  main.out  [unknown]         [.] 0x552e53555f6e653d  \n     0.00%     0.00%  main.out  [unknown]         [.] 0x00007ffe1cf20fdb  \n     0.00%     0.00%  main.out  ld-2.27.so        [.] _start              \n\nTODO: what happened on the -O3 execution? Is it simply that maybe_slow and fast were too fast and did not get any samples? Does it work well with -O3 on larger programs that take longer to execute? Did I miss some CLI option? I found out about -F to control the sample frequency in Hertz, but I turned it up to the max allowed by default of -F 39500 (could be increased with sudo) and I still don&apos;t see clear calls.\nOne cool thing about perf is the FlameGraph tool from Brendan Gregg which displays the call stack timings in a very neat way that allows you to quickly see the big calls. The tool is available at: https://github.com/brendangregg/FlameGraph and is also mentioned on his perf tutorial at: http://www.brendangregg.com/perf.html#FlameGraphs When I ran perf without sudo I got ERROR: No stack counts found so for now I&apos;ll be doing it with sudo:\ngit clone https://github.com/brendangregg/FlameGraph\nsudo perf record -F 99 -g -o perf_with_stack.data ./main.out 10000\nsudo perf script -i perf_with_stack.data | FlameGraph/stackcollapse-perf.pl | FlameGraph/flamegraph.pl &gt; flamegraph.svg\n\nbut in such a simple program the output is not very easy to understand, since we can&apos;t easily see neither maybe_slow nor fast on that graph:\n\nOn the a more complex example it becomes clear what the graph means:\n\nTODO there are a log of [unknown] functions in that example, why is that?\nAnother perf GUI interfaces which might be worth it include:\n\nEclipse Trace Compass plugin: https://www.eclipse.org/tracecompass/\nBut this has the downside that you have to first convert the data to the Common Trace Format, which can be done with perf data --to-ctf, but it needs to be enabled at build time/have perf new enough, either of which is not the case for the perf in Ubuntu 18.04\n\nhttps://github.com/KDAB/hotspot\nThe downside of this is that there seems to be no Ubuntu package, and building it requires Qt 5.10 while Ubuntu 18.04 is at Qt 5.9.\nBut David Faure mentions in the comments that there is no an AppImage package which might be a convenient way to use it.\n\n\ngperftools\nPreviously called &quot;Google Performance Tools&quot;, source: https://github.com/gperftools/gperftools Sample based.\nFirst install gperftools with:\nsudo apt install google-perftools\n\nThen, we can enable the gperftools CPU profiler in two ways: at runtime, or at build time.\nAt runtime, we have to pass set the LD_PRELOAD to point to libprofiler.so, which you can find with locate libprofiler.so, e.g. on my system:\ngcc -ggdb3 -O3 -std=c99 -Wall -Wextra -pedantic -o main.out main.c\nLD_PRELOAD=/usr/lib/x86_64-linux-gnu/libprofiler.so \\\n  CPUPROFILE=prof.out ./main.out 10000\n\nAlternatively, we can build the library in at link time, dispensing passing LD_PRELOAD at runtime:\ngcc -Wl,--no-as-needed,-lprofiler,--as-needed -ggdb3 -O3 -std=c99 -Wall -Wextra -pedantic -o main.out main.c\nCPUPROFILE=prof.out ./main.out 10000\n\nSee also: gperftools - profile file not dumped\nThe nicest way to view this data I&apos;ve found so far is to make pprof output the same format that kcachegrind takes as input (yes, the Valgrind-project-viewer-tool) and use kcachegrind to view that:\ngoogle-pprof --callgrind main.out prof.out  &gt; callgrind.out\nkcachegrind callgrind.out\n\nAfter running with either of those methods, we get a prof.out profile data file as output. We can view that file graphically as an SVG with:\ngoogle-pprof --web main.out prof.out\n\n\nwhich gives as a familiar call graph like other tools, but with the clunky unit of number of samples rather than seconds.\nAlternatively, we can also get some textual data with:\ngoogle-pprof --text main.out prof.out\n\nwhich gives:\nUsing local file main.out.\nUsing local file prof.out.\nTotal: 187 samples\n     187 100.0% 100.0%      187 100.0% common\n       0   0.0% 100.0%      187 100.0% __libc_start_main\n       0   0.0% 100.0%      187 100.0% _start\n       0   0.0% 100.0%        4   2.1% fast\n       0   0.0% 100.0%      187 100.0% main\n       0   0.0% 100.0%      183  97.9% maybe_slow\n\nSee also: How to use google perf tools\nInstrument your code with raw perf_event_open syscalls\nI think this is the same underlying subsystem that perf uses, but you could of course attain even greater control by explicitly instrumenting your program at compile time with events of interest.\nThis is likely too hardcore for most people, but it&apos;s kind of fun. Minimal runnable example at: Quick way to count number of instructions executed in a C program\nIntel VTune\nhttps://en.wikipedia.org/wiki/VTune\nThis seems to be closed source and x86-only, but it is likely to be amazing from what I&apos;ve heard. I&apos;m not sure how free it is to use, but it seems to be free to download. TODO evaluate.\nTested in Ubuntu 18.04, gprof2dot 2019.11.30, valgrind 3.13.0, perf 4.15.18, Linux kernel 4.15.0, FLameGraph 1a0dc6985aad06e76857cf2a354bd5ba0c9ce96b, gperftools 2.5-2.\n    ","url":"/questions/[slug]#solution8","@type":"Answer","upvoteCount":0},{"text":"Use Valgrind, callgrind and kcachegrind: \n\nvalgrind --tool=callgrind ./(Your binary)\n\n\ngenerates callgrind.out.x. Read it using kcachegrind.\n\nUse gprof (add -pg): \n\ncc -o myprog myprog.c utils.c -g -pg \n\n\n(not so good for multi-threads, function pointers)\n\nUse google-perftools: \n\nUses time sampling, I/O and CPU bottlenecks are revealed.\n\nIntel VTune is the best (free for educational purposes).\n\nOthers: AMD Codeanalyst (since replaced with AMD CodeXL), OProfile, &apos;perf&apos; tools (apt-get install linux-tools)\n    ","url":"/questions/[slug]#solution9","@type":"Answer","upvoteCount":0},{"text":"For single-threaded programs you can use igprof, The Ignominous Profiler: https://igprof.org/ .\n\nIt is a sampling profiler, along the lines of the... long... answer by Mike Dunlavey, which will gift wrap the results in a browsable call stack tree, annotated with the time or memory spent in each function, either cumulative or per-function.\n    ","url":"/questions/[slug]#solution10","@type":"Answer","upvoteCount":0},{"text":"Also worth mentioning are\n\n\nHPCToolkit (http://hpctoolkit.org/) - Open-source, works for parallel programs and has a GUI with which to look at the results multiple ways\nIntel VTune (https://software.intel.com/en-us/vtune) - If you have intel compilers this is very good \nTAU (http://www.cs.uoregon.edu/research/tau/home.php) \n\n\nI have used HPCToolkit and VTune and they are very effective at finding the long pole in the tent and do not need your code to be recompiled (except that you have to use -g -O or RelWithDebInfo type build in CMake to get meaningful output). I have heard TAU is similar in capabilities.\n    ","url":"/questions/[slug]#solution11","@type":"Answer","upvoteCount":0},{"text":"Actually a bit surprised not many mentioned about google/benchmark , while it is a bit cumbersome to pin the specific area of code, specially if the code base is a little big one, however I found this really helpful when used in combination with callgrind\nIMHO identifying the piece that is causing bottleneck is the key here. I&apos;d however try and answer the following questions first and choose tool based on that\n\nis my algorithm correct ?\nare there locks that are proving to be bottle necks ?\nis there a specific section of code that&apos;s proving to be a culprit ?\nhow about IO, handled and optimized ?\n\nvalgrind with the combination of callgrind and kcachegrind should provide a decent estimation on the points above, and once it&apos;s established that there are issues with some section of code, I&apos;d suggest to do a micro bench mark - google benchmark is a good place to start.\n    ","url":"/questions/[slug]#solution12","@type":"Answer","upvoteCount":0},{"text":"These are the two methods I use for speeding up my code:\n\nFor CPU bound applications:\n\n\nUse a profiler in DEBUG mode to identify questionable parts of your code\nThen switch to RELEASE mode and comment out the questionable sections of your code (stub it with nothing) until you see changes in performance.\n\n\nFor I/O bound applications:\n\n\nUse a profiler in RELEASE mode to identify questionable parts of your code.\n\n\n\n\nN.B.\n\nIf you don&apos;t have a profiler, use the poor man&apos;s profiler. Hit pause while debugging your application. Most developer suites will break into assembly with commented line numbers. You&apos;re statistically likely to land in a region that is eating most of your CPU cycles.\n\nFor CPU, the reason for profiling in DEBUG mode is because if your tried profiling in RELEASE mode, the compiler is going to reduce math, vectorize loops, and inline functions which tends to glob your code into an un-mappable mess when it&apos;s assembled. An un-mappable mess means your profiler will not be able to clearly identify what is taking so long because the assembly may not correspond to the source code under optimization. If you need the performance (e.g. timing sensitive) of RELEASE mode, disable debugger features as needed to keep a usable performance.\n\nFor I/O-bound, the profiler can still identify I/O operations in RELEASE mode because I/O operations are either externally linked to a shared library (most of the time) or in the worst case, will result in a sys-call interrupt vector (which is also easily identifiable by the profiler).\n    ","url":"/questions/[slug]#solution13","@type":"Answer","upvoteCount":0},{"text":"You can use the iprof library:\n\nhttps://gitlab.com/Neurochrom/iprof\n\nhttps://github.com/Neurochrom/iprof\n\nIt&apos;s cross-platform and allows you not to measure performance of your application also in real-time. You can even couple it with a live graph.\nFull disclaimer: I am the author.\n    ","url":"/questions/[slug]#solution14","@type":"Answer","upvoteCount":0},{"text":"You can use a logging framework like loguru since it includes timestamps and total uptime which can be used nicely for profiling:\n\n\n    ","url":"/questions/[slug]#solution15","@type":"Answer","upvoteCount":0},{"text":"At work we have a really nice tool that helps us monitoring what we want in terms of scheduling. This has been useful numerous times.\n\nIt&apos;s in C++ and must be customized to your needs. Unfortunately I can&apos;t share code, just concepts.\nYou use a &quot;large&quot; volatile buffer containing timestamps and event ID that you can dump post mortem or after stopping the logging system (and dump this into a file for example).\n\nYou retrieve the so-called large buffer with all the data and a small interface parses it and shows events with name (up/down + value) like an oscilloscope does with colors (configured in .hpp file).\n\nYou customize the amount of events generated to focus solely on what you desire. It helped us a lot for scheduling issues while consuming the amount of CPU we wanted based on the amount of logged events per second. \n\nYou need 3 files : \n\ntoolname.hpp // interface\ntoolname.cpp // code\ntool_events_id.hpp // Events ID\n\n\nThe concept is to define events in tool_events_id.hpp like that :\n\n// EVENT_NAME                         ID      BEGIN_END BG_COLOR NAME\n#define SOCK_PDU_RECV_D               0x0301  //@D00301 BGEEAAAA # TX_PDU_Recv\n#define SOCK_PDU_RECV_F               0x0302  //@F00301 BGEEAAAA # TX_PDU_Recv\n\n\nYou also define a few functions in toolname.hpp :\n\n#define LOG_LEVEL_ERROR 0\n#define LOG_LEVEL_WARN 1\n// ...\n\nvoid init(void);\nvoid probe(id,payload);\n// etc\n\n\nWherever in you code you can use :\n\ntoolname&lt;LOG_LEVEL&gt;::log(EVENT_NAME,VALUE);\n\n\nThe probe function uses a few assembly lines to retrieve the clock timestamp ASAP and then sets an entry in the buffer. We also have an atomic increment to safely find an index where to store the log event.\nOf course buffer is circular.\n\nHope the idea is not obfuscated by the lack of sample code.\n    ","url":"/questions/[slug]#solution16","@type":"Answer","upvoteCount":0},{"text":"use a debugging software \nhow to identify where the code is running slowly ?\n\njust think you have a obstacle while you are in motion then it will decrease your speed  \n\nlike that unwanted reallocation&apos;s looping,buffer overflows,searching,memory leakages etc operations consumes more execution power it will effect adversely over performance of the code,\nBe sure to add -pg to compilation before profiling:\n\ng++ your_prg.cpp -pg or cc my_program.cpp -g -pg as per your compiler\n\nhaven&apos;t tried it yet but I&apos;ve heard good things about google-perftools. It is definitely worth a try.\n\nvalgrind --tool=callgrind ./(Your binary)\n\nIt will generate a file called gmon.out or callgrind.out.x. You can then use kcachegrind or debugger tool to read this file. It will give you a graphical analysis of things with results like which lines cost how much. \n\ni think so\n    ","url":"/questions/[slug]#solution17","@type":"Answer","upvoteCount":0},{"text":"As no one mentioned Arm MAP, I&apos;d add it as personally I have successfully used Map to profile a C++ scientific program. \n\nArm MAP is the profiler for parallel, multithreaded or single threaded C, C++, Fortran and F90 codes.  It provides in-depth analysis and bottleneck pinpointing to the source line.  Unlike most profilers, it&apos;s designed to be able to profile pthreads, OpenMP or MPI for parallel and threaded code.\n\nMAP is commercial software. \n    ","url":"/questions/[slug]#solution18","@type":"Answer","upvoteCount":0},{"text":"Use -pg flag when compiling and linking the code and run the executable file. While this program is executed, profiling data is collected in the file a.out.\nThere is two different type of profiling\n\n1- Flat profiling: \n by running the command gprog --flat-profile a.out you got the following data\n - what percentage of the overall time was spent for the function,\n - how many seconds were spent in a functionincluding and excluding calls to sub-functions,\n - the number of calls,\n - the average time per call.\n\n2- graph profiling\nus the command gprof --graph a.out to get the following data for each function which includes\n - In each section, one function is marked with an index number.\n - Above function , there is a list of functions that call the function .\n - Below function , there is a list of functions that are called by the function .\n\nTo get more info you can look in https://sourceware.org/binutils/docs-2.32/gprof/\n    ","url":"/questions/[slug]#solution19","@type":"Answer","upvoteCount":0}],"@type":"Question"}}</script><meta name="next-head-count" content="17"/><link rel="preload" href="/_next/static/css/08bcc42a26fe5c92.css" as="style"/><link rel="stylesheet" href="/_next/static/css/08bcc42a26fe5c92.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-0d1b80a048d4787e.js"></script><script src="/_next/static/chunks/webpack-42cdea76c8170223.js" defer=""></script><script src="/_next/static/chunks/framework-4556c45dd113b893.js" defer=""></script><script src="/_next/static/chunks/main-ccfab947c79712f4.js" defer=""></script><script src="/_next/static/chunks/pages/_app-7e79b78ef884195a.js" defer=""></script><script src="/_next/static/chunks/294-106ef8570fa99deb.js" defer=""></script><script src="/_next/static/chunks/490-7f0418bb4354ac73.js" defer=""></script><script src="/_next/static/chunks/pages/questions/%5Bslug%5D-c1a73f3e758d48b9.js" defer=""></script><script src="/_next/static/XDXakEY6gSPdgAODPxtjg/_buildManifest.js" defer=""></script><script src="/_next/static/XDXakEY6gSPdgAODPxtjg/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="wrapper"><header><nav class="bg-white border-gray-200 px-4 lg:px-6 py-2.5 dark:bg-gray-800"><div class="flex flex-wrap justify-between items-center mx-auto max-w-screen-xl"><a class="flex items-center" href="/"><img src="/logo-second.png" class="mr-3 h-6 sm:h-9" alt="Solution Checker Logo"/><h4 class="self-center text-xl font-semibold whitespace-nowrap dark:text-white">Solution Checker</h4></a><div class="flex items-center lg:order-2"><button data-collapse-toggle="mobile-menu-2" type="button" class="inline-flex items-center p-2 ml-1 text-sm text-gray-500 rounded-lg lg:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200 dark:text-gray-400 dark:hover:bg-gray-700 dark:focus:ring-gray-600" aria-controls="mobile-menu-2" aria-expanded="false"><span class="sr-only">Open main menu</span><svg class="w-6 h-6" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 10a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 15a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z" clip-rule="evenodd"></path></svg><svg class="hidden w-6 h-6" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M4.293 4.293a1 1 0 011.414 0L10 8.586l4.293-4.293a1 1 0 111.414 1.414L11.414 10l4.293 4.293a1 1 0 01-1.414 1.414L10 11.414l-4.293 4.293a1 1 0 01-1.414-1.414L8.586 10 4.293 5.707a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></button></div><div class="hidden justify-between items-center w-full lg:flex lg:w-auto lg:order-1" id="mobile-menu-2"><ul class="flex flex-col mt-4 font-medium lg:flex-row lg:space-x-8 lg:mt-0"><li><a class="block py-2 pr-4 pl-3 text-gray-700 border-b border-gray-100 hover:bg-gray-50 lg:hover:bg-transparent lg:border-0 lg:hover:text-blue-700 lg:p-0 dark:text-gray-400 lg:dark:hover:text-white dark:hover:bg-gray-700 dark:hover:text-white lg:dark:hover:bg-transparent dark:border-gray-700" aria-current="page" href="/">Home</a></li><li><a class="block py-2 pr-4 pl-3 text-gray-700 border-b border-gray-100 hover:bg-gray-50 lg:hover:bg-transparent lg:border-0 lg:hover:text-blue-700 lg:p-0 dark:text-gray-400 lg:dark:hover:text-white dark:hover:bg-gray-700 dark:hover:text-white lg:dark:hover:bg-transparent dark:border-gray-700" href="/questions?tab=news">Questions</a></li><li><a class="block py-2 pr-4 pl-3 text-gray-700 border-b border-gray-100 hover:bg-gray-50 lg:hover:bg-transparent lg:border-0 lg:hover:text-blue-700 lg:p-0 dark:text-gray-400 lg:dark:hover:text-white dark:hover:bg-gray-700 dark:hover:text-white lg:dark:hover:bg-transparent dark:border-gray-700" href="/post?tab=news">Post</a></li><li><a class="block py-2 pr-4 pl-3 text-gray-700 border-b border-gray-100 hover:bg-gray-50 lg:hover:bg-transparent lg:border-0 lg:hover:text-blue-700 lg:p-0 dark:text-gray-400 lg:dark:hover:text-white dark:hover:bg-gray-700 dark:hover:text-white lg:dark:hover:bg-transparent dark:border-gray-700" href="/questions/how-do-i-profile-c++-code-running-on-linux-1657384754272#">Coding</a></li></ul></div></div></nav></header><div class="main-content"><div class="question my-5"><div class="flex question-header items-center m-auto justify-center"><div class="rounded-xl w-full border p-5 shadow-md bg-white"><div class="flex w-full items-center justify-between border-b pb-3"><div class="flex items-center space-x-3"><div class="text-lg font-bold text-slate-700"><a href="/questions/how-do-i-profile-c++-code-running-on-linux-1657384754272"><h1>How do I profile C++ code running on Linux?</h1></a></div></div><div class="flex flex-wrap h-auto justify-end items-center space-x-8"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold" href="/questions/tag/profiling">profiling</a></div></div><div class="question-content mt-5">
                
<p>How do I find areas of my code that run slowly in a C++ application running on Linux?</p>
    </div></div></div><div class="solution-section"><nav class="flex pagination-solution flex-col justify-end"><ul class="inline-flex -space-x-px overflow-auto"><li class="pagination-solution-item"><span data-id="#solution1" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">1</span></li><li class="pagination-solution-item"><span data-id="#solution2" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">2</span></li><li class="pagination-solution-item"><span data-id="#solution3" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">3</span></li><li class="pagination-solution-item"><span data-id="#solution4" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">4</span></li><li class="pagination-solution-item"><span data-id="#solution5" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">5</span></li><li class="pagination-solution-item"><span data-id="#solution6" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">6</span></li><li class="pagination-solution-item"><span data-id="#solution7" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">7</span></li><li class="pagination-solution-item"><span data-id="#solution8" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">8</span></li><li class="pagination-solution-item"><span data-id="#solution9" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">9</span></li><li class="pagination-solution-item"><span data-id="#solution10" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">10</span></li><li class="pagination-solution-item"><span data-id="#solution11" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">11</span></li><li class="pagination-solution-item"><span data-id="#solution12" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">12</span></li><li class="pagination-solution-item"><span data-id="#solution13" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">13</span></li><li class="pagination-solution-item"><span data-id="#solution14" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">14</span></li><li class="pagination-solution-item"><span data-id="#solution15" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">15</span></li><li class="pagination-solution-item"><span data-id="#solution16" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">16</span></li><li class="pagination-solution-item"><span data-id="#solution17" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">17</span></li><li class="pagination-solution-item"><span data-id="#solution18" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">18</span></li><li class="pagination-solution-item"><span data-id="#solution19" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">19</span></li></ul></nav><div id="solution1" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 1</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiling">profiling</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>If your goal is to use a profiler, use one of the suggested ones.</p>

<p>However, if you're in a hurry and you can manually interrupt your program under the debugger while it's being subjectively slow, there's a simple way to find performance problems.</p>

<p>Just halt it several times, and each time look at the call stack. If there is some code that is wasting some percentage of the time, 20% or 50% or whatever, that is the probability that you will catch it in the act on each sample. So, that is roughly the percentage of samples on which you will see it. There is no educated guesswork required. If you do have a guess as to what the problem is, this will prove or disprove it.</p>

<p>You may have multiple performance problems of different sizes. If you clean out any one of them, the remaining ones will take a larger percentage, and be easier to spot, on subsequent passes. This <em>magnification effect</em>, when compounded over multiple problems, can lead to truly massive speedup factors.</p>

<p><strong>Caveat</strong>: Programmers tend to be skeptical of this technique unless they've used it themselves. They will say that profilers give you this information, but that is only true if they sample the entire call stack, and then let you examine a random set of samples. (The summaries are where the insight is lost.) Call graphs don't give you the same information, because </p>

<ol>
<li>They don't summarize at the instruction level, and</li>
<li>They give confusing summaries in the presence of recursion.</li>
</ol>

<p>They will also say it only works on toy programs, when actually it works on any program, and it seems to work better on bigger programs, because they tend to have more problems to find. They will say it sometimes finds things that aren't problems, but that is only true if you see something <em>once</em>. If you see a problem on more than one sample, it is real.</p>

<p><strong>P.S.</strong> This can also be done on multi-thread programs if there is a way to collect call-stack samples of the thread pool at a point in time, as there is in Java.</p>

<p><strong>P.P.S</strong> As a rough generality, the more layers of abstraction you have in your software, the more likely you are to find that that is the cause of performance problems (and the opportunity to get speedup).</p>

<p><strong>Added</strong>: It might not be obvious, but the stack sampling technique works equally well in the presence of recursion. The reason is that the time that would be saved by removal of an instruction is approximated by the fraction of samples containing it, regardless of the number of times it may occur within a sample.</p>

<p>Another objection I often hear is: "<em>It will stop someplace random, and it will miss the real problem</em>".
This comes from having a prior concept of what the real problem is.
A key property of performance problems is that they defy expectations.
Sampling tells you something is a problem, and your first reaction is disbelief.
That is natural, but you can be sure if it finds a problem it is real, and vice-versa.</p>

<p><strong>Added</strong>: Let me make a Bayesian explanation of how it works.  Suppose there is some instruction <code>I</code> (call or otherwise) which is on the call stack some fraction <code>f</code> of the time (and thus costs that much). For simplicity, suppose we don't know what <code>f</code> is, but assume it is either 0.1, 0.2, 0.3, ... 0.9, 1.0, and the prior probability of each of these possibilities is 0.1, so all of these costs are equally likely a-priori.</p>

<p>Then suppose we take just 2 stack samples, and we see instruction <code>I</code> on both samples, designated observation <code>o=2/2</code>. This gives us new estimates of the frequency <code>f</code> of <code>I</code>, according to this:</p>

<pre class="lang-cpp s-code-block"><code class="hljs language-cpp"><span class="hljs-function">Prior                                    
<span class="hljs-title">P</span><span class="hljs-params">(f=x)</span> x  <span class="hljs-title">P</span><span class="hljs-params">(o=<span class="hljs-number">2</span>/<span class="hljs-number">2</span>|f=x)</span> <span class="hljs-title">P</span><span class="hljs-params">(o=<span class="hljs-number">2</span>/<span class="hljs-number">2</span>&amp;&amp;f=x)</span>  <span class="hljs-title">P</span><span class="hljs-params">(o=<span class="hljs-number">2</span>/<span class="hljs-number">2</span>&amp;&amp;f &gt;= x)</span>  <span class="hljs-title">P</span><span class="hljs-params">(f &gt;= x | o=<span class="hljs-number">2</span>/<span class="hljs-number">2</span>)</span>

0.1    1     1             0.1          0.1            0.25974026
0.1    0.9   0.81          0.081        0.181          0.47012987
0.1    0.8   0.64          0.064        0.245          0.636363636
0.1    0.7   0.49          0.049        0.294          0.763636364
0.1    0.6   0.36          0.036        0.33           0.857142857
0.1    0.5   0.25          0.025        0.355          0.922077922
0.1    0.4   0.16          0.016        0.371          0.963636364
0.1    0.3   0.09          0.009        0.38           0.987012987
0.1    0.2   0.04          0.004        0.384          0.997402597
0.1    0.1   0.01          0.001        0.385          1

                  <span class="hljs-title">P</span><span class="hljs-params">(o=<span class="hljs-number">2</span>/<span class="hljs-number">2</span>)</span> 0.385                
</span></code></pre>

<p>The last column says that, for example, the probability that <code>f</code> &gt;= 0.5 is 92%, up from the prior assumption of 60%.</p>

<p>Suppose the prior assumptions are different. Suppose we assume <code>P(f=0.1)</code> is .991 (nearly certain), and all the other possibilities are almost impossible (0.001). In other words, our prior certainty is that <code>I</code> is cheap. Then we get:</p>

<pre class="lang-cpp s-code-block"><code class="hljs language-cpp"><span class="hljs-function">Prior                                    
<span class="hljs-title">P</span><span class="hljs-params">(f=x)</span> x  <span class="hljs-title">P</span><span class="hljs-params">(o=<span class="hljs-number">2</span>/<span class="hljs-number">2</span>|f=x)</span> <span class="hljs-title">P</span><span class="hljs-params">(o=<span class="hljs-number">2</span>/<span class="hljs-number">2</span>&amp;&amp; f=x)</span>  <span class="hljs-title">P</span><span class="hljs-params">(o=<span class="hljs-number">2</span>/<span class="hljs-number">2</span>&amp;&amp;f &gt;= x)</span>  <span class="hljs-title">P</span><span class="hljs-params">(f &gt;= x | o=<span class="hljs-number">2</span>/<span class="hljs-number">2</span>)</span>

0.001  1    1              0.001        0.001          0.072727273
0.001  0.9  0.81           0.00081      0.00181        0.131636364
0.001  0.8  0.64           0.00064      0.00245        0.178181818
0.001  0.7  0.49           0.00049      0.00294        0.213818182
0.001  0.6  0.36           0.00036      0.0033         0.24
0.001  0.5  0.25           0.00025      0.00355        0.258181818
0.001  0.4  0.16           0.00016      0.00371        0.269818182
0.001  0.3  0.09           0.00009      0.0038         0.276363636
0.001  0.2  0.04           0.00004      0.00384        0.279272727
0.991  0.1  0.01           0.00991      0.01375        1

                  <span class="hljs-title">P</span><span class="hljs-params">(o=<span class="hljs-number">2</span>/<span class="hljs-number">2</span>)</span> 0.01375                
</span></code></pre>

<p>Now it says <code>P(f &gt;= 0.5)</code> is 26%, up from the prior assumption of 0.6%. So Bayes allows us to update our estimate of the probable cost of <code>I</code>. If the amount of data is small, it doesn't tell us accurately what the cost is, only that it is big enough to be worth fixing.</p>

<p>Yet another way to look at it is called the <a href="http://en.wikipedia.org/wiki/Rule_of_succession" rel="noreferrer">Rule Of Succession</a>.
If you flip a coin 2 times, and it comes up heads both times, what does that tell you about the probable weighting of the coin?
The respected way to answer is to say that it's a Beta distribution, with average value <code>(number of hits + 1) / (number of tries + 2) = (2+1)/(2+2) = 75%</code>.</p>

<p>(The key is that we see <code>I</code> more than once. If we only see it once, that doesn't tell us much except that <code>f</code> &gt; 0.)</p>

<p>So, even a very small number of samples can tell us a lot about the cost of instructions that it sees. (And it will see them with a frequency, on average, proportional to their cost. If <code>n</code> samples are taken, and <code>f</code> is the cost, then <code>I</code> will appear on <code>nf+/-sqrt(nf(1-f))</code> samples. Example, <code>n=10</code>, <code>f=0.3</code>, that is <code>3+/-1.4</code> samples.)</p>

<hr>

<p><strong>Added</strong>: To give an intuitive feel for the difference between measuring and random stack sampling:<br>
There are profilers now that sample the stack, even on wall-clock time, but <em>what comes out</em> is measurements (or hot path, or hot spot, from which a "bottleneck" can easily hide). What they don't show you (and they easily could) is the actual samples themselves. And if your goal is to <em>find</em> the bottleneck, the number of them you need to see is, <em>on average</em>, 2 divided by the fraction of time it takes.
So if it takes 30% of time, 2/.3 = 6.7 samples, on average, will show it, and the chance that 20 samples will show it is 99.2%.</p>

<p>Here is an off-the-cuff illustration of the difference between examining measurements and examining stack samples.
The bottleneck could be one big blob like this, or numerous small ones, it makes no difference.</p>

<p><a href="https://i.stack.imgur.com/FpWuS.png" rel="noreferrer"><img src="https://i.stack.imgur.com/FpWuS.png" alt="enter image description here"></a></p>

<p>Measurement is horizontal; it tells you what fraction of time specific routines take.
Sampling is vertical.
If there is any way to avoid what the whole program is doing at that moment, <em>and if you see it on a second sample</em>, you've found the bottleneck.
That's what makes the difference - seeing the whole reason for the time being spent, not just how much.</p>
    </div></div></div></div><div id="solution2" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 2</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiling">profiling</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Use <a href="http://en.wikipedia.org/wiki/Valgrind" rel="nofollow noreferrer">Valgrind</a> with the following options:</p>
<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">valgrind --tool=callgrind ./(Your binary)
</code></pre>
<p>This generates a file called <code>callgrind.out.x</code>. Use the <code>kcachegrind</code> tool to read this file. It will give you a graphical analysis of things with results like which lines cost how much.</p>
    </div></div></div></div><div id="solution3" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 3</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiling">profiling</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>I assume you're using GCC. The standard solution would be to profile with <a href="http://www.math.utah.edu/docs/info/gprof_toc.html" rel="noreferrer">gprof</a>.</p>

<p>Be sure to add <code>-pg</code> to compilation before profiling:</p>

<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">cc -o myprog myprog.c utils.c -g -pg
</code></pre>

<p>I haven't tried it yet but I've heard good things about <a href="https://github.com/gperftools/gperftools" rel="noreferrer">google-perftools</a>. It is definitely worth a try.</p>

<p>Related question <a href="https://stackoverflow.com/questions/56672/how-do-you-profile-your-code">here</a>.</p>

<p>A few other buzzwords if <code>gprof</code> does not do the job for you: <a href="http://en.wikipedia.org/wiki/Valgrind" rel="noreferrer">Valgrind</a>, Intel <a href="http://en.wikipedia.org/wiki/VTune" rel="noreferrer">VTune</a>, Sun <a href="http://en.wikipedia.org/wiki/DTrace" rel="noreferrer">DTrace</a>.</p>
    </div></div></div></div><div id="solution4" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 4</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiling">profiling</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Newer kernels (e.g. the latest Ubuntu kernels) come with the new 'perf' tools (<code>apt-get install linux-tools</code>) AKA <a href="https://en.wikipedia.org/wiki/Perf_(Linux)" rel="noreferrer">perf_events</a>.</p>

<p>These come with classic sampling profilers (<a href="http://manpages.ubuntu.com/manpages/trusty/man1/perf.1.html" rel="noreferrer">man-page</a>) as well as the awesome <a href="http://web.archive.org/web/20090922171904/http://blog.fenrus.org/?p=5" rel="noreferrer">timechart</a>!</p>

<p>The important thing is that these tools can be <strong>system profiling</strong> and not just process profiling - they can show the interaction between threads, processes and the kernel and let you understand the scheduling and I/O dependencies between processes.</p>

<p><img src="https://i.stack.imgur.com/FMYp4.png" alt="Alt text"></p>
    </div></div></div></div><div id="solution5" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 5</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiling">profiling</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>The answer to run <code>valgrind --tool=callgrind</code> is not quite complete without some options. We usually do not want to profile 10 minutes of slow startup time under Valgrind and want to profile our program when it is doing some task.</p>

<p>So this is what I recommend. Run program first:</p>

<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">valgrind --tool=callgrind --dump-instr=yes -v --instr-atstart=no ./binary &gt; tmp
</code></pre>

<p>Now when it works and we want to start profiling we should run in another window:</p>

<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">callgrind_control -i on
</code></pre>

<p>This turns profiling on. To turn it off and stop whole task we might use:</p>

<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">callgrind_control -k
</code></pre>

<p>Now we have some files named callgrind.out.* in current directory. To see profiling results use:</p>

<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">kcachegrind callgrind.out.*
</code></pre>

<p>I recommend in next window to click on "Self" column header, otherwise it shows that "main()" is most time consuming task. "Self" shows how much each function itself took time, not together with dependents. </p>
    </div></div></div></div><div id="solution6" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 6</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiling">profiling</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>I would use Valgrind and Callgrind as a base for my profiling tool suite. What is important to know is that Valgrind is basically a Virtual Machine:</p>

<blockquote>
  <p>(wikipedia) Valgrind is in essence a virtual
  machine using just-in-time (JIT)
  compilation techniques, including
  dynamic recompilation. Nothing from
  the original program ever gets run
  directly on the host processor.
  Instead, Valgrind first translates the
  program into a temporary, simpler form
  called Intermediate Representation
  (IR), which is a processor-neutral,
  SSA-based form. After the conversion,
  a tool (see below) is free to do
  whatever transformations it would like
  on the IR, before Valgrind translates
  the IR back into machine code and lets
  the host processor run it. </p>
</blockquote>

<p>Callgrind is a profiler build upon that. Main benefit is that you don't have to run your aplication for hours to get reliable result. Even one second run is sufficient to get rock-solid, reliable results, because Callgrind is a <strong>non-probing</strong> profiler. </p>

<p>Another tool build upon Valgrind is Massif. I use it to profile heap memory usage. It works great. What it does is that it gives you snapshots of memory usage -- detailed information WHAT holds WHAT percentage of memory, and WHO had put it there. Such information is available at different points of time of application run.</p>
    </div></div></div></div><div id="solution7" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 7</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiling">profiling</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>This is a response to <a href="https://stackoverflow.com/a/375930/321731">Nazgob's Gprof answer</a>.</p>

<p>I've been using Gprof the last couple of days and have already found three significant limitations, one of which I've not seen documented anywhere else (yet):</p>

<ol>
<li><p>It doesn't work properly on multi-threaded code, unless you use a <a href="http://sam.zoy.org/writings/programming/gprof.html" rel="noreferrer">workaround</a></p></li>
<li><p>The call graph gets confused by function pointers. Example: I have a function called <code>multithread()</code> which enables me to multi-thread a specified function over a specified array (both passed as arguments). Gprof however, views all calls to <code>multithread()</code> as equivalent for the purposes of computing time spent in children. Since some functions I pass to <code>multithread()</code> take much longer than others my call graphs are mostly useless. (To those wondering if threading is the issue here: no, <code>multithread()</code> can optionally, and did in this case, run everything sequentially on the calling thread only).</p></li>
<li><p>It says <a href="http://www.cs.utah.edu/dept/old/texinfo/as/gprof.html" rel="noreferrer">here</a> that "... the number-of-calls figures are derived by counting, not sampling. They are completely accurate...". Yet I find my call graph giving me 5345859132+784984078 as call stats to my most-called function, where the first number is supposed to be direct calls, and the second recursive calls (which are all from itself). Since this implied I had a bug, I put in long (64-bit) counters into the code and did the same run again. My counts: 5345859132 direct, and 78094395406 self-recursive calls.  There are a lot of digits there, so I'll point out the recursive calls I measure are 78bn, versus 784m from Gprof: a factor of 100 different. Both runs were single threaded and unoptimised code, one compiled <code>-g</code> and the other <code>-pg</code>.</p></li>
</ol>

<p>This was GNU <a href="https://en.wikipedia.org/wiki/Gprof" rel="noreferrer">Gprof</a> (GNU Binutils for Debian) 2.18.0.20080103 running under 64-bit Debian Lenny, if that helps anyone.</p>
    </div></div></div></div><div id="solution8" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 8</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiling">profiling</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p><strong>Survey of C++ profiling techniques: gprof vs valgrind vs perf vs gperftools</strong></p>
<p>In this answer, I will use several different tools to a analyze a few very simple test programs, in order to concretely compare how those tools work.</p>
<p>The following test program is very simple and does the following:</p>
<ul>
<li><p><code>main</code> calls <code>fast</code> and <code>maybe_slow</code> 3 times, one of the <code>maybe_slow</code> calls being slow</p>
<p>The slow call of <code>maybe_slow</code> is 10x longer, and dominates runtime if we consider calls to the child function <code>common</code>. Ideally, the profiling tool will be able to point us to the specific slow call.</p>
</li>
<li><p>both <code>fast</code> and <code>maybe_slow</code> call <code>common</code>, which accounts for the bulk of the program execution</p>
</li>
<li><p>The program interface is:</p>
<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">./main.out [n [seed]]
</code></pre>
<p>and the program does <code>O(n^2)</code> loops in total. <code>seed</code> is just to get different output without affecting runtime.</p>
</li>
</ul>
<p>main.c</p>
<pre class="lang-cpp s-code-block"><code class="hljs language-cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;inttypes.h&gt;</span></span>
<span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span></span>
<span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdlib.h&gt;</span></span>

<span class="hljs-type">uint64_t</span> __attribute__ ((noinline)) <span class="hljs-built_in">common</span>(<span class="hljs-type">uint64_t</span> n, <span class="hljs-type">uint64_t</span> seed) {
    <span class="hljs-keyword">for</span> (<span class="hljs-type">uint64_t</span> i = <span class="hljs-number">0</span>; i &lt; n; ++i) {
        seed = (seed * seed) - (<span class="hljs-number">3</span> * seed) + <span class="hljs-number">1</span>;
    }
    <span class="hljs-keyword">return</span> seed;
}

<span class="hljs-type">uint64_t</span> __attribute__ ((noinline)) <span class="hljs-built_in">fast</span>(<span class="hljs-type">uint64_t</span> n, <span class="hljs-type">uint64_t</span> seed) {
    <span class="hljs-type">uint64_t</span> max = (n / <span class="hljs-number">10</span>) + <span class="hljs-number">1</span>;
    <span class="hljs-keyword">for</span> (<span class="hljs-type">uint64_t</span> i = <span class="hljs-number">0</span>; i &lt; max; ++i) {
        seed = <span class="hljs-built_in">common</span>(n, (seed * seed) - (<span class="hljs-number">3</span> * seed) + <span class="hljs-number">1</span>);
    }
    <span class="hljs-keyword">return</span> seed;
}

<span class="hljs-type">uint64_t</span> __attribute__ ((noinline)) <span class="hljs-built_in">maybe_slow</span>(<span class="hljs-type">uint64_t</span> n, <span class="hljs-type">uint64_t</span> seed, <span class="hljs-type">int</span> is_slow) {
    <span class="hljs-type">uint64_t</span> max = n;
    <span class="hljs-keyword">if</span> (is_slow) {
        max *= <span class="hljs-number">10</span>;
    }
    <span class="hljs-keyword">for</span> (<span class="hljs-type">uint64_t</span> i = <span class="hljs-number">0</span>; i &lt; max; ++i) {
        seed = <span class="hljs-built_in">common</span>(n, (seed * seed) - (<span class="hljs-number">3</span> * seed) + <span class="hljs-number">1</span>);
    }
    <span class="hljs-keyword">return</span> seed;
}

<span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-type">int</span> argc, <span class="hljs-type">char</span> **argv)</span> </span>{
    <span class="hljs-type">uint64_t</span> n, seed;
    <span class="hljs-keyword">if</span> (argc &gt; <span class="hljs-number">1</span>) {
        n = <span class="hljs-built_in">strtoll</span>(argv[<span class="hljs-number">1</span>], <span class="hljs-literal">NULL</span>, <span class="hljs-number">0</span>);
    } <span class="hljs-keyword">else</span> {
        n = <span class="hljs-number">1</span>;
    }
    <span class="hljs-keyword">if</span> (argc &gt; <span class="hljs-number">2</span>) {
        seed = <span class="hljs-built_in">strtoll</span>(argv[<span class="hljs-number">2</span>], <span class="hljs-literal">NULL</span>, <span class="hljs-number">0</span>);
    } <span class="hljs-keyword">else</span> {
        seed = <span class="hljs-number">0</span>;
    }
    seed += <span class="hljs-built_in">maybe_slow</span>(n, seed, <span class="hljs-number">0</span>);
    seed += <span class="hljs-built_in">fast</span>(n, seed);
    seed += <span class="hljs-built_in">maybe_slow</span>(n, seed, <span class="hljs-number">1</span>);
    seed += <span class="hljs-built_in">fast</span>(n, seed);
    seed += <span class="hljs-built_in">maybe_slow</span>(n, seed, <span class="hljs-number">0</span>);
    seed += <span class="hljs-built_in">fast</span>(n, seed);
    <span class="hljs-built_in">printf</span>(<span class="hljs-string">"%"</span> PRIX64 <span class="hljs-string">"\n"</span>, seed);
    <span class="hljs-keyword">return</span> EXIT_SUCCESS;
}
</code></pre>
<p><strong>gprof</strong></p>
<p>gprof requires recompiling the software with instrumentation, and it also uses a sampling approach together with that instrumentation. It therefore strikes a balance between accuracy (sampling is not always fully accurate and can skip functions) and execution slowdown (instrumentation and sampling are relatively fast techniques that don't slow down execution very much).</p>
<p>gprof is built-into GCC/binutils, so all we have to do is to compile with the <code>-pg</code> option to enable gprof. We then run the program normally with a size CLI parameter that produces a run of reasonable duration of a few seconds (<code>10000</code>):</p>
<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">gcc -pg -ggdb3 -O3 -std=c99 -Wall -Wextra -pedantic -o main.out main.c
time ./main.out <span class="hljs-number">10000</span>
</code></pre>
<p>For educational reasons, we will also do a run without optimizations enabled. Note that this is useless in practice, as you normally only care about optimizing the performance of the optimized program:</p>
<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">gcc -pg -ggdb3 -O0 -std=c99 -Wall -Wextra -pedantic -o main.out main.c
./main.out <span class="hljs-number">10000</span>
</code></pre>
<p>First, <code>time</code> tells us that the execution time with and without <code>-pg</code> were the same, which is great: no slowdown! I have however seen accounts of 2x - 3x slowdowns on complex software, e.g. as <a href="https://gem5.atlassian.net/browse/GEM5-337" rel="nofollow noreferrer">shown in this ticket</a>.</p>
<p>Because we compiled with <code>-pg</code>, running the program produces a file <code>gmon.out</code> file containing the profiling data.</p>
<p>We can observe that file graphically with <code>gprof2dot</code> as asked at: <a href="https://stackoverflow.com/questions/2439060/is-it-possible-to-get-a-graphical-representation-of-gprof-results">Is it possible to get a graphical representation of gprof results?</a></p>
<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">sudo apt install graphviz
python3 -m pip install --user gprof2dot
gprof main.out &gt; main.gprof
gprof2dot &lt; main.gprof | dot -Tsvg -o output.svg
</code></pre>
<p>Here, the <code>gprof</code> tool reads the <code>gmon.out</code> trace information, and generates a human readable report in <code>main.gprof</code>, which <code>gprof2dot</code> then reads to generate a graph.</p>
<p>The source for gprof2dot is at: <a href="https://github.com/jrfonseca/gprof2dot" rel="nofollow noreferrer">https://github.com/jrfonseca/gprof2dot</a></p>
<p>We observe the following for the <code>-O0</code> run:</p>
<p><a href="https://i.stack.imgur.com/mM8NQ.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/mM8NQ.png" alt="enter image description here"></a></p>
<p>and for the <code>-O3</code> run:</p>
<p><a href="https://i.stack.imgur.com/31VNy.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/31VNy.png" alt="enter image description here"></a></p>
<p>The <code>-O0</code> output is pretty much self-explanatory. For example, it shows that the 3 <code>maybe_slow</code> calls and their child calls take up 97.56% of the total runtime, although execution of <code>maybe_slow</code> itself without children accounts for 0.00% of the total execution time, i.e. almost all the time spent in that function was spent on child calls.</p>
<p>TODO: why is <code>main</code> missing from the <code>-O3</code> output, even though I can see it on a <code>bt</code> in GDB? <a href="https://stackoverflow.com/questions/39041871/missing-function-from-gprof-output">Missing function from GProf output</a> I think it is because gprof is also sampling based in addition to its compiled instrumentation, and the <code>-O3</code> <code>main</code> is just too fast and got no samples.</p>
<p>I choose SVG output instead of PNG because the SVG is searchable with <kbd>Ctrl</kbd> + <kbd>F</kbd> and the file size can be about 10x smaller. Also, the width and height of the generated image can be humoungous with tens of thousands of pixels for complex software, and GNOME <code>eog</code> 3.28.1 bugs out in that case for PNGs, while SVGs get opened by my browser automatically. gimp 2.8 worked well though, see also:</p>
<ul>
<li><a href="https://askubuntu.com/questions/1112641/how-to-view-extremely-large-images">https://askubuntu.com/questions/1112641/how-to-view-extremely-large-images</a></li>
<li><a href="https://unix.stackexchange.com/questions/77968/viewing-large-image-on-linux">https://unix.stackexchange.com/questions/77968/viewing-large-image-on-linux</a></li>
<li><a href="https://superuser.com/questions/356038/viewer-for-huge-images-under-linux-100-mp-color-images">https://superuser.com/questions/356038/viewer-for-huge-images-under-linux-100-mp-color-images</a></li>
</ul>
<p>but even then, you will be dragging the image around a lot to find what you want, see e.g. this image from a "real" software example taken from <a href="https://gem5.atlassian.net/browse/GEM5-337" rel="nofollow noreferrer">this ticket</a>:</p>
<p><a href="https://i.stack.imgur.com/Nvg9G.jpg" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/Nvg9G.jpg" alt="enter image description here"></a></p>
<p>Can you find the most critical call stack easily with all those tiny unsorted spaghetti lines going over one another? There might be better <code>dot</code> options I'm sure, but I don't want to go there now. What we really need is a proper dedicated viewer for it, but I haven't found one yet:</p>
<ul>
<li><a href="https://stackoverflow.com/questions/7274095/view-gprof-output-in-kcachegrind">View gprof output in kcachegrind</a></li>
<li><a href="https://stackoverflow.com/questions/1576666/which-is-the-best-replacement-for-kprof">Which is the best replacement for KProf?</a></li>
</ul>
<p>You can however use the color map to mitigate those problems a bit. For example, on the previous huge image, I finally managed to find the critical path on the left when I made the brilliant deduction that green comes after red, followed finally by darker and darker blue.</p>
<p>Alternatively, we can also observe the text output of the <code>gprof</code> built-in binutils tool which we previously saved at:</p>
<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">cat main.gprof
</code></pre>
<p>By default, this produces an extremely verbose output that explains what the output data means. Since I can't explain better than that, I'll let you read it yourself.</p>
<p>Once you have understood the data output format, you can reduce verbosity to show just the data without the tutorial with the <code>-b</code> option:</p>
<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">gprof -b main.out
</code></pre>
<p>In our example, outputs were for <code>-O0</code>:</p>
<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">Flat profile:

Each sample counts as <span class="hljs-number">0.01</span> seconds.
  %   cumulative   self              self     total           
 time   seconds   seconds    calls   s/call   s/call  name    
<span class="hljs-number">100.35</span>      <span class="hljs-number">3.67</span>     <span class="hljs-number">3.67</span>   <span class="hljs-number">123003</span>     <span class="hljs-number">0.00</span>     <span class="hljs-number">0.00</span>  common
  <span class="hljs-number">0.00</span>      <span class="hljs-number">3.67</span>     <span class="hljs-number">0.00</span>        <span class="hljs-number">3</span>     <span class="hljs-number">0.00</span>     <span class="hljs-number">0.03</span>  fast
  <span class="hljs-number">0.00</span>      <span class="hljs-number">3.67</span>     <span class="hljs-number">0.00</span>        <span class="hljs-number">3</span>     <span class="hljs-number">0.00</span>     <span class="hljs-number">1.19</span>  maybe_slow

            Call graph


granularity: each sample hit covers <span class="hljs-number">2</span> <span class="hljs-built_in">byte</span>(s) <span class="hljs-keyword">for</span> <span class="hljs-number">0.27</span>% of <span class="hljs-number">3.67</span> seconds

index % time    self  children    called     name
                <span class="hljs-number">0.09</span>    <span class="hljs-number">0.00</span>    <span class="hljs-number">3003</span>/<span class="hljs-number">123003</span>      fast [<span class="hljs-number">4</span>]
                <span class="hljs-number">3.58</span>    <span class="hljs-number">0.00</span>  <span class="hljs-number">120000</span>/<span class="hljs-number">123003</span>      maybe_slow [<span class="hljs-number">3</span>]
[<span class="hljs-number">1</span>]    <span class="hljs-number">100.0</span>    <span class="hljs-number">3.67</span>    <span class="hljs-number">0.00</span>  <span class="hljs-number">123003</span>         common [<span class="hljs-number">1</span>]
-----------------------------------------------
                                                 &lt;spontaneous&gt;
[<span class="hljs-number">2</span>]    <span class="hljs-number">100.0</span>    <span class="hljs-number">0.00</span>    <span class="hljs-number">3.67</span>                 main [<span class="hljs-number">2</span>]
                <span class="hljs-number">0.00</span>    <span class="hljs-number">3.58</span>       <span class="hljs-number">3</span>/<span class="hljs-number">3</span>           maybe_slow [<span class="hljs-number">3</span>]
                <span class="hljs-number">0.00</span>    <span class="hljs-number">0.09</span>       <span class="hljs-number">3</span>/<span class="hljs-number">3</span>           fast [<span class="hljs-number">4</span>]
-----------------------------------------------
                <span class="hljs-number">0.00</span>    <span class="hljs-number">3.58</span>       <span class="hljs-number">3</span>/<span class="hljs-number">3</span>           main [<span class="hljs-number">2</span>]
[<span class="hljs-number">3</span>]     <span class="hljs-number">97.6</span>    <span class="hljs-number">0.00</span>    <span class="hljs-number">3.58</span>       <span class="hljs-number">3</span>         maybe_slow [<span class="hljs-number">3</span>]
                <span class="hljs-number">3.58</span>    <span class="hljs-number">0.00</span>  <span class="hljs-number">120000</span>/<span class="hljs-number">123003</span>      common [<span class="hljs-number">1</span>]
-----------------------------------------------
                <span class="hljs-number">0.00</span>    <span class="hljs-number">0.09</span>       <span class="hljs-number">3</span>/<span class="hljs-number">3</span>           main [<span class="hljs-number">2</span>]
[<span class="hljs-number">4</span>]      <span class="hljs-number">2.4</span>    <span class="hljs-number">0.00</span>    <span class="hljs-number">0.09</span>       <span class="hljs-number">3</span>         fast [<span class="hljs-number">4</span>]
                <span class="hljs-number">0.09</span>    <span class="hljs-number">0.00</span>    <span class="hljs-number">3003</span>/<span class="hljs-number">123003</span>      common [<span class="hljs-number">1</span>]
-----------------------------------------------

Index by function name

   [<span class="hljs-number">1</span>] common                  [<span class="hljs-number">4</span>] fast                    [<span class="hljs-number">3</span>] maybe_slow
</code></pre>
<p>and for <code>-O3</code>:</p>
<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">Flat profile:

Each sample counts as <span class="hljs-number">0.01</span> seconds.
  %   cumulative   self              self     total           
 time   seconds   seconds    calls  us/call  us/call  name    
<span class="hljs-number">100.52</span>      <span class="hljs-number">1.84</span>     <span class="hljs-number">1.84</span>   <span class="hljs-number">123003</span>    <span class="hljs-number">14.96</span>    <span class="hljs-number">14.96</span>  common

            Call graph


granularity: each sample hit covers <span class="hljs-number">2</span> <span class="hljs-built_in">byte</span>(s) <span class="hljs-keyword">for</span> <span class="hljs-number">0.54</span>% of <span class="hljs-number">1.84</span> seconds

index % time    self  children    called     name
                <span class="hljs-number">0.04</span>    <span class="hljs-number">0.00</span>    <span class="hljs-number">3003</span>/<span class="hljs-number">123003</span>      fast [<span class="hljs-number">3</span>]
                <span class="hljs-number">1.79</span>    <span class="hljs-number">0.00</span>  <span class="hljs-number">120000</span>/<span class="hljs-number">123003</span>      maybe_slow [<span class="hljs-number">2</span>]
[<span class="hljs-number">1</span>]    <span class="hljs-number">100.0</span>    <span class="hljs-number">1.84</span>    <span class="hljs-number">0.00</span>  <span class="hljs-number">123003</span>         common [<span class="hljs-number">1</span>]
-----------------------------------------------
                                                 &lt;spontaneous&gt;
[<span class="hljs-number">2</span>]     <span class="hljs-number">97.6</span>    <span class="hljs-number">0.00</span>    <span class="hljs-number">1.79</span>                 maybe_slow [<span class="hljs-number">2</span>]
                <span class="hljs-number">1.79</span>    <span class="hljs-number">0.00</span>  <span class="hljs-number">120000</span>/<span class="hljs-number">123003</span>      common [<span class="hljs-number">1</span>]
-----------------------------------------------
                                                 &lt;spontaneous&gt;
[<span class="hljs-number">3</span>]      <span class="hljs-number">2.4</span>    <span class="hljs-number">0.00</span>    <span class="hljs-number">0.04</span>                 fast [<span class="hljs-number">3</span>]
                <span class="hljs-number">0.04</span>    <span class="hljs-number">0.00</span>    <span class="hljs-number">3003</span>/<span class="hljs-number">123003</span>      common [<span class="hljs-number">1</span>]
-----------------------------------------------

Index by function name

   [<span class="hljs-number">1</span>] common
</code></pre>
<p>As a very quick summary for each section e.g.:</p>
<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">                <span class="hljs-number">0.00</span>    <span class="hljs-number">3.58</span>       <span class="hljs-number">3</span>/<span class="hljs-number">3</span>           main [<span class="hljs-number">2</span>]
[<span class="hljs-number">3</span>]     <span class="hljs-number">97.6</span>    <span class="hljs-number">0.00</span>    <span class="hljs-number">3.58</span>       <span class="hljs-number">3</span>         maybe_slow [<span class="hljs-number">3</span>]
                <span class="hljs-number">3.58</span>    <span class="hljs-number">0.00</span>  <span class="hljs-number">120000</span>/<span class="hljs-number">123003</span>      common [<span class="hljs-number">1</span>]
</code></pre>
<p>centers around the function that is left indented (<code>maybe_flow</code>). <code>[3]</code> is the ID of that function. Above the function, are its callers, and below it the callees.</p>
<p>For <code>-O3</code>, see here like in the graphical output that <code>maybe_slow</code> and <code>fast</code> don't have a known parent, which is what the documentation says that <code>&lt;spontaneous&gt;</code> means.</p>
<p>I'm not sure if there is a nice way to do line-by-line profiling with gprof: <a href="https://stackoverflow.com/questions/9608949/gprof-time-spent-in-particular-lines-of-code">`gprof` time spent in particular lines of code</a></p>
<p><strong>valgrind callgrind</strong></p>
<p>valgrind runs the program through the valgrind virtual machine. This makes the profiling very accurate, but it also produces a very large slowdown of the program. I have also mentioned kcachegrind previously at: <a href="https://stackoverflow.com/questions/517589/tools-to-get-a-pictorial-function-call-graph-of-code/31190167#31190167">Tools to get a pictorial function call graph of code</a></p>
<p>callgrind is the valgrind's tool to profile code and kcachegrind is a KDE program that can visualize cachegrind output.</p>
<p>First we have to remove the <code>-pg</code> flag to go back to normal compilation, otherwise the run actually fails with <a href="https://stackoverflow.com/questions/2146082/valgrind-profiling-timer-expired"><code>Profiling timer expired</code></a>, and yes, this is so common that I did and there was a Stack Overflow question for it.</p>
<p>So we compile and run as:</p>
<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">sudo apt install kcachegrind valgrind
gcc -ggdb3 -O3 -std=c99 -Wall -Wextra -pedantic -o main.out main.c
time valgrind --tool=callgrind valgrind --dump-instr=yes \
  --collect-jumps=yes ./main.out <span class="hljs-number">10000</span>
</code></pre>
<p>I enable <code>--dump-instr=yes --collect-jumps=yes</code> because this also dumps information that enables us to view a per assembly line breakdown of performance, at a relatively small added overhead cost.</p>
<p>Off the bat, <code>time</code> tells us that the program took 29.5 seconds to execute, so we had a slowdown of about 15x on this example. Clearly, this slowdown is going to be a serious limitation for larger workloads. On the "real world software example" <a href="https://gem5.atlassian.net/browse/GEM5-337" rel="nofollow noreferrer">mentioned here</a>, I observed a slowdown of 80x.</p>
<p>The run generates a profile data file named <code>callgrind.out.&lt;pid&gt;</code> e.g. <code>callgrind.out.8554</code> in my case. We view that file with:</p>
<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">kcachegrind callgrind.out<span class="hljs-number">.8554</span>
</code></pre>
<p>which shows a GUI that contains data similar to the textual gprof output:</p>
<p><a href="https://i.stack.imgur.com/v1kfK.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/v1kfK.png" alt="enter image description here"></a></p>
<p>Also, if we go on the bottom right "Call Graph" tab, we see a call graph which we can export by right clicking it to obtain the following image with unreasonable amounts of white border :-)</p>
<p><a href="https://i.stack.imgur.com/ZTdAJ.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/ZTdAJ.png" alt="enter image description here"></a></p>
<p>I think <code>fast</code> is not showing on that graph because kcachegrind must have simplified the visualization because that call takes up too little time, this will likely be the behavior you want on a real program. The right click menu has some settings to control when to cull such nodes, but I couldn't get it to show such a short call after a quick attempt. If I click on <code>fast</code> on the left window, it does show a call graph with <code>fast</code>, so that stack was actually captured. No one had yet found a way to show the complete graph call graph: <a href="https://stackoverflow.com/questions/33769323/make-callgrind-show-all-function-calls-in-the-kcachegrind-callgraph">Make callgrind show all function calls in the kcachegrind callgraph</a></p>
<p>TODO on complex C++ software, I see some entries of type <code>&lt;cycle N&gt;</code>, e.g. <code>&lt;cycle 11&gt;</code> where I'd expect function names, what does that mean? I noticed there is a "Cycle Detection" button to toggle that on and off, but what does it mean?</p>
<p><strong><code>perf</code> from <code>linux-tools</code></strong></p>
<p><code>perf</code> seems to use exclusively Linux kernel sampling mechanisms. This makes it very simple to setup, but also not fully accurate.</p>
<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">sudo apt install linux-tools
time perf record -g ./main.out <span class="hljs-number">10000</span>
</code></pre>
<p>This added 0.2s to execution, so we are fine time-wise, but I still don't see much of interest, after expanding the <code>common</code> node with the keyboard right arrow:</p>
<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">Samples: <span class="hljs-number">7</span>K of event <span class="hljs-string">'cycles:uppp'</span>, <span class="hljs-function">Event <span class="hljs-title">count</span> <span class="hljs-params">(approx.)</span>: <span class="hljs-number">6228527608</span>     
  Children      Self  Command   Shared Object     Symbol                  
-   <span class="hljs-number">99.98</span>%    <span class="hljs-number">99.88</span>%  main.out  main.out          [.] common              
     common                                                               
     <span class="hljs-number">0.11</span>%     <span class="hljs-number">0.11</span>%  main.out  [kernel]          [k] <span class="hljs-number">0xffffffff8a6009e7</span>  
     <span class="hljs-number">0.01</span>%     <span class="hljs-number">0.01</span>%  main.out  [kernel]          [k] <span class="hljs-number">0xffffffff8a600158</span>  
     <span class="hljs-number">0.01</span>%     <span class="hljs-number">0.00</span>%  main.out  [unknown]         [k] <span class="hljs-number">0x0000000000000040</span>  
     <span class="hljs-number">0.01</span>%     <span class="hljs-number">0.00</span>%  main.out  ld<span class="hljs-number">-2.27</span>.so        [.] _dl_sysdep_start    
     <span class="hljs-number">0.01</span>%     <span class="hljs-number">0.00</span>%  main.out  ld<span class="hljs-number">-2.27</span>.so        [.] dl_main             
     <span class="hljs-number">0.01</span>%     <span class="hljs-number">0.00</span>%  main.out  ld<span class="hljs-number">-2.27</span>.so        [.] mprotect            
     <span class="hljs-number">0.01</span>%     <span class="hljs-number">0.00</span>%  main.out  ld<span class="hljs-number">-2.27</span>.so        [.] _dl_map_object      
     <span class="hljs-number">0.01</span>%     <span class="hljs-number">0.00</span>%  main.out  ld<span class="hljs-number">-2.27</span>.so        [.] _xstat              
     <span class="hljs-number">0.00</span>%     <span class="hljs-number">0.00</span>%  main.out  ld<span class="hljs-number">-2.27</span>.so        [.] __GI___tunables_init
     <span class="hljs-number">0.00</span>%     <span class="hljs-number">0.00</span>%  main.out  [unknown]         [.] <span class="hljs-number">0x2f3d4f4944555453</span>  
     <span class="hljs-number">0.00</span>%     <span class="hljs-number">0.00</span>%  main.out  [unknown]         [.] <span class="hljs-number">0x00007fff3cfc57ac</span>  
     <span class="hljs-number">0.00</span>%     <span class="hljs-number">0.00</span>%  main.out  ld<span class="hljs-number">-2.27</span>.so        [.] _start              
</span></code></pre>
<p>So then I try to benchmark the <code>-O0</code> program to see if that shows anything, and only now, at last, do I see a call graph:</p>
<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">Samples: <span class="hljs-number">15</span>K of event <span class="hljs-string">'cycles:uppp'</span>, <span class="hljs-function">Event <span class="hljs-title">count</span> <span class="hljs-params">(approx.)</span>: <span class="hljs-number">12438962281</span>   
  Children      Self  Command   Shared Object     Symbol                  
+   <span class="hljs-number">99.99</span>%     <span class="hljs-number">0.00</span>%  main.out  [unknown]         [.] <span class="hljs-number">0x04be258d4c544155</span>  
+   <span class="hljs-number">99.99</span>%     <span class="hljs-number">0.00</span>%  main.out  libc<span class="hljs-number">-2.27</span>.so      [.] __libc_start_main   
-   <span class="hljs-number">99.99</span>%     <span class="hljs-number">0.00</span>%  main.out  main.out          [.] main                
   - main                                                                 
      - <span class="hljs-number">97.54</span>% maybe_slow                                                 
           common                                                         
      - <span class="hljs-number">2.45</span>% fast                                                        
           common                                                         
+   <span class="hljs-number">99.96</span>%    <span class="hljs-number">99.85</span>%  main.out  main.out          [.] common              
+   <span class="hljs-number">97.54</span>%     <span class="hljs-number">0.03</span>%  main.out  main.out          [.] maybe_slow          
+    <span class="hljs-number">2.45</span>%     <span class="hljs-number">0.00</span>%  main.out  main.out          [.] fast                
     <span class="hljs-number">0.11</span>%     <span class="hljs-number">0.11</span>%  main.out  [kernel]          [k] <span class="hljs-number">0xffffffff8a6009e7</span>  
     <span class="hljs-number">0.00</span>%     <span class="hljs-number">0.00</span>%  main.out  [unknown]         [k] <span class="hljs-number">0x0000000000000040</span>  
     <span class="hljs-number">0.00</span>%     <span class="hljs-number">0.00</span>%  main.out  ld<span class="hljs-number">-2.27</span>.so        [.] _dl_sysdep_start    
     <span class="hljs-number">0.00</span>%     <span class="hljs-number">0.00</span>%  main.out  ld<span class="hljs-number">-2.27</span>.so        [.] dl_main             
     <span class="hljs-number">0.00</span>%     <span class="hljs-number">0.00</span>%  main.out  ld<span class="hljs-number">-2.27</span>.so        [.] _dl_lookup_symbol_x 
     <span class="hljs-number">0.00</span>%     <span class="hljs-number">0.00</span>%  main.out  [kernel]          [k] <span class="hljs-number">0xffffffff8a600158</span>  
     <span class="hljs-number">0.00</span>%     <span class="hljs-number">0.00</span>%  main.out  ld<span class="hljs-number">-2.27</span>.so        [.] mmap64              
     <span class="hljs-number">0.00</span>%     <span class="hljs-number">0.00</span>%  main.out  ld<span class="hljs-number">-2.27</span>.so        [.] _dl_map_object      
     <span class="hljs-number">0.00</span>%     <span class="hljs-number">0.00</span>%  main.out  ld<span class="hljs-number">-2.27</span>.so        [.] __GI___tunables_init
     <span class="hljs-number">0.00</span>%     <span class="hljs-number">0.00</span>%  main.out  [unknown]         [.] <span class="hljs-number">0x552e53555f6e653d</span>  
     <span class="hljs-number">0.00</span>%     <span class="hljs-number">0.00</span>%  main.out  [unknown]         [.] <span class="hljs-number">0x00007ffe1cf20fdb</span>  
     <span class="hljs-number">0.00</span>%     <span class="hljs-number">0.00</span>%  main.out  ld<span class="hljs-number">-2.27</span>.so        [.] _start              
</span></code></pre>
<p>TODO: what happened on the <code>-O3</code> execution? Is it simply that <code>maybe_slow</code> and <code>fast</code> were too fast and did not get any samples? Does it work well with <code>-O3</code> on larger programs that take longer to execute? Did I miss some CLI option? I found out about <code>-F</code> to control the sample frequency in Hertz, but I turned it up to the max allowed by default of <code>-F 39500</code> (could be increased with <code>sudo</code>) and I still don't see clear calls.</p>
<p>One cool thing about <code>perf</code> is the FlameGraph tool from Brendan Gregg which displays the call stack timings in a very neat way that allows you to quickly see the big calls. The tool is available at: <a href="https://github.com/brendangregg/FlameGraph" rel="nofollow noreferrer">https://github.com/brendangregg/FlameGraph</a> and is also mentioned on his perf tutorial at: <a href="http://www.brendangregg.com/perf.html#FlameGraphs" rel="nofollow noreferrer">http://www.brendangregg.com/perf.html#FlameGraphs</a> When I ran <code>perf</code> without <code>sudo</code> I got <a href="https://github.com/brendangregg/FlameGraph/issues/132" rel="nofollow noreferrer"><code>ERROR: No stack counts found</code></a> so for now I'll be doing it with <code>sudo</code>:</p>
<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">git clone https:<span class="hljs-comment">//github.com/brendangregg/FlameGraph</span>
sudo perf record -F <span class="hljs-number">99</span> -g -o perf_with_stack.data ./main.out <span class="hljs-number">10000</span>
sudo perf script -i perf_with_stack.data | FlameGraph/stackcollapse-perf.pl | FlameGraph/flamegraph.pl &gt; flamegraph.svg
</code></pre>
<p>but in such a simple program the output is not very easy to understand, since we can't easily see neither <code>maybe_slow</code> nor <code>fast</code> on that graph:</p>
<p><a href="https://i.stack.imgur.com/QFKSS.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/QFKSS.png" alt="enter image description here"></a></p>
<p>On the a more complex example it becomes clear what the graph means:</p>
<p><a href="https://i.stack.imgur.com/4Ufpd.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/4Ufpd.png" alt="enter image description here"></a></p>
<p>TODO there are a log of <code>[unknown]</code> functions in that example, why is that?</p>
<p>Another perf GUI interfaces which might be worth it include:</p>
<ul>
<li><p>Eclipse Trace Compass plugin: <a href="https://www.eclipse.org/tracecompass/" rel="nofollow noreferrer">https://www.eclipse.org/tracecompass/</a></p>
<p>But this has the downside that you have to first convert the data to the Common Trace Format, which can be done with <code>perf data --to-ctf</code>, but it needs to be enabled at build time/have <code>perf</code> new enough, either of which is not the case for the perf in Ubuntu 18.04</p>
</li>
<li><p><a href="https://github.com/KDAB/hotspot" rel="nofollow noreferrer">https://github.com/KDAB/hotspot</a></p>
<p>The downside of this is that there seems to be no Ubuntu package, and building it requires Qt 5.10 while Ubuntu 18.04 is at Qt 5.9.</p>
<p>But <a href="https://stackoverflow.com/users/758288/david-faure">David Faure</a> mentions in the comments that there is no an AppImage package which might be a convenient way to use it.</p>
</li>
</ul>
<p><strong>gperftools</strong></p>
<p>Previously called "Google Performance Tools", source: <a href="https://github.com/gperftools/gperftools" rel="nofollow noreferrer">https://github.com/gperftools/gperftools</a> Sample based.</p>
<p>First install gperftools with:</p>
<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">sudo apt install google-perftools
</code></pre>
<p>Then, we can enable the gperftools CPU profiler in two ways: at runtime, or at build time.</p>
<p>At runtime, we have to pass set the <code>LD_PRELOAD</code> to point to <code>libprofiler.so</code>, which you can find with <code>locate libprofiler.so</code>, e.g. on my system:</p>
<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">gcc -ggdb3 -O3 -std=c99 -Wall -Wextra -pedantic -o main.out main.c
LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libprofiler.so \
  CPUPROFILE=prof.out ./main.out <span class="hljs-number">10000</span>
</code></pre>
<p>Alternatively, we can build the library in at link time, dispensing passing <code>LD_PRELOAD</code> at runtime:</p>
<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">gcc -Wl,--no-as-needed,-lprofiler,--as-needed -ggdb3 -O3 -std=c99 -Wall -Wextra -pedantic -o main.out main.c
CPUPROFILE=prof.out ./main.out <span class="hljs-number">10000</span>
</code></pre>
<p>See also: <a href="https://stackoverflow.com/questions/46949407/gperftools-profile-file-not-dumped">gperftools - profile file not dumped</a></p>
<p>The nicest way to view this data I've found so far is to make pprof output the same format that kcachegrind takes as input (yes, the Valgrind-project-viewer-tool) and use kcachegrind to view that:</p>
<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">google-pprof --callgrind main.out prof.out  &gt; callgrind.out
kcachegrind callgrind.out
</code></pre>
<p>After running with either of those methods, we get a <code>prof.out</code> profile data file as output. We can view that file graphically as an SVG with:</p>
<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">google-pprof --web main.out prof.out
</code></pre>
<p><a href="https://i.stack.imgur.com/SiISF.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/SiISF.png" alt="enter image description here"></a></p>
<p>which gives as a familiar call graph like other tools, but with the clunky unit of number of samples rather than seconds.</p>
<p>Alternatively, we can also get some textual data with:</p>
<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">google-pprof --text main.out prof.out
</code></pre>
<p>which gives:</p>
<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">Using local file main.out.
Using local file prof.out.
Total: <span class="hljs-number">187</span> samples
     <span class="hljs-number">187</span> <span class="hljs-number">100.0</span>% <span class="hljs-number">100.0</span>%      <span class="hljs-number">187</span> <span class="hljs-number">100.0</span>% common
       <span class="hljs-number">0</span>   <span class="hljs-number">0.0</span>% <span class="hljs-number">100.0</span>%      <span class="hljs-number">187</span> <span class="hljs-number">100.0</span>% __libc_start_main
       <span class="hljs-number">0</span>   <span class="hljs-number">0.0</span>% <span class="hljs-number">100.0</span>%      <span class="hljs-number">187</span> <span class="hljs-number">100.0</span>% _start
       <span class="hljs-number">0</span>   <span class="hljs-number">0.0</span>% <span class="hljs-number">100.0</span>%        <span class="hljs-number">4</span>   <span class="hljs-number">2.1</span>% fast
       <span class="hljs-number">0</span>   <span class="hljs-number">0.0</span>% <span class="hljs-number">100.0</span>%      <span class="hljs-number">187</span> <span class="hljs-number">100.0</span>% main
       <span class="hljs-number">0</span>   <span class="hljs-number">0.0</span>% <span class="hljs-number">100.0</span>%      <span class="hljs-number">183</span>  <span class="hljs-number">97.9</span>% maybe_slow
</code></pre>
<p>See also: <a href="https://stackoverflow.com/questions/10874308/how-to-use-google-perf-tools">How to use google perf tools</a></p>
<p><strong>Instrument your code with raw <code>perf_event_open</code> syscalls</strong></p>
<p>I think this is the same underlying subsystem that <code>perf</code> uses, but you could of course attain even greater control by explicitly instrumenting your program at compile time with events of interest.</p>
<p>This is likely too hardcore for most people, but it's kind of fun. Minimal runnable example at: <a href="https://stackoverflow.com/questions/13313510/quick-way-to-count-number-of-instructions-executed-in-a-c-program/64863392#64863392">Quick way to count number of instructions executed in a C program</a></p>
<p><strong>Intel VTune</strong></p>
<p><a href="https://en.wikipedia.org/wiki/VTune" rel="nofollow noreferrer">https://en.wikipedia.org/wiki/VTune</a></p>
<p>This seems to be closed source and x86-only, but it is likely to be amazing from what I've heard. I'm not sure how free it is to use, but it seems to be free to download. TODO evaluate.</p>
<p>Tested in Ubuntu 18.04, gprof2dot 2019.11.30, valgrind 3.13.0, perf 4.15.18, Linux kernel 4.15.0, FLameGraph 1a0dc6985aad06e76857cf2a354bd5ba0c9ce96b, gperftools 2.5-2.</p>
    </div></div></div></div><div id="solution9" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 9</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiling">profiling</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p><strong>Use Valgrind, callgrind and kcachegrind:</strong> </p>

<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">valgrind --tool=callgrind ./(Your binary)
</code></pre>

<p>generates callgrind.out.x. Read it using kcachegrind.</p>

<p><strong>Use gprof (add -pg):</strong> </p>

<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">cc -o myprog myprog.c utils.c -g -pg 
</code></pre>

<p>(not so good for multi-threads, function pointers)</p>

<p><strong>Use google-perftools:</strong> </p>

<p>Uses time sampling, I/O and CPU bottlenecks are revealed.</p>

<p><strong>Intel VTune is the best (free for educational purposes).</strong></p>

<p><strong>Others:</strong> AMD Codeanalyst (since replaced with AMD CodeXL), OProfile, 'perf' tools (apt-get install linux-tools)</p>
    </div></div></div></div><div id="solution10" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 10</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiling">profiling</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>For single-threaded programs you can use <strong>igprof</strong>, The Ignominous Profiler: <a href="https://igprof.org/" rel="noreferrer">https://igprof.org/</a> .</p>

<p>It is a sampling profiler, along the lines of the... long... answer by Mike Dunlavey, which will gift wrap the results in a browsable call stack tree, annotated with the time or memory spent in each function, either cumulative or per-function.</p>
    </div></div></div></div><div id="solution11" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 11</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiling">profiling</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Also worth mentioning are</p>

<ol>
<li>HPCToolkit (<a href="http://hpctoolkit.org/" rel="noreferrer">http://hpctoolkit.org/</a>) - Open-source, works for parallel programs and has a GUI with which to look at the results multiple ways</li>
<li>Intel VTune (<a href="https://software.intel.com/en-us/vtune" rel="noreferrer">https://software.intel.com/en-us/vtune</a>) - If you have intel compilers this is very good </li>
<li>TAU (<a href="http://www.cs.uoregon.edu/research/tau/home.php" rel="noreferrer">http://www.cs.uoregon.edu/research/tau/home.php</a>) </li>
</ol>

<p>I have used HPCToolkit and VTune and they are very effective at finding the long pole in the tent and do not need your code to be recompiled (except that you have to use -g -O or RelWithDebInfo type build in CMake to get meaningful output). I have heard TAU is similar in capabilities.</p>
    </div></div></div></div><div id="solution12" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 12</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiling">profiling</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Actually a bit surprised not many mentioned about <a href="https://github.com/google/benchmark" rel="nofollow noreferrer">google/benchmark</a> , while it is a bit cumbersome to pin the specific area of code, specially if the code base is a little big one, however I found this really helpful when used in combination with <code>callgrind</code></p>
<p>IMHO identifying the piece that is causing bottleneck is the key here. I'd however try and answer the following questions first and choose tool based on that</p>
<ol>
<li>is my algorithm correct ?</li>
<li>are there locks that are proving to be bottle necks ?</li>
<li>is there a specific section of code that's proving to be a culprit ?</li>
<li>how about IO, handled and optimized ?</li>
</ol>
<p><code>valgrind</code> with the combination of <code>callgrind</code> and <code>kcachegrind</code> should provide a decent estimation on the points above, and once it's established that there are issues with some section of code, I'd suggest to do a micro bench mark - <code>google benchmark</code> is a good place to start.</p>
    </div></div></div></div><div id="solution13" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 13</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiling">profiling</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>These are the two methods I use for speeding up my code:</p>

<p><strong><em>For CPU bound applications:</em></strong></p>

<ol>
<li>Use a profiler in DEBUG mode to identify questionable parts of your code</li>
<li>Then switch to RELEASE mode and comment out the questionable sections of your code (stub it with nothing) until you see changes in performance.</li>
</ol>

<p><strong><em>For I/O bound applications:</em></strong></p>

<ol>
<li>Use a profiler in RELEASE mode to identify questionable parts of your code.</li>
</ol>

<hr>

<p>N.B.</p>

<p>If you don't have a profiler, use the poor man's profiler. Hit pause while debugging your application. Most developer suites will break into assembly with commented line numbers. You're statistically likely to land in a region that is eating most of your CPU cycles.</p>

<p>For CPU, the reason for profiling in <strong>DEBUG</strong> mode is because if your tried profiling in <strong>RELEASE</strong> mode, the compiler is going to reduce math, vectorize loops, and inline functions which tends to glob your code into an un-mappable mess when it's assembled. <strong>An un-mappable mess means your profiler will not be able to clearly identify what is taking so long because the assembly may not correspond to the source code under optimization</strong>. If you need the performance (e.g. timing sensitive) of <strong>RELEASE</strong> mode, disable debugger features as needed to keep a usable performance.</p>

<p>For I/O-bound, the profiler can still identify I/O operations in <strong>RELEASE</strong> mode because I/O operations are either externally linked to a shared library (most of the time) or in the worst case, will result in a sys-call interrupt vector (which is also easily identifiable by the profiler).</p>
    </div></div></div></div><div id="solution14" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 14</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiling">profiling</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>You can use the iprof library:</p>

<p><a href="https://gitlab.com/Neurochrom/iprof" rel="nofollow noreferrer">https://gitlab.com/Neurochrom/iprof</a></p>

<p><a href="https://github.com/Neurochrom/iprof" rel="nofollow noreferrer">https://github.com/Neurochrom/iprof</a></p>

<p>It's cross-platform and allows you not to measure performance of your application also in real-time. You can even couple it with a live graph.
Full disclaimer: I am the author.</p>
    </div></div></div></div><div id="solution15" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 15</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiling">profiling</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>You can use a logging framework like <a href="https://github.com/emilk/loguru" rel="nofollow noreferrer"><code>loguru</code></a> since it includes timestamps and total uptime which can be used nicely for profiling:</p>

<p><a href="https://i.stack.imgur.com/eHqdn.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/eHqdn.png" alt=""></a></p>
    </div></div></div></div><div id="solution16" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 16</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiling">profiling</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>At work we have a really nice tool that helps us monitoring what we want in terms of scheduling. This has been useful numerous times.</p>

<p>It's in C++ and must be customized to your needs. Unfortunately I can't share code, just concepts.
You use a "large" <code>volatile</code> buffer containing timestamps and event ID that you can dump post mortem or after stopping the logging system (and dump this into a file for example).</p>

<p>You retrieve the so-called large buffer with all the data and a small interface parses it and shows events with name (up/down + value) like an oscilloscope does with colors (configured in <code>.hpp</code> file).</p>

<p>You customize the amount of events generated to focus solely on what you desire. It helped us a lot for scheduling issues while consuming the amount of CPU we wanted based on the amount of logged events per second. </p>

<p>You need 3 files : </p>

<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">toolname.hpp <span class="hljs-comment">// interface</span>
toolname.cpp <span class="hljs-comment">// code</span>
tool_events_id.hpp <span class="hljs-comment">// Events ID</span>
</code></pre>

<p>The concept is to define events in <code>tool_events_id.hpp</code> like that :</p>

<pre class="lang-cpp s-code-block"><code class="hljs language-cpp"><span class="hljs-comment">// EVENT_NAME                         ID      BEGIN_END BG_COLOR NAME</span>
<span class="hljs-meta">#<span class="hljs-keyword">define</span> SOCK_PDU_RECV_D               0x0301  <span class="hljs-comment">//@D00301 BGEEAAAA # TX_PDU_Recv</span></span>
<span class="hljs-meta">#<span class="hljs-keyword">define</span> SOCK_PDU_RECV_F               0x0302  <span class="hljs-comment">//@F00301 BGEEAAAA # TX_PDU_Recv</span></span>
</code></pre>

<p>You also define a few functions in <code>toolname.hpp</code> :</p>

<pre class="lang-cpp s-code-block"><code class="hljs language-cpp"><span class="hljs-meta">#<span class="hljs-keyword">define</span> LOG_LEVEL_ERROR 0</span>
<span class="hljs-meta">#<span class="hljs-keyword">define</span> LOG_LEVEL_WARN 1</span>
<span class="hljs-comment">// ...</span>

<span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">init</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span></span>;
<span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">probe</span><span class="hljs-params">(id,payload)</span></span>;
<span class="hljs-comment">// etc</span>
</code></pre>

<p>Wherever in you code you can use :</p>

<pre class="lang-cpp s-code-block"><code class="hljs language-cpp">toolname&lt;LOG_LEVEL&gt;::<span class="hljs-built_in">log</span>(EVENT_NAME,VALUE);
</code></pre>

<p>The <code>probe</code> function uses a few assembly lines to retrieve the clock timestamp ASAP and then sets an entry in the buffer. We also have an atomic increment to safely find an index where to store the log event.
Of course buffer is circular.</p>

<p>Hope the idea is not obfuscated by the lack of sample code.</p>
    </div></div></div></div><div id="solution17" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 17</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiling">profiling</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p><strong>use a debugging software</strong> 
how to identify where the code is running slowly ?</p>

<p><strong>just think you have a obstacle while you are in motion then it will decrease your speed</strong>  </p>

<p>like that unwanted reallocation's looping,buffer overflows,searching,memory leakages etc operations consumes more execution power it will effect adversely over performance of the code,
Be sure to add -pg to compilation before profiling:</p>

<p><code>g++ your_prg.cpp -pg</code> or <code>cc my_program.cpp -g -pg</code> as per your compiler</p>

<p>haven't tried it yet but I've heard good things about google-perftools. It is definitely worth a try.</p>

<p><code>valgrind --tool=callgrind ./(Your binary)</code></p>

<p>It will generate a file called gmon.out or callgrind.out.x. You can then use kcachegrind or debugger tool to read this file. It will give you a graphical analysis of things with results like which lines cost how much. </p>

<p>i think so</p>
    </div></div></div></div><div id="solution18" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 18</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiling">profiling</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>As no one mentioned Arm MAP, I'd add it as personally I have successfully used Map to profile a C++ scientific program. </p>

<p>Arm MAP is the profiler for parallel, multithreaded or single threaded C, C++, Fortran and F90 codes.  It provides in-depth analysis and bottleneck pinpointing to the source line.  Unlike most profilers, it's designed to be able to profile pthreads, OpenMP or MPI for parallel and threaded code.</p>

<p>MAP is commercial software. </p>
    </div></div></div></div><div id="solution19" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 19</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiling">profiling</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Use <code>-pg</code> flag when compiling and linking the code and run the executable file. While this program is executed, profiling data is collected in the file a.out.<br>
There is two different type of profiling<br></p>

<p>1- Flat profiling: <br>
 by running the command <code>gprog --flat-profile a.out</code> you got the following data<br>
 - what percentage of the overall time was spent for the function,<br>
 - how many seconds were spent in a functionincluding and excluding calls to sub-functions,<br>
 - the number of calls,<br>
 - the average time per call.<br></p>

<p>2- graph profiling<br>
us the command <code>gprof --graph a.out</code> to get the following data for each function which includes<br>
 - In each section, one function is marked with an index number.<br>
 - Above function , there is a list of functions that call the function .<br>
 - Below function , there is a list of functions that are called by the function .<br></p>

<p>To get more info you can look in <a href="https://sourceware.org/binutils/docs-2.32/gprof/" rel="nofollow noreferrer">https://sourceware.org/binutils/docs-2.32/gprof/</a><br></p>
    </div></div></div></div></div></div><div class="widget"><a href="/questions/difference-between-single-and-double-quotes-in-bash-1657385460827">Difference between single and double quotes in Bash</a><a href="/questions/trouble-with-utf-8-characters-what-i-see-is-not-what-i-stored-1657384817490">Trouble with UTF-8 characters; what I see is not what I stored</a><a href="/questions/how-do-i-profile-a-python-script-1657388346692">How do I profile a Python script?</a><a href="/questions/are-dictionaries-ordered-in-python-3.6+-1657387834234">Are dictionaries ordered in Python 3.6+?</a><a href="/questions/sorting-object-property-by-values-1657388367300">Sorting object property by values</a><a href="/questions/how-to-test-multiple-variables-for-equality-against-a-single-value-1657384358504">How to test multiple variables for equality against a single value?</a><a href="/questions/how-do-i-pass-variables-and-data-from-php-to-javascript-1657384684553">How do I pass variables and data from PHP to JavaScript?</a><a href="/questions/how-to-print-without-a-newline-or-space-1657387814213">How to print without a newline or space</a><a href="/questions/how-do-you-use-a-variable-in-a-regular-expression-1657387939007">How do you use a variable in a regular expression?</a><a href="/questions/sorting-an-array-of-objects-by-property-values-1657387447490">Sorting an array of objects by property values</a><a href="/questions/selenium-%22selenium.common.exceptions.nosuchelementexception%22-when-using-chrome-1657388136699">Selenium &quot;selenium.common.exceptions.NoSuchElementException&quot; when using Chrome</a><a href="/questions/scanner-is-skipping-nextline()-after-using-next()-or-nextfoo()-1657384379697">Scanner is skipping nextLine() after using next() or nextFoo()?</a><a href="/questions/why-is-%22using-namespace-std%22-considered-bad-practice-1657384296377">Why is &quot;using namespace std;&quot; considered bad practice?</a><a href="/questions/how-do-javascript-closures-work-1657384418555">How do JavaScript closures work?</a><a href="/questions/can-i-mix-mysql-apis-in-php-1657384597444">Can I mix MySQL APIs in PHP?</a><a href="/questions/how-do-i-properly-compare-strings-in-c-1657387467202">How do I properly compare strings in C?</a><a href="/questions/http-get-with-request-body-1657387379038">HTTP GET with request body</a><a href="/questions/split-array-into-chunks-1657387896420">Split array into chunks</a><a href="/questions/randomize-a-listlesstgreater-1657388172793">Randomize a List&lt;T&gt;</a><a href="/questions/why-does-jquery-or-a-dom-method-such-as-getelementbyid-not-find-the-element-1657384326458">Why does jQuery or a DOM method such as getElementById not find the element?</a></div></div><span class="cursor-pointer text-lg p-2" style="position:fixed;bottom:20px;left:20px;background:#000;z-index:2000;color:white">Go go top</span></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"data":{"answer":["\n\u0026lt;p\u0026gt;If your goal is to use a profiler, use one of the suggested ones.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;However, if you\u0026apos;re in a hurry and you can manually interrupt your program under the debugger while it\u0026apos;s being subjectively slow, there\u0026apos;s a simple way to find performance problems.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Just halt it several times, and each time look at the call stack. If there is some code that is wasting some percentage of the time, 20% or 50% or whatever, that is the probability that you will catch it in the act on each sample. So, that is roughly the percentage of samples on which you will see it. There is no educated guesswork required. If you do have a guess as to what the problem is, this will prove or disprove it.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;You may have multiple performance problems of different sizes. If you clean out any one of them, the remaining ones will take a larger percentage, and be easier to spot, on subsequent passes. This \u0026lt;em\u0026gt;magnification effect\u0026lt;/em\u0026gt;, when compounded over multiple problems, can lead to truly massive speedup factors.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Caveat\u0026lt;/strong\u0026gt;: Programmers tend to be skeptical of this technique unless they\u0026apos;ve used it themselves. They will say that profilers give you this information, but that is only true if they sample the entire call stack, and then let you examine a random set of samples. (The summaries are where the insight is lost.) Call graphs don\u0026apos;t give you the same information, because \u0026lt;/p\u0026gt;\n\n\u0026lt;ol\u0026gt;\n\u0026lt;li\u0026gt;They don\u0026apos;t summarize at the instruction level, and\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;They give confusing summaries in the presence of recursion.\u0026lt;/li\u0026gt;\n\u0026lt;/ol\u0026gt;\n\n\u0026lt;p\u0026gt;They will also say it only works on toy programs, when actually it works on any program, and it seems to work better on bigger programs, because they tend to have more problems to find. They will say it sometimes finds things that aren\u0026apos;t problems, but that is only true if you see something \u0026lt;em\u0026gt;once\u0026lt;/em\u0026gt;. If you see a problem on more than one sample, it is real.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;P.S.\u0026lt;/strong\u0026gt; This can also be done on multi-thread programs if there is a way to collect call-stack samples of the thread pool at a point in time, as there is in Java.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;P.P.S\u0026lt;/strong\u0026gt; As a rough generality, the more layers of abstraction you have in your software, the more likely you are to find that that is the cause of performance problems (and the opportunity to get speedup).\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Added\u0026lt;/strong\u0026gt;: It might not be obvious, but the stack sampling technique works equally well in the presence of recursion. The reason is that the time that would be saved by removal of an instruction is approximated by the fraction of samples containing it, regardless of the number of times it may occur within a sample.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Another objection I often hear is: \u0026quot;\u0026lt;em\u0026gt;It will stop someplace random, and it will miss the real problem\u0026lt;/em\u0026gt;\u0026quot;.\nThis comes from having a prior concept of what the real problem is.\nA key property of performance problems is that they defy expectations.\nSampling tells you something is a problem, and your first reaction is disbelief.\nThat is natural, but you can be sure if it finds a problem it is real, and vice-versa.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Added\u0026lt;/strong\u0026gt;: Let me make a Bayesian explanation of how it works.  Suppose there is some instruction \u0026lt;code\u0026gt;I\u0026lt;/code\u0026gt; (call or otherwise) which is on the call stack some fraction \u0026lt;code\u0026gt;f\u0026lt;/code\u0026gt; of the time (and thus costs that much). For simplicity, suppose we don\u0026apos;t know what \u0026lt;code\u0026gt;f\u0026lt;/code\u0026gt; is, but assume it is either 0.1, 0.2, 0.3, ... 0.9, 1.0, and the prior probability of each of these possibilities is 0.1, so all of these costs are equally likely a-priori.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Then suppose we take just 2 stack samples, and we see instruction \u0026lt;code\u0026gt;I\u0026lt;/code\u0026gt; on both samples, designated observation \u0026lt;code\u0026gt;o=2/2\u0026lt;/code\u0026gt;. This gives us new estimates of the frequency \u0026lt;code\u0026gt;f\u0026lt;/code\u0026gt; of \u0026lt;code\u0026gt;I\u0026lt;/code\u0026gt;, according to this:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-function\u0026quot;\u0026gt;Prior                                    \n\u0026lt;span class=\u0026quot;hljs-title\u0026quot;\u0026gt;P\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-params\u0026quot;\u0026gt;(f=x)\u0026lt;/span\u0026gt; x  \u0026lt;span class=\u0026quot;hljs-title\u0026quot;\u0026gt;P\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-params\u0026quot;\u0026gt;(o=\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;|f=x)\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-title\u0026quot;\u0026gt;P\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-params\u0026quot;\u0026gt;(o=\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;\u0026amp;amp;\u0026amp;amp;f=x)\u0026lt;/span\u0026gt;  \u0026lt;span class=\u0026quot;hljs-title\u0026quot;\u0026gt;P\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-params\u0026quot;\u0026gt;(o=\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;\u0026amp;amp;\u0026amp;amp;f \u0026amp;gt;= x)\u0026lt;/span\u0026gt;  \u0026lt;span class=\u0026quot;hljs-title\u0026quot;\u0026gt;P\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-params\u0026quot;\u0026gt;(f \u0026amp;gt;= x | o=\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;)\u0026lt;/span\u0026gt;\n\n0.1    1     1             0.1          0.1            0.25974026\n0.1    0.9   0.81          0.081        0.181          0.47012987\n0.1    0.8   0.64          0.064        0.245          0.636363636\n0.1    0.7   0.49          0.049        0.294          0.763636364\n0.1    0.6   0.36          0.036        0.33           0.857142857\n0.1    0.5   0.25          0.025        0.355          0.922077922\n0.1    0.4   0.16          0.016        0.371          0.963636364\n0.1    0.3   0.09          0.009        0.38           0.987012987\n0.1    0.2   0.04          0.004        0.384          0.997402597\n0.1    0.1   0.01          0.001        0.385          1\n\n                  \u0026lt;span class=\u0026quot;hljs-title\u0026quot;\u0026gt;P\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-params\u0026quot;\u0026gt;(o=\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;)\u0026lt;/span\u0026gt; 0.385                \n\u0026lt;/span\u0026gt;\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;The last column says that, for example, the probability that \u0026lt;code\u0026gt;f\u0026lt;/code\u0026gt; \u0026amp;gt;= 0.5 is 92%, up from the prior assumption of 60%.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Suppose the prior assumptions are different. Suppose we assume \u0026lt;code\u0026gt;P(f=0.1)\u0026lt;/code\u0026gt; is .991 (nearly certain), and all the other possibilities are almost impossible (0.001). In other words, our prior certainty is that \u0026lt;code\u0026gt;I\u0026lt;/code\u0026gt; is cheap. Then we get:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-function\u0026quot;\u0026gt;Prior                                    \n\u0026lt;span class=\u0026quot;hljs-title\u0026quot;\u0026gt;P\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-params\u0026quot;\u0026gt;(f=x)\u0026lt;/span\u0026gt; x  \u0026lt;span class=\u0026quot;hljs-title\u0026quot;\u0026gt;P\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-params\u0026quot;\u0026gt;(o=\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;|f=x)\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-title\u0026quot;\u0026gt;P\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-params\u0026quot;\u0026gt;(o=\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;\u0026amp;amp;\u0026amp;amp; f=x)\u0026lt;/span\u0026gt;  \u0026lt;span class=\u0026quot;hljs-title\u0026quot;\u0026gt;P\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-params\u0026quot;\u0026gt;(o=\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;\u0026amp;amp;\u0026amp;amp;f \u0026amp;gt;= x)\u0026lt;/span\u0026gt;  \u0026lt;span class=\u0026quot;hljs-title\u0026quot;\u0026gt;P\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-params\u0026quot;\u0026gt;(f \u0026amp;gt;= x | o=\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;)\u0026lt;/span\u0026gt;\n\n0.001  1    1              0.001        0.001          0.072727273\n0.001  0.9  0.81           0.00081      0.00181        0.131636364\n0.001  0.8  0.64           0.00064      0.00245        0.178181818\n0.001  0.7  0.49           0.00049      0.00294        0.213818182\n0.001  0.6  0.36           0.00036      0.0033         0.24\n0.001  0.5  0.25           0.00025      0.00355        0.258181818\n0.001  0.4  0.16           0.00016      0.00371        0.269818182\n0.001  0.3  0.09           0.00009      0.0038         0.276363636\n0.001  0.2  0.04           0.00004      0.00384        0.279272727\n0.991  0.1  0.01           0.00991      0.01375        1\n\n                  \u0026lt;span class=\u0026quot;hljs-title\u0026quot;\u0026gt;P\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-params\u0026quot;\u0026gt;(o=\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;)\u0026lt;/span\u0026gt; 0.01375                \n\u0026lt;/span\u0026gt;\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;Now it says \u0026lt;code\u0026gt;P(f \u0026amp;gt;= 0.5)\u0026lt;/code\u0026gt; is 26%, up from the prior assumption of 0.6%. So Bayes allows us to update our estimate of the probable cost of \u0026lt;code\u0026gt;I\u0026lt;/code\u0026gt;. If the amount of data is small, it doesn\u0026apos;t tell us accurately what the cost is, only that it is big enough to be worth fixing.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Yet another way to look at it is called the \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Rule_of_succession\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;Rule Of Succession\u0026lt;/a\u0026gt;.\nIf you flip a coin 2 times, and it comes up heads both times, what does that tell you about the probable weighting of the coin?\nThe respected way to answer is to say that it\u0026apos;s a Beta distribution, with average value \u0026lt;code\u0026gt;(number of hits + 1) / (number of tries + 2) = (2+1)/(2+2) = 75%\u0026lt;/code\u0026gt;.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;(The key is that we see \u0026lt;code\u0026gt;I\u0026lt;/code\u0026gt; more than once. If we only see it once, that doesn\u0026apos;t tell us much except that \u0026lt;code\u0026gt;f\u0026lt;/code\u0026gt; \u0026amp;gt; 0.)\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;So, even a very small number of samples can tell us a lot about the cost of instructions that it sees. (And it will see them with a frequency, on average, proportional to their cost. If \u0026lt;code\u0026gt;n\u0026lt;/code\u0026gt; samples are taken, and \u0026lt;code\u0026gt;f\u0026lt;/code\u0026gt; is the cost, then \u0026lt;code\u0026gt;I\u0026lt;/code\u0026gt; will appear on \u0026lt;code\u0026gt;nf+/-sqrt(nf(1-f))\u0026lt;/code\u0026gt; samples. Example, \u0026lt;code\u0026gt;n=10\u0026lt;/code\u0026gt;, \u0026lt;code\u0026gt;f=0.3\u0026lt;/code\u0026gt;, that is \u0026lt;code\u0026gt;3+/-1.4\u0026lt;/code\u0026gt; samples.)\u0026lt;/p\u0026gt;\n\n\u0026lt;hr\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Added\u0026lt;/strong\u0026gt;: To give an intuitive feel for the difference between measuring and random stack sampling:\u0026lt;br\u0026gt;\nThere are profilers now that sample the stack, even on wall-clock time, but \u0026lt;em\u0026gt;what comes out\u0026lt;/em\u0026gt; is measurements (or hot path, or hot spot, from which a \u0026quot;bottleneck\u0026quot; can easily hide). What they don\u0026apos;t show you (and they easily could) is the actual samples themselves. And if your goal is to \u0026lt;em\u0026gt;find\u0026lt;/em\u0026gt; the bottleneck, the number of them you need to see is, \u0026lt;em\u0026gt;on average\u0026lt;/em\u0026gt;, 2 divided by the fraction of time it takes.\nSo if it takes 30% of time, 2/.3 = 6.7 samples, on average, will show it, and the chance that 20 samples will show it is 99.2%.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Here is an off-the-cuff illustration of the difference between examining measurements and examining stack samples.\nThe bottleneck could be one big blob like this, or numerous small ones, it makes no difference.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;https://i.stack.imgur.com/FpWuS.png\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;\u0026lt;img src=\u0026quot;https://i.stack.imgur.com/FpWuS.png\u0026quot; alt=\u0026quot;enter image description here\u0026quot;\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Measurement is horizontal; it tells you what fraction of time specific routines take.\nSampling is vertical.\nIf there is any way to avoid what the whole program is doing at that moment, \u0026lt;em\u0026gt;and if you see it on a second sample\u0026lt;/em\u0026gt;, you\u0026apos;ve found the bottleneck.\nThat\u0026apos;s what makes the difference - seeing the whole reason for the time being spent, not just how much.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Use \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Valgrind\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;Valgrind\u0026lt;/a\u0026gt; with the following options:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;valgrind --tool=callgrind ./(Your binary)\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;This generates a file called \u0026lt;code\u0026gt;callgrind.out.x\u0026lt;/code\u0026gt;. Use the \u0026lt;code\u0026gt;kcachegrind\u0026lt;/code\u0026gt; tool to read this file. It will give you a graphical analysis of things with results like which lines cost how much.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;I assume you\u0026apos;re using GCC. The standard solution would be to profile with \u0026lt;a href=\u0026quot;http://www.math.utah.edu/docs/info/gprof_toc.html\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;gprof\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Be sure to add \u0026lt;code\u0026gt;-pg\u0026lt;/code\u0026gt; to compilation before profiling:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;cc -o myprog myprog.c utils.c -g -pg\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;I haven\u0026apos;t tried it yet but I\u0026apos;ve heard good things about \u0026lt;a href=\u0026quot;https://github.com/gperftools/gperftools\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;google-perftools\u0026lt;/a\u0026gt;. It is definitely worth a try.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Related question \u0026lt;a href=\u0026quot;https://stackoverflow.com/questions/56672/how-do-you-profile-your-code\u0026quot;\u0026gt;here\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;A few other buzzwords if \u0026lt;code\u0026gt;gprof\u0026lt;/code\u0026gt; does not do the job for you: \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Valgrind\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;Valgrind\u0026lt;/a\u0026gt;, Intel \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/VTune\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;VTune\u0026lt;/a\u0026gt;, Sun \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/DTrace\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;DTrace\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Newer kernels (e.g. the latest Ubuntu kernels) come with the new \u0026apos;perf\u0026apos; tools (\u0026lt;code\u0026gt;apt-get install linux-tools\u0026lt;/code\u0026gt;) AKA \u0026lt;a href=\u0026quot;https://en.wikipedia.org/wiki/Perf_(Linux)\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;perf_events\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;These come with classic sampling profilers (\u0026lt;a href=\u0026quot;http://manpages.ubuntu.com/manpages/trusty/man1/perf.1.html\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;man-page\u0026lt;/a\u0026gt;) as well as the awesome \u0026lt;a href=\u0026quot;http://web.archive.org/web/20090922171904/http://blog.fenrus.org/?p=5\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;timechart\u0026lt;/a\u0026gt;!\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;The important thing is that these tools can be \u0026lt;strong\u0026gt;system profiling\u0026lt;/strong\u0026gt; and not just process profiling - they can show the interaction between threads, processes and the kernel and let you understand the scheduling and I/O dependencies between processes.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;img src=\u0026quot;https://i.stack.imgur.com/FMYp4.png\u0026quot; alt=\u0026quot;Alt text\u0026quot;\u0026gt;\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;The answer to run \u0026lt;code\u0026gt;valgrind --tool=callgrind\u0026lt;/code\u0026gt; is not quite complete without some options. We usually do not want to profile 10 minutes of slow startup time under Valgrind and want to profile our program when it is doing some task.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;So this is what I recommend. Run program first:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;valgrind --tool=callgrind --dump-instr=yes -v --instr-atstart=no ./binary \u0026amp;gt; tmp\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;Now when it works and we want to start profiling we should run in another window:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;callgrind_control -i on\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;This turns profiling on. To turn it off and stop whole task we might use:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;callgrind_control -k\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;Now we have some files named callgrind.out.* in current directory. To see profiling results use:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;kcachegrind callgrind.out.*\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;I recommend in next window to click on \u0026quot;Self\u0026quot; column header, otherwise it shows that \u0026quot;main()\u0026quot; is most time consuming task. \u0026quot;Self\u0026quot; shows how much each function itself took time, not together with dependents. \u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;I would use Valgrind and Callgrind as a base for my profiling tool suite. What is important to know is that Valgrind is basically a Virtual Machine:\u0026lt;/p\u0026gt;\n\n\u0026lt;blockquote\u0026gt;\n  \u0026lt;p\u0026gt;(wikipedia) Valgrind is in essence a virtual\n  machine using just-in-time (JIT)\n  compilation techniques, including\n  dynamic recompilation. Nothing from\n  the original program ever gets run\n  directly on the host processor.\n  Instead, Valgrind first translates the\n  program into a temporary, simpler form\n  called Intermediate Representation\n  (IR), which is a processor-neutral,\n  SSA-based form. After the conversion,\n  a tool (see below) is free to do\n  whatever transformations it would like\n  on the IR, before Valgrind translates\n  the IR back into machine code and lets\n  the host processor run it. \u0026lt;/p\u0026gt;\n\u0026lt;/blockquote\u0026gt;\n\n\u0026lt;p\u0026gt;Callgrind is a profiler build upon that. Main benefit is that you don\u0026apos;t have to run your aplication for hours to get reliable result. Even one second run is sufficient to get rock-solid, reliable results, because Callgrind is a \u0026lt;strong\u0026gt;non-probing\u0026lt;/strong\u0026gt; profiler. \u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Another tool build upon Valgrind is Massif. I use it to profile heap memory usage. It works great. What it does is that it gives you snapshots of memory usage -- detailed information WHAT holds WHAT percentage of memory, and WHO had put it there. Such information is available at different points of time of application run.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;This is a response to \u0026lt;a href=\u0026quot;https://stackoverflow.com/a/375930/321731\u0026quot;\u0026gt;Nazgob\u0026apos;s Gprof answer\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;I\u0026apos;ve been using Gprof the last couple of days and have already found three significant limitations, one of which I\u0026apos;ve not seen documented anywhere else (yet):\u0026lt;/p\u0026gt;\n\n\u0026lt;ol\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;It doesn\u0026apos;t work properly on multi-threaded code, unless you use a \u0026lt;a href=\u0026quot;http://sam.zoy.org/writings/programming/gprof.html\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;workaround\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;The call graph gets confused by function pointers. Example: I have a function called \u0026lt;code\u0026gt;multithread()\u0026lt;/code\u0026gt; which enables me to multi-thread a specified function over a specified array (both passed as arguments). Gprof however, views all calls to \u0026lt;code\u0026gt;multithread()\u0026lt;/code\u0026gt; as equivalent for the purposes of computing time spent in children. Since some functions I pass to \u0026lt;code\u0026gt;multithread()\u0026lt;/code\u0026gt; take much longer than others my call graphs are mostly useless. (To those wondering if threading is the issue here: no, \u0026lt;code\u0026gt;multithread()\u0026lt;/code\u0026gt; can optionally, and did in this case, run everything sequentially on the calling thread only).\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;It says \u0026lt;a href=\u0026quot;http://www.cs.utah.edu/dept/old/texinfo/as/gprof.html\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;here\u0026lt;/a\u0026gt; that \u0026quot;... the number-of-calls figures are derived by counting, not sampling. They are completely accurate...\u0026quot;. Yet I find my call graph giving me 5345859132+784984078 as call stats to my most-called function, where the first number is supposed to be direct calls, and the second recursive calls (which are all from itself). Since this implied I had a bug, I put in long (64-bit) counters into the code and did the same run again. My counts: 5345859132 direct, and 78094395406 self-recursive calls.  There are a lot of digits there, so I\u0026apos;ll point out the recursive calls I measure are 78bn, versus 784m from Gprof: a factor of 100 different. Both runs were single threaded and unoptimised code, one compiled \u0026lt;code\u0026gt;-g\u0026lt;/code\u0026gt; and the other \u0026lt;code\u0026gt;-pg\u0026lt;/code\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;/ol\u0026gt;\n\n\u0026lt;p\u0026gt;This was GNU \u0026lt;a href=\u0026quot;https://en.wikipedia.org/wiki/Gprof\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;Gprof\u0026lt;/a\u0026gt; (GNU Binutils for Debian) 2.18.0.20080103 running under 64-bit Debian Lenny, if that helps anyone.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Survey of C++ profiling techniques: gprof vs valgrind vs perf vs gperftools\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;In this answer, I will use several different tools to a analyze a few very simple test programs, in order to concretely compare how those tools work.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;The following test program is very simple and does the following:\u0026lt;/p\u0026gt;\n\u0026lt;ul\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;code\u0026gt;main\u0026lt;/code\u0026gt; calls \u0026lt;code\u0026gt;fast\u0026lt;/code\u0026gt; and \u0026lt;code\u0026gt;maybe_slow\u0026lt;/code\u0026gt; 3 times, one of the \u0026lt;code\u0026gt;maybe_slow\u0026lt;/code\u0026gt; calls being slow\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;The slow call of \u0026lt;code\u0026gt;maybe_slow\u0026lt;/code\u0026gt; is 10x longer, and dominates runtime if we consider calls to the child function \u0026lt;code\u0026gt;common\u0026lt;/code\u0026gt;. Ideally, the profiling tool will be able to point us to the specific slow call.\u0026lt;/p\u0026gt;\n\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;both \u0026lt;code\u0026gt;fast\u0026lt;/code\u0026gt; and \u0026lt;code\u0026gt;maybe_slow\u0026lt;/code\u0026gt; call \u0026lt;code\u0026gt;common\u0026lt;/code\u0026gt;, which accounts for the bulk of the program execution\u0026lt;/p\u0026gt;\n\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;The program interface is:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;./main.out [n [seed]]\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;and the program does \u0026lt;code\u0026gt;O(n^2)\u0026lt;/code\u0026gt; loops in total. \u0026lt;code\u0026gt;seed\u0026lt;/code\u0026gt; is just to get different output without affecting runtime.\u0026lt;/p\u0026gt;\n\u0026lt;/li\u0026gt;\n\u0026lt;/ul\u0026gt;\n\u0026lt;p\u0026gt;main.c\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-meta\u0026quot;\u0026gt;#\u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;include\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026amp;lt;inttypes.h\u0026amp;gt;\u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\n\u0026lt;span class=\u0026quot;hljs-meta\u0026quot;\u0026gt;#\u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;include\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026amp;lt;stdio.h\u0026amp;gt;\u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\n\u0026lt;span class=\u0026quot;hljs-meta\u0026quot;\u0026gt;#\u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;include\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026amp;lt;stdlib.h\u0026amp;gt;\u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\n\n\u0026lt;span class=\u0026quot;hljs-type\u0026quot;\u0026gt;uint64_t\u0026lt;/span\u0026gt; __attribute__ ((noinline)) \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;common\u0026lt;/span\u0026gt;(\u0026lt;span class=\u0026quot;hljs-type\u0026quot;\u0026gt;uint64_t\u0026lt;/span\u0026gt; n, \u0026lt;span class=\u0026quot;hljs-type\u0026quot;\u0026gt;uint64_t\u0026lt;/span\u0026gt; seed) {\n    \u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;for\u0026lt;/span\u0026gt; (\u0026lt;span class=\u0026quot;hljs-type\u0026quot;\u0026gt;uint64_t\u0026lt;/span\u0026gt; i = \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0\u0026lt;/span\u0026gt;; i \u0026amp;lt; n; ++i) {\n        seed = (seed * seed) - (\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt; * seed) + \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1\u0026lt;/span\u0026gt;;\n    }\n    \u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;return\u0026lt;/span\u0026gt; seed;\n}\n\n\u0026lt;span class=\u0026quot;hljs-type\u0026quot;\u0026gt;uint64_t\u0026lt;/span\u0026gt; __attribute__ ((noinline)) \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;fast\u0026lt;/span\u0026gt;(\u0026lt;span class=\u0026quot;hljs-type\u0026quot;\u0026gt;uint64_t\u0026lt;/span\u0026gt; n, \u0026lt;span class=\u0026quot;hljs-type\u0026quot;\u0026gt;uint64_t\u0026lt;/span\u0026gt; seed) {\n    \u0026lt;span class=\u0026quot;hljs-type\u0026quot;\u0026gt;uint64_t\u0026lt;/span\u0026gt; max = (n / \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;10\u0026lt;/span\u0026gt;) + \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1\u0026lt;/span\u0026gt;;\n    \u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;for\u0026lt;/span\u0026gt; (\u0026lt;span class=\u0026quot;hljs-type\u0026quot;\u0026gt;uint64_t\u0026lt;/span\u0026gt; i = \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0\u0026lt;/span\u0026gt;; i \u0026amp;lt; max; ++i) {\n        seed = \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;common\u0026lt;/span\u0026gt;(n, (seed * seed) - (\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt; * seed) + \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1\u0026lt;/span\u0026gt;);\n    }\n    \u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;return\u0026lt;/span\u0026gt; seed;\n}\n\n\u0026lt;span class=\u0026quot;hljs-type\u0026quot;\u0026gt;uint64_t\u0026lt;/span\u0026gt; __attribute__ ((noinline)) \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;maybe_slow\u0026lt;/span\u0026gt;(\u0026lt;span class=\u0026quot;hljs-type\u0026quot;\u0026gt;uint64_t\u0026lt;/span\u0026gt; n, \u0026lt;span class=\u0026quot;hljs-type\u0026quot;\u0026gt;uint64_t\u0026lt;/span\u0026gt; seed, \u0026lt;span class=\u0026quot;hljs-type\u0026quot;\u0026gt;int\u0026lt;/span\u0026gt; is_slow) {\n    \u0026lt;span class=\u0026quot;hljs-type\u0026quot;\u0026gt;uint64_t\u0026lt;/span\u0026gt; max = n;\n    \u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; (is_slow) {\n        max *= \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;10\u0026lt;/span\u0026gt;;\n    }\n    \u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;for\u0026lt;/span\u0026gt; (\u0026lt;span class=\u0026quot;hljs-type\u0026quot;\u0026gt;uint64_t\u0026lt;/span\u0026gt; i = \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0\u0026lt;/span\u0026gt;; i \u0026amp;lt; max; ++i) {\n        seed = \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;common\u0026lt;/span\u0026gt;(n, (seed * seed) - (\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt; * seed) + \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1\u0026lt;/span\u0026gt;);\n    }\n    \u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;return\u0026lt;/span\u0026gt; seed;\n}\n\n\u0026lt;span class=\u0026quot;hljs-function\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-type\u0026quot;\u0026gt;int\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-title\u0026quot;\u0026gt;main\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-params\u0026quot;\u0026gt;(\u0026lt;span class=\u0026quot;hljs-type\u0026quot;\u0026gt;int\u0026lt;/span\u0026gt; argc, \u0026lt;span class=\u0026quot;hljs-type\u0026quot;\u0026gt;char\u0026lt;/span\u0026gt; **argv)\u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt;{\n    \u0026lt;span class=\u0026quot;hljs-type\u0026quot;\u0026gt;uint64_t\u0026lt;/span\u0026gt; n, seed;\n    \u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; (argc \u0026amp;gt; \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1\u0026lt;/span\u0026gt;) {\n        n = \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;strtoll\u0026lt;/span\u0026gt;(argv[\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1\u0026lt;/span\u0026gt;], \u0026lt;span class=\u0026quot;hljs-literal\u0026quot;\u0026gt;NULL\u0026lt;/span\u0026gt;, \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0\u0026lt;/span\u0026gt;);\n    } \u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;else\u0026lt;/span\u0026gt; {\n        n = \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1\u0026lt;/span\u0026gt;;\n    }\n    \u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; (argc \u0026amp;gt; \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;) {\n        seed = \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;strtoll\u0026lt;/span\u0026gt;(argv[\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;], \u0026lt;span class=\u0026quot;hljs-literal\u0026quot;\u0026gt;NULL\u0026lt;/span\u0026gt;, \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0\u0026lt;/span\u0026gt;);\n    } \u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;else\u0026lt;/span\u0026gt; {\n        seed = \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0\u0026lt;/span\u0026gt;;\n    }\n    seed += \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;maybe_slow\u0026lt;/span\u0026gt;(n, seed, \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0\u0026lt;/span\u0026gt;);\n    seed += \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;fast\u0026lt;/span\u0026gt;(n, seed);\n    seed += \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;maybe_slow\u0026lt;/span\u0026gt;(n, seed, \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1\u0026lt;/span\u0026gt;);\n    seed += \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;fast\u0026lt;/span\u0026gt;(n, seed);\n    seed += \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;maybe_slow\u0026lt;/span\u0026gt;(n, seed, \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0\u0026lt;/span\u0026gt;);\n    seed += \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;fast\u0026lt;/span\u0026gt;(n, seed);\n    \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;printf\u0026lt;/span\u0026gt;(\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;%\u0026quot;\u0026lt;/span\u0026gt; PRIX64 \u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;\\n\u0026quot;\u0026lt;/span\u0026gt;, seed);\n    \u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;return\u0026lt;/span\u0026gt; EXIT_SUCCESS;\n}\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;gprof\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;gprof requires recompiling the software with instrumentation, and it also uses a sampling approach together with that instrumentation. It therefore strikes a balance between accuracy (sampling is not always fully accurate and can skip functions) and execution slowdown (instrumentation and sampling are relatively fast techniques that don\u0026apos;t slow down execution very much).\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;gprof is built-into GCC/binutils, so all we have to do is to compile with the \u0026lt;code\u0026gt;-pg\u0026lt;/code\u0026gt; option to enable gprof. We then run the program normally with a size CLI parameter that produces a run of reasonable duration of a few seconds (\u0026lt;code\u0026gt;10000\u0026lt;/code\u0026gt;):\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;gcc -pg -ggdb3 -O3 -std=c99 -Wall -Wextra -pedantic -o main.out main.c\ntime ./main.out \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;10000\u0026lt;/span\u0026gt;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;For educational reasons, we will also do a run without optimizations enabled. Note that this is useless in practice, as you normally only care about optimizing the performance of the optimized program:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;gcc -pg -ggdb3 -O0 -std=c99 -Wall -Wextra -pedantic -o main.out main.c\n./main.out \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;10000\u0026lt;/span\u0026gt;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;First, \u0026lt;code\u0026gt;time\u0026lt;/code\u0026gt; tells us that the execution time with and without \u0026lt;code\u0026gt;-pg\u0026lt;/code\u0026gt; were the same, which is great: no slowdown! I have however seen accounts of 2x - 3x slowdowns on complex software, e.g. as \u0026lt;a href=\u0026quot;https://gem5.atlassian.net/browse/GEM5-337\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;shown in this ticket\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Because we compiled with \u0026lt;code\u0026gt;-pg\u0026lt;/code\u0026gt;, running the program produces a file \u0026lt;code\u0026gt;gmon.out\u0026lt;/code\u0026gt; file containing the profiling data.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;We can observe that file graphically with \u0026lt;code\u0026gt;gprof2dot\u0026lt;/code\u0026gt; as asked at: \u0026lt;a href=\u0026quot;https://stackoverflow.com/questions/2439060/is-it-possible-to-get-a-graphical-representation-of-gprof-results\u0026quot;\u0026gt;Is it possible to get a graphical representation of gprof results?\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;sudo apt install graphviz\npython3 -m pip install --user gprof2dot\ngprof main.out \u0026amp;gt; main.gprof\ngprof2dot \u0026amp;lt; main.gprof | dot -Tsvg -o output.svg\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;Here, the \u0026lt;code\u0026gt;gprof\u0026lt;/code\u0026gt; tool reads the \u0026lt;code\u0026gt;gmon.out\u0026lt;/code\u0026gt; trace information, and generates a human readable report in \u0026lt;code\u0026gt;main.gprof\u0026lt;/code\u0026gt;, which \u0026lt;code\u0026gt;gprof2dot\u0026lt;/code\u0026gt; then reads to generate a graph.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;The source for gprof2dot is at: \u0026lt;a href=\u0026quot;https://github.com/jrfonseca/gprof2dot\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;https://github.com/jrfonseca/gprof2dot\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;We observe the following for the \u0026lt;code\u0026gt;-O0\u0026lt;/code\u0026gt; run:\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;https://i.stack.imgur.com/mM8NQ.png\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;\u0026lt;img src=\u0026quot;https://i.stack.imgur.com/mM8NQ.png\u0026quot; alt=\u0026quot;enter image description here\u0026quot;\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;and for the \u0026lt;code\u0026gt;-O3\u0026lt;/code\u0026gt; run:\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;https://i.stack.imgur.com/31VNy.png\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;\u0026lt;img src=\u0026quot;https://i.stack.imgur.com/31VNy.png\u0026quot; alt=\u0026quot;enter image description here\u0026quot;\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;The \u0026lt;code\u0026gt;-O0\u0026lt;/code\u0026gt; output is pretty much self-explanatory. For example, it shows that the 3 \u0026lt;code\u0026gt;maybe_slow\u0026lt;/code\u0026gt; calls and their child calls take up 97.56% of the total runtime, although execution of \u0026lt;code\u0026gt;maybe_slow\u0026lt;/code\u0026gt; itself without children accounts for 0.00% of the total execution time, i.e. almost all the time spent in that function was spent on child calls.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;TODO: why is \u0026lt;code\u0026gt;main\u0026lt;/code\u0026gt; missing from the \u0026lt;code\u0026gt;-O3\u0026lt;/code\u0026gt; output, even though I can see it on a \u0026lt;code\u0026gt;bt\u0026lt;/code\u0026gt; in GDB? \u0026lt;a href=\u0026quot;https://stackoverflow.com/questions/39041871/missing-function-from-gprof-output\u0026quot;\u0026gt;Missing function from GProf output\u0026lt;/a\u0026gt; I think it is because gprof is also sampling based in addition to its compiled instrumentation, and the \u0026lt;code\u0026gt;-O3\u0026lt;/code\u0026gt; \u0026lt;code\u0026gt;main\u0026lt;/code\u0026gt; is just too fast and got no samples.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;I choose SVG output instead of PNG because the SVG is searchable with \u0026lt;kbd\u0026gt;Ctrl\u0026lt;/kbd\u0026gt; + \u0026lt;kbd\u0026gt;F\u0026lt;/kbd\u0026gt; and the file size can be about 10x smaller. Also, the width and height of the generated image can be humoungous with tens of thousands of pixels for complex software, and GNOME \u0026lt;code\u0026gt;eog\u0026lt;/code\u0026gt; 3.28.1 bugs out in that case for PNGs, while SVGs get opened by my browser automatically. gimp 2.8 worked well though, see also:\u0026lt;/p\u0026gt;\n\u0026lt;ul\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;a href=\u0026quot;https://askubuntu.com/questions/1112641/how-to-view-extremely-large-images\u0026quot;\u0026gt;https://askubuntu.com/questions/1112641/how-to-view-extremely-large-images\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;a href=\u0026quot;https://unix.stackexchange.com/questions/77968/viewing-large-image-on-linux\u0026quot;\u0026gt;https://unix.stackexchange.com/questions/77968/viewing-large-image-on-linux\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;a href=\u0026quot;https://superuser.com/questions/356038/viewer-for-huge-images-under-linux-100-mp-color-images\u0026quot;\u0026gt;https://superuser.com/questions/356038/viewer-for-huge-images-under-linux-100-mp-color-images\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;/ul\u0026gt;\n\u0026lt;p\u0026gt;but even then, you will be dragging the image around a lot to find what you want, see e.g. this image from a \u0026quot;real\u0026quot; software example taken from \u0026lt;a href=\u0026quot;https://gem5.atlassian.net/browse/GEM5-337\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;this ticket\u0026lt;/a\u0026gt;:\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;https://i.stack.imgur.com/Nvg9G.jpg\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;\u0026lt;img src=\u0026quot;https://i.stack.imgur.com/Nvg9G.jpg\u0026quot; alt=\u0026quot;enter image description here\u0026quot;\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Can you find the most critical call stack easily with all those tiny unsorted spaghetti lines going over one another? There might be better \u0026lt;code\u0026gt;dot\u0026lt;/code\u0026gt; options I\u0026apos;m sure, but I don\u0026apos;t want to go there now. What we really need is a proper dedicated viewer for it, but I haven\u0026apos;t found one yet:\u0026lt;/p\u0026gt;\n\u0026lt;ul\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;a href=\u0026quot;https://stackoverflow.com/questions/7274095/view-gprof-output-in-kcachegrind\u0026quot;\u0026gt;View gprof output in kcachegrind\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;a href=\u0026quot;https://stackoverflow.com/questions/1576666/which-is-the-best-replacement-for-kprof\u0026quot;\u0026gt;Which is the best replacement for KProf?\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;/ul\u0026gt;\n\u0026lt;p\u0026gt;You can however use the color map to mitigate those problems a bit. For example, on the previous huge image, I finally managed to find the critical path on the left when I made the brilliant deduction that green comes after red, followed finally by darker and darker blue.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Alternatively, we can also observe the text output of the \u0026lt;code\u0026gt;gprof\u0026lt;/code\u0026gt; built-in binutils tool which we previously saved at:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;cat main.gprof\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;By default, this produces an extremely verbose output that explains what the output data means. Since I can\u0026apos;t explain better than that, I\u0026apos;ll let you read it yourself.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Once you have understood the data output format, you can reduce verbosity to show just the data without the tutorial with the \u0026lt;code\u0026gt;-b\u0026lt;/code\u0026gt; option:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;gprof -b main.out\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;In our example, outputs were for \u0026lt;code\u0026gt;-O0\u0026lt;/code\u0026gt;:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;Flat profile:\n\nEach sample counts as \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.01\u0026lt;/span\u0026gt; seconds.\n  %   cumulative   self              self     total           \n time   seconds   seconds    calls   s/call   s/call  name    \n\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;100.35\u0026lt;/span\u0026gt;      \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3.67\u0026lt;/span\u0026gt;     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3.67\u0026lt;/span\u0026gt;   \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;123003\u0026lt;/span\u0026gt;     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;  common\n  \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;      \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3.67\u0026lt;/span\u0026gt;     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;        \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt;     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.03\u0026lt;/span\u0026gt;  fast\n  \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;      \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3.67\u0026lt;/span\u0026gt;     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;        \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt;     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1.19\u0026lt;/span\u0026gt;  maybe_slow\n\n            Call graph\n\n\ngranularity: each sample hit covers \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;byte\u0026lt;/span\u0026gt;(s) \u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;for\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.27\u0026lt;/span\u0026gt;% of \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3.67\u0026lt;/span\u0026gt; seconds\n\nindex % time    self  children    called     name\n                \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.09\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3003\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;123003\u0026lt;/span\u0026gt;      fast [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;4\u0026lt;/span\u0026gt;]\n                \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3.58\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;  \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;120000\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;123003\u0026lt;/span\u0026gt;      maybe_slow [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt;]\n[\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1\u0026lt;/span\u0026gt;]    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;100.0\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3.67\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;  \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;123003\u0026lt;/span\u0026gt;         common [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1\u0026lt;/span\u0026gt;]\n-----------------------------------------------\n                                                 \u0026amp;lt;spontaneous\u0026amp;gt;\n[\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;]    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;100.0\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3.67\u0026lt;/span\u0026gt;                 main [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;]\n                \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3.58\u0026lt;/span\u0026gt;       \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt;           maybe_slow [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt;]\n                \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.09\u0026lt;/span\u0026gt;       \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt;           fast [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;4\u0026lt;/span\u0026gt;]\n-----------------------------------------------\n                \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3.58\u0026lt;/span\u0026gt;       \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt;           main [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;]\n[\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt;]     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;97.6\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3.58\u0026lt;/span\u0026gt;       \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt;         maybe_slow [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt;]\n                \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3.58\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;  \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;120000\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;123003\u0026lt;/span\u0026gt;      common [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1\u0026lt;/span\u0026gt;]\n-----------------------------------------------\n                \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.09\u0026lt;/span\u0026gt;       \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt;           main [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;]\n[\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;4\u0026lt;/span\u0026gt;]      \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2.4\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.09\u0026lt;/span\u0026gt;       \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt;         fast [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;4\u0026lt;/span\u0026gt;]\n                \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.09\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3003\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;123003\u0026lt;/span\u0026gt;      common [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1\u0026lt;/span\u0026gt;]\n-----------------------------------------------\n\nIndex by function name\n\n   [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1\u0026lt;/span\u0026gt;] common                  [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;4\u0026lt;/span\u0026gt;] fast                    [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt;] maybe_slow\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;and for \u0026lt;code\u0026gt;-O3\u0026lt;/code\u0026gt;:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;Flat profile:\n\nEach sample counts as \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.01\u0026lt;/span\u0026gt; seconds.\n  %   cumulative   self              self     total           \n time   seconds   seconds    calls  us/call  us/call  name    \n\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;100.52\u0026lt;/span\u0026gt;      \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1.84\u0026lt;/span\u0026gt;     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1.84\u0026lt;/span\u0026gt;   \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;123003\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;14.96\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;14.96\u0026lt;/span\u0026gt;  common\n\n            Call graph\n\n\ngranularity: each sample hit covers \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;byte\u0026lt;/span\u0026gt;(s) \u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;for\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.54\u0026lt;/span\u0026gt;% of \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1.84\u0026lt;/span\u0026gt; seconds\n\nindex % time    self  children    called     name\n                \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.04\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3003\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;123003\u0026lt;/span\u0026gt;      fast [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt;]\n                \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1.79\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;  \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;120000\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;123003\u0026lt;/span\u0026gt;      maybe_slow [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;]\n[\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1\u0026lt;/span\u0026gt;]    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;100.0\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1.84\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;  \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;123003\u0026lt;/span\u0026gt;         common [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1\u0026lt;/span\u0026gt;]\n-----------------------------------------------\n                                                 \u0026amp;lt;spontaneous\u0026amp;gt;\n[\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;]     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;97.6\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1.79\u0026lt;/span\u0026gt;                 maybe_slow [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;]\n                \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1.79\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;  \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;120000\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;123003\u0026lt;/span\u0026gt;      common [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1\u0026lt;/span\u0026gt;]\n-----------------------------------------------\n                                                 \u0026amp;lt;spontaneous\u0026amp;gt;\n[\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt;]      \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2.4\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.04\u0026lt;/span\u0026gt;                 fast [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt;]\n                \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.04\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3003\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;123003\u0026lt;/span\u0026gt;      common [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1\u0026lt;/span\u0026gt;]\n-----------------------------------------------\n\nIndex by function name\n\n   [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1\u0026lt;/span\u0026gt;] common\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;As a very quick summary for each section e.g.:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;                \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3.58\u0026lt;/span\u0026gt;       \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt;           main [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;]\n[\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt;]     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;97.6\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3.58\u0026lt;/span\u0026gt;       \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt;         maybe_slow [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3\u0026lt;/span\u0026gt;]\n                \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;3.58\u0026lt;/span\u0026gt;    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;  \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;120000\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;123003\u0026lt;/span\u0026gt;      common [\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;1\u0026lt;/span\u0026gt;]\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;centers around the function that is left indented (\u0026lt;code\u0026gt;maybe_flow\u0026lt;/code\u0026gt;). \u0026lt;code\u0026gt;[3]\u0026lt;/code\u0026gt; is the ID of that function. Above the function, are its callers, and below it the callees.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;For \u0026lt;code\u0026gt;-O3\u0026lt;/code\u0026gt;, see here like in the graphical output that \u0026lt;code\u0026gt;maybe_slow\u0026lt;/code\u0026gt; and \u0026lt;code\u0026gt;fast\u0026lt;/code\u0026gt; don\u0026apos;t have a known parent, which is what the documentation says that \u0026lt;code\u0026gt;\u0026amp;lt;spontaneous\u0026amp;gt;\u0026lt;/code\u0026gt; means.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;I\u0026apos;m not sure if there is a nice way to do line-by-line profiling with gprof: \u0026lt;a href=\u0026quot;https://stackoverflow.com/questions/9608949/gprof-time-spent-in-particular-lines-of-code\u0026quot;\u0026gt;`gprof` time spent in particular lines of code\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;valgrind callgrind\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;valgrind runs the program through the valgrind virtual machine. This makes the profiling very accurate, but it also produces a very large slowdown of the program. I have also mentioned kcachegrind previously at: \u0026lt;a href=\u0026quot;https://stackoverflow.com/questions/517589/tools-to-get-a-pictorial-function-call-graph-of-code/31190167#31190167\u0026quot;\u0026gt;Tools to get a pictorial function call graph of code\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;callgrind is the valgrind\u0026apos;s tool to profile code and kcachegrind is a KDE program that can visualize cachegrind output.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;First we have to remove the \u0026lt;code\u0026gt;-pg\u0026lt;/code\u0026gt; flag to go back to normal compilation, otherwise the run actually fails with \u0026lt;a href=\u0026quot;https://stackoverflow.com/questions/2146082/valgrind-profiling-timer-expired\u0026quot;\u0026gt;\u0026lt;code\u0026gt;Profiling timer expired\u0026lt;/code\u0026gt;\u0026lt;/a\u0026gt;, and yes, this is so common that I did and there was a Stack Overflow question for it.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;So we compile and run as:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;sudo apt install kcachegrind valgrind\ngcc -ggdb3 -O3 -std=c99 -Wall -Wextra -pedantic -o main.out main.c\ntime valgrind --tool=callgrind valgrind --dump-instr=yes \\\n  --collect-jumps=yes ./main.out \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;10000\u0026lt;/span\u0026gt;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;I enable \u0026lt;code\u0026gt;--dump-instr=yes --collect-jumps=yes\u0026lt;/code\u0026gt; because this also dumps information that enables us to view a per assembly line breakdown of performance, at a relatively small added overhead cost.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Off the bat, \u0026lt;code\u0026gt;time\u0026lt;/code\u0026gt; tells us that the program took 29.5 seconds to execute, so we had a slowdown of about 15x on this example. Clearly, this slowdown is going to be a serious limitation for larger workloads. On the \u0026quot;real world software example\u0026quot; \u0026lt;a href=\u0026quot;https://gem5.atlassian.net/browse/GEM5-337\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;mentioned here\u0026lt;/a\u0026gt;, I observed a slowdown of 80x.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;The run generates a profile data file named \u0026lt;code\u0026gt;callgrind.out.\u0026amp;lt;pid\u0026amp;gt;\u0026lt;/code\u0026gt; e.g. \u0026lt;code\u0026gt;callgrind.out.8554\u0026lt;/code\u0026gt; in my case. We view that file with:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;kcachegrind callgrind.out\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;.8554\u0026lt;/span\u0026gt;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;which shows a GUI that contains data similar to the textual gprof output:\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;https://i.stack.imgur.com/v1kfK.png\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;\u0026lt;img src=\u0026quot;https://i.stack.imgur.com/v1kfK.png\u0026quot; alt=\u0026quot;enter image description here\u0026quot;\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Also, if we go on the bottom right \u0026quot;Call Graph\u0026quot; tab, we see a call graph which we can export by right clicking it to obtain the following image with unreasonable amounts of white border :-)\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;https://i.stack.imgur.com/ZTdAJ.png\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;\u0026lt;img src=\u0026quot;https://i.stack.imgur.com/ZTdAJ.png\u0026quot; alt=\u0026quot;enter image description here\u0026quot;\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;I think \u0026lt;code\u0026gt;fast\u0026lt;/code\u0026gt; is not showing on that graph because kcachegrind must have simplified the visualization because that call takes up too little time, this will likely be the behavior you want on a real program. The right click menu has some settings to control when to cull such nodes, but I couldn\u0026apos;t get it to show such a short call after a quick attempt. If I click on \u0026lt;code\u0026gt;fast\u0026lt;/code\u0026gt; on the left window, it does show a call graph with \u0026lt;code\u0026gt;fast\u0026lt;/code\u0026gt;, so that stack was actually captured. No one had yet found a way to show the complete graph call graph: \u0026lt;a href=\u0026quot;https://stackoverflow.com/questions/33769323/make-callgrind-show-all-function-calls-in-the-kcachegrind-callgraph\u0026quot;\u0026gt;Make callgrind show all function calls in the kcachegrind callgraph\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;TODO on complex C++ software, I see some entries of type \u0026lt;code\u0026gt;\u0026amp;lt;cycle N\u0026amp;gt;\u0026lt;/code\u0026gt;, e.g. \u0026lt;code\u0026gt;\u0026amp;lt;cycle 11\u0026amp;gt;\u0026lt;/code\u0026gt; where I\u0026apos;d expect function names, what does that mean? I noticed there is a \u0026quot;Cycle Detection\u0026quot; button to toggle that on and off, but what does it mean?\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;\u0026lt;code\u0026gt;perf\u0026lt;/code\u0026gt; from \u0026lt;code\u0026gt;linux-tools\u0026lt;/code\u0026gt;\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;code\u0026gt;perf\u0026lt;/code\u0026gt; seems to use exclusively Linux kernel sampling mechanisms. This makes it very simple to setup, but also not fully accurate.\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;sudo apt install linux-tools\ntime perf record -g ./main.out \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;10000\u0026lt;/span\u0026gt;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;This added 0.2s to execution, so we are fine time-wise, but I still don\u0026apos;t see much of interest, after expanding the \u0026lt;code\u0026gt;common\u0026lt;/code\u0026gt; node with the keyboard right arrow:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;Samples: \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;7\u0026lt;/span\u0026gt;K of event \u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026apos;cycles:uppp\u0026apos;\u0026lt;/span\u0026gt;, \u0026lt;span class=\u0026quot;hljs-function\u0026quot;\u0026gt;Event \u0026lt;span class=\u0026quot;hljs-title\u0026quot;\u0026gt;count\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-params\u0026quot;\u0026gt;(approx.)\u0026lt;/span\u0026gt;: \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;6228527608\u0026lt;/span\u0026gt;     \n  Children      Self  Command   Shared Object     Symbol                  \n-   \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;99.98\u0026lt;/span\u0026gt;%    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;99.88\u0026lt;/span\u0026gt;%  main.out  main.out          [.] common              \n     common                                                               \n     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.11\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.11\u0026lt;/span\u0026gt;%  main.out  [kernel]          [k] \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0xffffffff8a6009e7\u0026lt;/span\u0026gt;  \n     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.01\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.01\u0026lt;/span\u0026gt;%  main.out  [kernel]          [k] \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0xffffffff8a600158\u0026lt;/span\u0026gt;  \n     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.01\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%  main.out  [unknown]         [k] \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0x0000000000000040\u0026lt;/span\u0026gt;  \n     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.01\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%  main.out  ld\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;-2.27\u0026lt;/span\u0026gt;.so        [.] _dl_sysdep_start    \n     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.01\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%  main.out  ld\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;-2.27\u0026lt;/span\u0026gt;.so        [.] dl_main             \n     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.01\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%  main.out  ld\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;-2.27\u0026lt;/span\u0026gt;.so        [.] mprotect            \n     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.01\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%  main.out  ld\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;-2.27\u0026lt;/span\u0026gt;.so        [.] _dl_map_object      \n     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.01\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%  main.out  ld\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;-2.27\u0026lt;/span\u0026gt;.so        [.] _xstat              \n     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%  main.out  ld\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;-2.27\u0026lt;/span\u0026gt;.so        [.] __GI___tunables_init\n     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%  main.out  [unknown]         [.] \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0x2f3d4f4944555453\u0026lt;/span\u0026gt;  \n     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%  main.out  [unknown]         [.] \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0x00007fff3cfc57ac\u0026lt;/span\u0026gt;  \n     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%  main.out  ld\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;-2.27\u0026lt;/span\u0026gt;.so        [.] _start              \n\u0026lt;/span\u0026gt;\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;So then I try to benchmark the \u0026lt;code\u0026gt;-O0\u0026lt;/code\u0026gt; program to see if that shows anything, and only now, at last, do I see a call graph:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;Samples: \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;15\u0026lt;/span\u0026gt;K of event \u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026apos;cycles:uppp\u0026apos;\u0026lt;/span\u0026gt;, \u0026lt;span class=\u0026quot;hljs-function\u0026quot;\u0026gt;Event \u0026lt;span class=\u0026quot;hljs-title\u0026quot;\u0026gt;count\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-params\u0026quot;\u0026gt;(approx.)\u0026lt;/span\u0026gt;: \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;12438962281\u0026lt;/span\u0026gt;   \n  Children      Self  Command   Shared Object     Symbol                  \n+   \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;99.99\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%  main.out  [unknown]         [.] \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0x04be258d4c544155\u0026lt;/span\u0026gt;  \n+   \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;99.99\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%  main.out  libc\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;-2.27\u0026lt;/span\u0026gt;.so      [.] __libc_start_main   \n-   \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;99.99\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%  main.out  main.out          [.] main                \n   - main                                                                 \n      - \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;97.54\u0026lt;/span\u0026gt;% maybe_slow                                                 \n           common                                                         \n      - \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2.45\u0026lt;/span\u0026gt;% fast                                                        \n           common                                                         \n+   \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;99.96\u0026lt;/span\u0026gt;%    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;99.85\u0026lt;/span\u0026gt;%  main.out  main.out          [.] common              \n+   \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;97.54\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.03\u0026lt;/span\u0026gt;%  main.out  main.out          [.] maybe_slow          \n+    \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2.45\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%  main.out  main.out          [.] fast                \n     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.11\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.11\u0026lt;/span\u0026gt;%  main.out  [kernel]          [k] \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0xffffffff8a6009e7\u0026lt;/span\u0026gt;  \n     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%  main.out  [unknown]         [k] \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0x0000000000000040\u0026lt;/span\u0026gt;  \n     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%  main.out  ld\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;-2.27\u0026lt;/span\u0026gt;.so        [.] _dl_sysdep_start    \n     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%  main.out  ld\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;-2.27\u0026lt;/span\u0026gt;.so        [.] dl_main             \n     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%  main.out  ld\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;-2.27\u0026lt;/span\u0026gt;.so        [.] _dl_lookup_symbol_x \n     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%  main.out  [kernel]          [k] \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0xffffffff8a600158\u0026lt;/span\u0026gt;  \n     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%  main.out  ld\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;-2.27\u0026lt;/span\u0026gt;.so        [.] mmap64              \n     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%  main.out  ld\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;-2.27\u0026lt;/span\u0026gt;.so        [.] _dl_map_object      \n     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%  main.out  ld\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;-2.27\u0026lt;/span\u0026gt;.so        [.] __GI___tunables_init\n     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%  main.out  [unknown]         [.] \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0x552e53555f6e653d\u0026lt;/span\u0026gt;  \n     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%  main.out  [unknown]         [.] \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0x00007ffe1cf20fdb\u0026lt;/span\u0026gt;  \n     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.00\u0026lt;/span\u0026gt;%  main.out  ld\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;-2.27\u0026lt;/span\u0026gt;.so        [.] _start              \n\u0026lt;/span\u0026gt;\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;TODO: what happened on the \u0026lt;code\u0026gt;-O3\u0026lt;/code\u0026gt; execution? Is it simply that \u0026lt;code\u0026gt;maybe_slow\u0026lt;/code\u0026gt; and \u0026lt;code\u0026gt;fast\u0026lt;/code\u0026gt; were too fast and did not get any samples? Does it work well with \u0026lt;code\u0026gt;-O3\u0026lt;/code\u0026gt; on larger programs that take longer to execute? Did I miss some CLI option? I found out about \u0026lt;code\u0026gt;-F\u0026lt;/code\u0026gt; to control the sample frequency in Hertz, but I turned it up to the max allowed by default of \u0026lt;code\u0026gt;-F 39500\u0026lt;/code\u0026gt; (could be increased with \u0026lt;code\u0026gt;sudo\u0026lt;/code\u0026gt;) and I still don\u0026apos;t see clear calls.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;One cool thing about \u0026lt;code\u0026gt;perf\u0026lt;/code\u0026gt; is the FlameGraph tool from Brendan Gregg which displays the call stack timings in a very neat way that allows you to quickly see the big calls. The tool is available at: \u0026lt;a href=\u0026quot;https://github.com/brendangregg/FlameGraph\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;https://github.com/brendangregg/FlameGraph\u0026lt;/a\u0026gt; and is also mentioned on his perf tutorial at: \u0026lt;a href=\u0026quot;http://www.brendangregg.com/perf.html#FlameGraphs\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;http://www.brendangregg.com/perf.html#FlameGraphs\u0026lt;/a\u0026gt; When I ran \u0026lt;code\u0026gt;perf\u0026lt;/code\u0026gt; without \u0026lt;code\u0026gt;sudo\u0026lt;/code\u0026gt; I got \u0026lt;a href=\u0026quot;https://github.com/brendangregg/FlameGraph/issues/132\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;\u0026lt;code\u0026gt;ERROR: No stack counts found\u0026lt;/code\u0026gt;\u0026lt;/a\u0026gt; so for now I\u0026apos;ll be doing it with \u0026lt;code\u0026gt;sudo\u0026lt;/code\u0026gt;:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;git clone https:\u0026lt;span class=\u0026quot;hljs-comment\u0026quot;\u0026gt;//github.com/brendangregg/FlameGraph\u0026lt;/span\u0026gt;\nsudo perf record -F \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;99\u0026lt;/span\u0026gt; -g -o perf_with_stack.data ./main.out \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;10000\u0026lt;/span\u0026gt;\nsudo perf script -i perf_with_stack.data | FlameGraph/stackcollapse-perf.pl | FlameGraph/flamegraph.pl \u0026amp;gt; flamegraph.svg\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;but in such a simple program the output is not very easy to understand, since we can\u0026apos;t easily see neither \u0026lt;code\u0026gt;maybe_slow\u0026lt;/code\u0026gt; nor \u0026lt;code\u0026gt;fast\u0026lt;/code\u0026gt; on that graph:\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;https://i.stack.imgur.com/QFKSS.png\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;\u0026lt;img src=\u0026quot;https://i.stack.imgur.com/QFKSS.png\u0026quot; alt=\u0026quot;enter image description here\u0026quot;\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;On the a more complex example it becomes clear what the graph means:\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;https://i.stack.imgur.com/4Ufpd.png\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;\u0026lt;img src=\u0026quot;https://i.stack.imgur.com/4Ufpd.png\u0026quot; alt=\u0026quot;enter image description here\u0026quot;\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;TODO there are a log of \u0026lt;code\u0026gt;[unknown]\u0026lt;/code\u0026gt; functions in that example, why is that?\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Another perf GUI interfaces which might be worth it include:\u0026lt;/p\u0026gt;\n\u0026lt;ul\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Eclipse Trace Compass plugin: \u0026lt;a href=\u0026quot;https://www.eclipse.org/tracecompass/\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;https://www.eclipse.org/tracecompass/\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;But this has the downside that you have to first convert the data to the Common Trace Format, which can be done with \u0026lt;code\u0026gt;perf data --to-ctf\u0026lt;/code\u0026gt;, but it needs to be enabled at build time/have \u0026lt;code\u0026gt;perf\u0026lt;/code\u0026gt; new enough, either of which is not the case for the perf in Ubuntu 18.04\u0026lt;/p\u0026gt;\n\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;https://github.com/KDAB/hotspot\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;https://github.com/KDAB/hotspot\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;The downside of this is that there seems to be no Ubuntu package, and building it requires Qt 5.10 while Ubuntu 18.04 is at Qt 5.9.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;But \u0026lt;a href=\u0026quot;https://stackoverflow.com/users/758288/david-faure\u0026quot;\u0026gt;David Faure\u0026lt;/a\u0026gt; mentions in the comments that there is no an AppImage package which might be a convenient way to use it.\u0026lt;/p\u0026gt;\n\u0026lt;/li\u0026gt;\n\u0026lt;/ul\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;gperftools\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Previously called \u0026quot;Google Performance Tools\u0026quot;, source: \u0026lt;a href=\u0026quot;https://github.com/gperftools/gperftools\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;https://github.com/gperftools/gperftools\u0026lt;/a\u0026gt; Sample based.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;First install gperftools with:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;sudo apt install google-perftools\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;Then, we can enable the gperftools CPU profiler in two ways: at runtime, or at build time.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;At runtime, we have to pass set the \u0026lt;code\u0026gt;LD_PRELOAD\u0026lt;/code\u0026gt; to point to \u0026lt;code\u0026gt;libprofiler.so\u0026lt;/code\u0026gt;, which you can find with \u0026lt;code\u0026gt;locate libprofiler.so\u0026lt;/code\u0026gt;, e.g. on my system:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;gcc -ggdb3 -O3 -std=c99 -Wall -Wextra -pedantic -o main.out main.c\nLD_PRELOAD=/usr/lib/x86_64-linux-gnu/libprofiler.so \\\n  CPUPROFILE=prof.out ./main.out \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;10000\u0026lt;/span\u0026gt;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;Alternatively, we can build the library in at link time, dispensing passing \u0026lt;code\u0026gt;LD_PRELOAD\u0026lt;/code\u0026gt; at runtime:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;gcc -Wl,--no-as-needed,-lprofiler,--as-needed -ggdb3 -O3 -std=c99 -Wall -Wextra -pedantic -o main.out main.c\nCPUPROFILE=prof.out ./main.out \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;10000\u0026lt;/span\u0026gt;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;See also: \u0026lt;a href=\u0026quot;https://stackoverflow.com/questions/46949407/gperftools-profile-file-not-dumped\u0026quot;\u0026gt;gperftools - profile file not dumped\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;The nicest way to view this data I\u0026apos;ve found so far is to make pprof output the same format that kcachegrind takes as input (yes, the Valgrind-project-viewer-tool) and use kcachegrind to view that:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;google-pprof --callgrind main.out prof.out  \u0026amp;gt; callgrind.out\nkcachegrind callgrind.out\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;After running with either of those methods, we get a \u0026lt;code\u0026gt;prof.out\u0026lt;/code\u0026gt; profile data file as output. We can view that file graphically as an SVG with:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;google-pprof --web main.out prof.out\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;https://i.stack.imgur.com/SiISF.png\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;\u0026lt;img src=\u0026quot;https://i.stack.imgur.com/SiISF.png\u0026quot; alt=\u0026quot;enter image description here\u0026quot;\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;which gives as a familiar call graph like other tools, but with the clunky unit of number of samples rather than seconds.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Alternatively, we can also get some textual data with:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;google-pprof --text main.out prof.out\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;which gives:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;Using local file main.out.\nUsing local file prof.out.\nTotal: \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;187\u0026lt;/span\u0026gt; samples\n     \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;187\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;100.0\u0026lt;/span\u0026gt;% \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;100.0\u0026lt;/span\u0026gt;%      \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;187\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;100.0\u0026lt;/span\u0026gt;% common\n       \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0\u0026lt;/span\u0026gt;   \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.0\u0026lt;/span\u0026gt;% \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;100.0\u0026lt;/span\u0026gt;%      \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;187\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;100.0\u0026lt;/span\u0026gt;% __libc_start_main\n       \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0\u0026lt;/span\u0026gt;   \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.0\u0026lt;/span\u0026gt;% \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;100.0\u0026lt;/span\u0026gt;%      \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;187\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;100.0\u0026lt;/span\u0026gt;% _start\n       \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0\u0026lt;/span\u0026gt;   \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.0\u0026lt;/span\u0026gt;% \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;100.0\u0026lt;/span\u0026gt;%        \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;4\u0026lt;/span\u0026gt;   \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2.1\u0026lt;/span\u0026gt;% fast\n       \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0\u0026lt;/span\u0026gt;   \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.0\u0026lt;/span\u0026gt;% \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;100.0\u0026lt;/span\u0026gt;%      \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;187\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;100.0\u0026lt;/span\u0026gt;% main\n       \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0\u0026lt;/span\u0026gt;   \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.0\u0026lt;/span\u0026gt;% \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;100.0\u0026lt;/span\u0026gt;%      \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;183\u0026lt;/span\u0026gt;  \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;97.9\u0026lt;/span\u0026gt;% maybe_slow\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;See also: \u0026lt;a href=\u0026quot;https://stackoverflow.com/questions/10874308/how-to-use-google-perf-tools\u0026quot;\u0026gt;How to use google perf tools\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Instrument your code with raw \u0026lt;code\u0026gt;perf_event_open\u0026lt;/code\u0026gt; syscalls\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;I think this is the same underlying subsystem that \u0026lt;code\u0026gt;perf\u0026lt;/code\u0026gt; uses, but you could of course attain even greater control by explicitly instrumenting your program at compile time with events of interest.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;This is likely too hardcore for most people, but it\u0026apos;s kind of fun. Minimal runnable example at: \u0026lt;a href=\u0026quot;https://stackoverflow.com/questions/13313510/quick-way-to-count-number-of-instructions-executed-in-a-c-program/64863392#64863392\u0026quot;\u0026gt;Quick way to count number of instructions executed in a C program\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Intel VTune\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;https://en.wikipedia.org/wiki/VTune\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;https://en.wikipedia.org/wiki/VTune\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;This seems to be closed source and x86-only, but it is likely to be amazing from what I\u0026apos;ve heard. I\u0026apos;m not sure how free it is to use, but it seems to be free to download. TODO evaluate.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Tested in Ubuntu 18.04, gprof2dot 2019.11.30, valgrind 3.13.0, perf 4.15.18, Linux kernel 4.15.0, FLameGraph 1a0dc6985aad06e76857cf2a354bd5ba0c9ce96b, gperftools 2.5-2.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Use Valgrind, callgrind and kcachegrind:\u0026lt;/strong\u0026gt; \u0026lt;/p\u0026gt;\n\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;valgrind --tool=callgrind ./(Your binary)\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;generates callgrind.out.x. Read it using kcachegrind.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Use gprof (add -pg):\u0026lt;/strong\u0026gt; \u0026lt;/p\u0026gt;\n\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;cc -o myprog myprog.c utils.c -g -pg \n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;(not so good for multi-threads, function pointers)\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Use google-perftools:\u0026lt;/strong\u0026gt; \u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Uses time sampling, I/O and CPU bottlenecks are revealed.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Intel VTune is the best (free for educational purposes).\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Others:\u0026lt;/strong\u0026gt; AMD Codeanalyst (since replaced with AMD CodeXL), OProfile, \u0026apos;perf\u0026apos; tools (apt-get install linux-tools)\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;For single-threaded programs you can use \u0026lt;strong\u0026gt;igprof\u0026lt;/strong\u0026gt;, The Ignominous Profiler: \u0026lt;a href=\u0026quot;https://igprof.org/\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;https://igprof.org/\u0026lt;/a\u0026gt; .\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;It is a sampling profiler, along the lines of the... long... answer by Mike Dunlavey, which will gift wrap the results in a browsable call stack tree, annotated with the time or memory spent in each function, either cumulative or per-function.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Also worth mentioning are\u0026lt;/p\u0026gt;\n\n\u0026lt;ol\u0026gt;\n\u0026lt;li\u0026gt;HPCToolkit (\u0026lt;a href=\u0026quot;http://hpctoolkit.org/\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;http://hpctoolkit.org/\u0026lt;/a\u0026gt;) - Open-source, works for parallel programs and has a GUI with which to look at the results multiple ways\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;Intel VTune (\u0026lt;a href=\u0026quot;https://software.intel.com/en-us/vtune\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;https://software.intel.com/en-us/vtune\u0026lt;/a\u0026gt;) - If you have intel compilers this is very good \u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;TAU (\u0026lt;a href=\u0026quot;http://www.cs.uoregon.edu/research/tau/home.php\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;http://www.cs.uoregon.edu/research/tau/home.php\u0026lt;/a\u0026gt;) \u0026lt;/li\u0026gt;\n\u0026lt;/ol\u0026gt;\n\n\u0026lt;p\u0026gt;I have used HPCToolkit and VTune and they are very effective at finding the long pole in the tent and do not need your code to be recompiled (except that you have to use -g -O or RelWithDebInfo type build in CMake to get meaningful output). I have heard TAU is similar in capabilities.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Actually a bit surprised not many mentioned about \u0026lt;a href=\u0026quot;https://github.com/google/benchmark\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;google/benchmark\u0026lt;/a\u0026gt; , while it is a bit cumbersome to pin the specific area of code, specially if the code base is a little big one, however I found this really helpful when used in combination with \u0026lt;code\u0026gt;callgrind\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;IMHO identifying the piece that is causing bottleneck is the key here. I\u0026apos;d however try and answer the following questions first and choose tool based on that\u0026lt;/p\u0026gt;\n\u0026lt;ol\u0026gt;\n\u0026lt;li\u0026gt;is my algorithm correct ?\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;are there locks that are proving to be bottle necks ?\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;is there a specific section of code that\u0026apos;s proving to be a culprit ?\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;how about IO, handled and optimized ?\u0026lt;/li\u0026gt;\n\u0026lt;/ol\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;code\u0026gt;valgrind\u0026lt;/code\u0026gt; with the combination of \u0026lt;code\u0026gt;callgrind\u0026lt;/code\u0026gt; and \u0026lt;code\u0026gt;kcachegrind\u0026lt;/code\u0026gt; should provide a decent estimation on the points above, and once it\u0026apos;s established that there are issues with some section of code, I\u0026apos;d suggest to do a micro bench mark - \u0026lt;code\u0026gt;google benchmark\u0026lt;/code\u0026gt; is a good place to start.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;These are the two methods I use for speeding up my code:\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;For CPU bound applications:\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;ol\u0026gt;\n\u0026lt;li\u0026gt;Use a profiler in DEBUG mode to identify questionable parts of your code\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;Then switch to RELEASE mode and comment out the questionable sections of your code (stub it with nothing) until you see changes in performance.\u0026lt;/li\u0026gt;\n\u0026lt;/ol\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;For I/O bound applications:\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;ol\u0026gt;\n\u0026lt;li\u0026gt;Use a profiler in RELEASE mode to identify questionable parts of your code.\u0026lt;/li\u0026gt;\n\u0026lt;/ol\u0026gt;\n\n\u0026lt;hr\u0026gt;\n\n\u0026lt;p\u0026gt;N.B.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;If you don\u0026apos;t have a profiler, use the poor man\u0026apos;s profiler. Hit pause while debugging your application. Most developer suites will break into assembly with commented line numbers. You\u0026apos;re statistically likely to land in a region that is eating most of your CPU cycles.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;For CPU, the reason for profiling in \u0026lt;strong\u0026gt;DEBUG\u0026lt;/strong\u0026gt; mode is because if your tried profiling in \u0026lt;strong\u0026gt;RELEASE\u0026lt;/strong\u0026gt; mode, the compiler is going to reduce math, vectorize loops, and inline functions which tends to glob your code into an un-mappable mess when it\u0026apos;s assembled. \u0026lt;strong\u0026gt;An un-mappable mess means your profiler will not be able to clearly identify what is taking so long because the assembly may not correspond to the source code under optimization\u0026lt;/strong\u0026gt;. If you need the performance (e.g. timing sensitive) of \u0026lt;strong\u0026gt;RELEASE\u0026lt;/strong\u0026gt; mode, disable debugger features as needed to keep a usable performance.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;For I/O-bound, the profiler can still identify I/O operations in \u0026lt;strong\u0026gt;RELEASE\u0026lt;/strong\u0026gt; mode because I/O operations are either externally linked to a shared library (most of the time) or in the worst case, will result in a sys-call interrupt vector (which is also easily identifiable by the profiler).\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;You can use the iprof library:\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;https://gitlab.com/Neurochrom/iprof\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;https://gitlab.com/Neurochrom/iprof\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;https://github.com/Neurochrom/iprof\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;https://github.com/Neurochrom/iprof\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;It\u0026apos;s cross-platform and allows you not to measure performance of your application also in real-time. You can even couple it with a live graph.\nFull disclaimer: I am the author.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;You can use a logging framework like \u0026lt;a href=\u0026quot;https://github.com/emilk/loguru\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;\u0026lt;code\u0026gt;loguru\u0026lt;/code\u0026gt;\u0026lt;/a\u0026gt; since it includes timestamps and total uptime which can be used nicely for profiling:\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;https://i.stack.imgur.com/eHqdn.png\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;\u0026lt;img src=\u0026quot;https://i.stack.imgur.com/eHqdn.png\u0026quot; alt=\u0026quot;\u0026quot;\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;At work we have a really nice tool that helps us monitoring what we want in terms of scheduling. This has been useful numerous times.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;It\u0026apos;s in C++ and must be customized to your needs. Unfortunately I can\u0026apos;t share code, just concepts.\nYou use a \u0026quot;large\u0026quot; \u0026lt;code\u0026gt;volatile\u0026lt;/code\u0026gt; buffer containing timestamps and event ID that you can dump post mortem or after stopping the logging system (and dump this into a file for example).\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;You retrieve the so-called large buffer with all the data and a small interface parses it and shows events with name (up/down + value) like an oscilloscope does with colors (configured in \u0026lt;code\u0026gt;.hpp\u0026lt;/code\u0026gt; file).\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;You customize the amount of events generated to focus solely on what you desire. It helped us a lot for scheduling issues while consuming the amount of CPU we wanted based on the amount of logged events per second. \u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;You need 3 files : \u0026lt;/p\u0026gt;\n\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;toolname.hpp \u0026lt;span class=\u0026quot;hljs-comment\u0026quot;\u0026gt;// interface\u0026lt;/span\u0026gt;\ntoolname.cpp \u0026lt;span class=\u0026quot;hljs-comment\u0026quot;\u0026gt;// code\u0026lt;/span\u0026gt;\ntool_events_id.hpp \u0026lt;span class=\u0026quot;hljs-comment\u0026quot;\u0026gt;// Events ID\u0026lt;/span\u0026gt;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;The concept is to define events in \u0026lt;code\u0026gt;tool_events_id.hpp\u0026lt;/code\u0026gt; like that :\u0026lt;/p\u0026gt;\n\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-comment\u0026quot;\u0026gt;// EVENT_NAME                         ID      BEGIN_END BG_COLOR NAME\u0026lt;/span\u0026gt;\n\u0026lt;span class=\u0026quot;hljs-meta\u0026quot;\u0026gt;#\u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;define\u0026lt;/span\u0026gt; SOCK_PDU_RECV_D               0x0301  \u0026lt;span class=\u0026quot;hljs-comment\u0026quot;\u0026gt;//@D00301 BGEEAAAA # TX_PDU_Recv\u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\n\u0026lt;span class=\u0026quot;hljs-meta\u0026quot;\u0026gt;#\u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;define\u0026lt;/span\u0026gt; SOCK_PDU_RECV_F               0x0302  \u0026lt;span class=\u0026quot;hljs-comment\u0026quot;\u0026gt;//@F00301 BGEEAAAA # TX_PDU_Recv\u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;You also define a few functions in \u0026lt;code\u0026gt;toolname.hpp\u0026lt;/code\u0026gt; :\u0026lt;/p\u0026gt;\n\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-meta\u0026quot;\u0026gt;#\u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;define\u0026lt;/span\u0026gt; LOG_LEVEL_ERROR 0\u0026lt;/span\u0026gt;\n\u0026lt;span class=\u0026quot;hljs-meta\u0026quot;\u0026gt;#\u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;define\u0026lt;/span\u0026gt; LOG_LEVEL_WARN 1\u0026lt;/span\u0026gt;\n\u0026lt;span class=\u0026quot;hljs-comment\u0026quot;\u0026gt;// ...\u0026lt;/span\u0026gt;\n\n\u0026lt;span class=\u0026quot;hljs-function\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-type\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-title\u0026quot;\u0026gt;init\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-params\u0026quot;\u0026gt;(\u0026lt;span class=\u0026quot;hljs-type\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt;)\u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;;\n\u0026lt;span class=\u0026quot;hljs-function\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-type\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-title\u0026quot;\u0026gt;probe\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-params\u0026quot;\u0026gt;(id,payload)\u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;;\n\u0026lt;span class=\u0026quot;hljs-comment\u0026quot;\u0026gt;// etc\u0026lt;/span\u0026gt;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;Wherever in you code you can use :\u0026lt;/p\u0026gt;\n\n\u0026lt;pre class=\u0026quot;lang-cpp s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-cpp\u0026quot;\u0026gt;toolname\u0026amp;lt;LOG_LEVEL\u0026amp;gt;::\u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;log\u0026lt;/span\u0026gt;(EVENT_NAME,VALUE);\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;The \u0026lt;code\u0026gt;probe\u0026lt;/code\u0026gt; function uses a few assembly lines to retrieve the clock timestamp ASAP and then sets an entry in the buffer. We also have an atomic increment to safely find an index where to store the log event.\nOf course buffer is circular.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Hope the idea is not obfuscated by the lack of sample code.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;use a debugging software\u0026lt;/strong\u0026gt; \nhow to identify where the code is running slowly ?\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;just think you have a obstacle while you are in motion then it will decrease your speed\u0026lt;/strong\u0026gt;  \u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;like that unwanted reallocation\u0026apos;s looping,buffer overflows,searching,memory leakages etc operations consumes more execution power it will effect adversely over performance of the code,\nBe sure to add -pg to compilation before profiling:\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;code\u0026gt;g++ your_prg.cpp -pg\u0026lt;/code\u0026gt; or \u0026lt;code\u0026gt;cc my_program.cpp -g -pg\u0026lt;/code\u0026gt; as per your compiler\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;haven\u0026apos;t tried it yet but I\u0026apos;ve heard good things about google-perftools. It is definitely worth a try.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;code\u0026gt;valgrind --tool=callgrind ./(Your binary)\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;It will generate a file called gmon.out or callgrind.out.x. You can then use kcachegrind or debugger tool to read this file. It will give you a graphical analysis of things with results like which lines cost how much. \u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;i think so\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;As no one mentioned Arm MAP, I\u0026apos;d add it as personally I have successfully used Map to profile a C++ scientific program. \u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Arm MAP is the profiler for parallel, multithreaded or single threaded C, C++, Fortran and F90 codes.  It provides in-depth analysis and bottleneck pinpointing to the source line.  Unlike most profilers, it\u0026apos;s designed to be able to profile pthreads, OpenMP or MPI for parallel and threaded code.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;MAP is commercial software. \u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Use \u0026lt;code\u0026gt;-pg\u0026lt;/code\u0026gt; flag when compiling and linking the code and run the executable file. While this program is executed, profiling data is collected in the file a.out.\u0026lt;br\u0026gt;\nThere is two different type of profiling\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;1- Flat profiling: \u0026lt;br\u0026gt;\n by running the command \u0026lt;code\u0026gt;gprog --flat-profile a.out\u0026lt;/code\u0026gt; you got the following data\u0026lt;br\u0026gt;\n - what percentage of the overall time was spent for the function,\u0026lt;br\u0026gt;\n - how many seconds were spent in a functionincluding and excluding calls to sub-functions,\u0026lt;br\u0026gt;\n - the number of calls,\u0026lt;br\u0026gt;\n - the average time per call.\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;2- graph profiling\u0026lt;br\u0026gt;\nus the command \u0026lt;code\u0026gt;gprof --graph a.out\u0026lt;/code\u0026gt; to get the following data for each function which includes\u0026lt;br\u0026gt;\n - In each section, one function is marked with an index number.\u0026lt;br\u0026gt;\n - Above function , there is a list of functions that call the function .\u0026lt;br\u0026gt;\n - Below function , there is a list of functions that are called by the function .\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;To get more info you can look in \u0026lt;a href=\u0026quot;https://sourceware.org/binutils/docs-2.32/gprof/\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;https://sourceware.org/binutils/docs-2.32/gprof/\u0026lt;/a\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\n    "],"id":144,"title":"How do I profile C++ code running on Linux?","content":"\n                \n\u0026lt;p\u0026gt;How do I find areas of my code that run slowly in a C++ application running on Linux?\u0026lt;/p\u0026gt;\n    ","slug":"how-do-i-profile-c++-code-running-on-linux-1657384754272","postType":"QUESTION","createdAt":"2022-07-09T16:39:14.000Z","updatedAt":"2022-07-09T16:39:14.000Z","tags":[{"id":524,"name":"profiling","slug":"profiling","createdAt":"2022-07-09T16:39:14.000Z","updatedAt":"2022-07-09T16:39:14.000Z","Questions_Tags":{"questionId":144,"tagId":524}}],"relatedQuestions":[{"title":"How do I profile C++ code running on Linux?","slug":"how-do-i-profile-c++-code-running-on-linux-1657384754272","tags":[{"name":"profiling","Questions_Tags":{"questionId":144,"tagId":524}}]}]},"randomQuestions":[{"title":"Difference between single and double quotes in Bash","slug":"difference-between-single-and-double-quotes-in-bash-1657385460827"},{"title":"Trouble with UTF-8 characters; what I see is not what I stored","slug":"trouble-with-utf-8-characters-what-i-see-is-not-what-i-stored-1657384817490"},{"title":"How do I profile a Python script?","slug":"how-do-i-profile-a-python-script-1657388346692"},{"title":"Are dictionaries ordered in Python 3.6+?","slug":"are-dictionaries-ordered-in-python-3.6+-1657387834234"},{"title":"Sorting object property by values","slug":"sorting-object-property-by-values-1657388367300"},{"title":"How to test multiple variables for equality against a single value?","slug":"how-to-test-multiple-variables-for-equality-against-a-single-value-1657384358504"},{"title":"How do I pass variables and data from PHP to JavaScript?","slug":"how-do-i-pass-variables-and-data-from-php-to-javascript-1657384684553"},{"title":"How to print without a newline or space","slug":"how-to-print-without-a-newline-or-space-1657387814213"},{"title":"How do you use a variable in a regular expression?","slug":"how-do-you-use-a-variable-in-a-regular-expression-1657387939007"},{"title":"Sorting an array of objects by property values","slug":"sorting-an-array-of-objects-by-property-values-1657387447490"},{"title":"Selenium \"selenium.common.exceptions.NoSuchElementException\" when using Chrome","slug":"selenium-\"selenium.common.exceptions.nosuchelementexception\"-when-using-chrome-1657388136699"},{"title":"Scanner is skipping nextLine() after using next() or nextFoo()?","slug":"scanner-is-skipping-nextline()-after-using-next()-or-nextfoo()-1657384379697"},{"title":"Why is \"using namespace std;\" considered bad practice?","slug":"why-is-\"using-namespace-std\"-considered-bad-practice-1657384296377"},{"title":"How do JavaScript closures work?","slug":"how-do-javascript-closures-work-1657384418555"},{"title":"Can I mix MySQL APIs in PHP?","slug":"can-i-mix-mysql-apis-in-php-1657384597444"},{"title":"How do I properly compare strings in C?","slug":"how-do-i-properly-compare-strings-in-c-1657387467202"},{"title":"HTTP GET with request body","slug":"http-get-with-request-body-1657387379038"},{"title":"Split array into chunks","slug":"split-array-into-chunks-1657387896420"},{"title":"Randomize a List\u003cT\u003e","slug":"randomize-a-listlesstgreater-1657388172793"},{"title":"Why does jQuery or a DOM method such as getElementById not find the element?","slug":"why-does-jquery-or-a-dom-method-such-as-getelementbyid-not-find-the-element-1657384326458"}]},"__N_SSG":true},"page":"/questions/[slug]","query":{"slug":"how-do-i-profile-c++-code-running-on-linux-1657384754272"},"buildId":"XDXakEY6gSPdgAODPxtjg","isFallback":false,"gsp":true,"locale":"en","locales":["en"],"defaultLocale":"en","scriptLoader":[]}</script></body></html>