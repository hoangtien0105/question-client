<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/2eccd4d47c856f2b.css" as="style"/><link rel="stylesheet" href="/_next/static/css/2eccd4d47c856f2b.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-0d1b80a048d4787e.js"></script><script src="/_next/static/chunks/webpack-cb7634a8b6194820.js" defer=""></script><script src="/_next/static/chunks/framework-4556c45dd113b893.js" defer=""></script><script src="/_next/static/chunks/main-25e5079ab4bd6ecd.js" defer=""></script><script src="/_next/static/chunks/pages/_app-20edbe0b078add93.js" defer=""></script><script src="/_next/static/chunks/29107295-fbcfe2172188e46f.js" defer=""></script><script src="/_next/static/chunks/613-1e0aa2b2023820bb.js" defer=""></script><script src="/_next/static/chunks/495-bb1d5b202c02d7f2.js" defer=""></script><script src="/_next/static/chunks/471-84c36aa98dd4107c.js" defer=""></script><script src="/_next/static/chunks/81-096e453de8a27667.js" defer=""></script><script src="/_next/static/chunks/pages/questions/%5Bslug%5D-e3d1b60e109d3ba1.js" defer=""></script><script src="/_next/static/4Y8KGE-c2lolBeBmcum_I/_buildManifest.js" defer=""></script><script src="/_next/static/4Y8KGE-c2lolBeBmcum_I/_ssgManifest.js" defer=""></script><style data-styled="" data-styled-version="5.3.5">.hFcsiD code{padding:5px;color:hsl(210deg 8% 15%);background-color:hsl(210deg 8% 90%);border-radius:3px;}/*!sc*/
data-styled.g5[id="sc-6e32c308-0"]{content:"hFcsiD,"}/*!sc*/
.ljvTUN .comment-item .comment-body .d-inline-flex.ai-center{text-align:right;}/*!sc*/
.ljvTUN .comment-item .comment-body .comment-date{float:right;}/*!sc*/
data-styled.g6[id="sc-6e32c308-1"]{content:"ljvTUN,"}/*!sc*/
</style></head><body><div id="__next"><div class="sc-9099c029-0 cIPEih"><header><nav class="bg-white border-gray-200 px-4 lg:px-6 py-2.5 dark:bg-gray-800"><div class="flex flex-wrap justify-between items-center mx-auto max-w-screen-xl"><a class="flex items-center" href="/"><img src="https://flowbite.com/docs/images/logo.svg" class="mr-3 h-6 sm:h-9" alt="Flowbite Logo"/><span class="self-center text-xl font-semibold whitespace-nowrap dark:text-white">Solution Hunter</span></a><div class="flex items-center lg:order-2"><button data-collapse-toggle="mobile-menu-2" type="button" class="inline-flex items-center p-2 ml-1 text-sm text-gray-500 rounded-lg lg:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200 dark:text-gray-400 dark:hover:bg-gray-700 dark:focus:ring-gray-600" aria-controls="mobile-menu-2" aria-expanded="false"><span class="sr-only">Open main menu</span><svg class="w-6 h-6" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 10a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 15a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z" clip-rule="evenodd"></path></svg><svg class="hidden w-6 h-6" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M4.293 4.293a1 1 0 011.414 0L10 8.586l4.293-4.293a1 1 0 111.414 1.414L11.414 10l4.293 4.293a1 1 0 01-1.414 1.414L10 11.414l-4.293 4.293a1 1 0 01-1.414-1.414L8.586 10 4.293 5.707a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></button></div><div class="hidden justify-between items-center w-full lg:flex lg:w-auto lg:order-1" id="mobile-menu-2"><ul class="flex flex-col mt-4 font-medium lg:flex-row lg:space-x-8 lg:mt-0"><li><a class="block py-2 pr-4 pl-3 text-gray-700 border-b border-gray-100 hover:bg-gray-50 lg:hover:bg-transparent lg:border-0 lg:hover:text-blue-700 lg:p-0 dark:text-gray-400 lg:dark:hover:text-white dark:hover:bg-gray-700 dark:hover:text-white lg:dark:hover:bg-transparent dark:border-gray-700" aria-current="page" href="/">Home</a></li><li><a class="block py-2 pr-4 pl-3 text-gray-700 border-b border-gray-100 hover:bg-gray-50 lg:hover:bg-transparent lg:border-0 lg:hover:text-blue-700 lg:p-0 dark:text-gray-400 lg:dark:hover:text-white dark:hover:bg-gray-700 dark:hover:text-white lg:dark:hover:bg-transparent dark:border-gray-700" href="/questions?tab=news">Questions</a></li><li><a class="block py-2 pr-4 pl-3 text-gray-700 border-b border-gray-100 hover:bg-gray-50 lg:hover:bg-transparent lg:border-0 lg:hover:text-blue-700 lg:p-0 dark:text-gray-400 lg:dark:hover:text-white dark:hover:bg-gray-700 dark:hover:text-white lg:dark:hover:bg-transparent dark:border-gray-700" href="/post?tab=news">Post</a></li><li><a class="block py-2 pr-4 pl-3 text-gray-700 border-b border-gray-100 hover:bg-gray-50 lg:hover:bg-transparent lg:border-0 lg:hover:text-blue-700 lg:p-0 dark:text-gray-400 lg:dark:hover:text-white dark:hover:bg-gray-700 dark:hover:text-white lg:dark:hover:bg-transparent dark:border-gray-700" href="/questions/is-floating-point-math-broken-1657384238910#">Coding</a></li></ul></div></div></nav></header><div class="main-content"><div class="sc-c5440139-0 figLul question my-5"><div class="sc-6e32c308-0 hFcsiD flex question-header items-center justify-center"><div class="rounded-xl border p-5 shadow-md w-9/12 bg-white"><div class="flex w-full items-center justify-between border-b pb-3"><div class="flex items-center space-x-3"><div class="text-lg font-bold text-slate-700"><a href="/questions/is-floating-point-math-broken-1657384238910">Is floating point math broken?</a></div></div><div class="flex flex-wrap h-auto justify-end items-center space-x-8"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold" href="/questions/tag/math">math</a><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold" href="/questions/tag/floating-accuracy">floating-accuracy</a><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold" href="/questions/tag/language-agnostic">language-agnostic</a><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold" href="/questions/tag/floating-point">floating-point</a></div></div><div class="question-content mt-5">
                
<p>Consider the following code:
</p>

<pre><code>0.1 + 0.2 == 0.3  -&gt;  false
</code></pre>

<pre class="lang-js s-code-block"><code class="hljs language-javascript"><span class="hljs-number">0.1</span> + <span class="hljs-number">0.2</span>         -&gt;  <span class="hljs-number">0.30000000000000004</span>
</code></pre>

<p>Why do these inaccuracies happen?</p>
    </div></div></div><div class="sc-6e32c308-1 ljvTUN flex text-xs pl-10 flex-col comment items-center justify-center"><h4 class="text-left mt-10 text-lg font-semibold">Comments</h4><div class="comment-item w-9/12 border-t divide-y-8">
            <div class="comment-body js-comment-edit-hide">
                
                <span class="comment-copy">Floating point variables typically have this behaviour. It's caused by how they are stored in hardware. For more info check out the <a href="http://en.wikipedia.org/wiki/Floating-point" rel="nofollow noreferrer">Wikipedia article on floating point numbers</a>.</span>
                
              <div class="d-inline-flex ai-center">
&nbsp;<a href="/users/68507/ben-s" title="67,275 reputation" class="comment-user">Ben S</a>
                </div>
                <span class="comment-date" dir="ltr"><a class="comment-link" href="#comment21420890_588004"><span title="2009-02-25 21:41:51Z, License: CC BY-SA 2.5" class="relativetime-clean">Feb 25, 2009 at 21:41</span></a></span>
            </div>
        </div><span class="divide-x-8 "></span><div class="comment-item w-9/12 border-t divide-y-8">
            <div class="comment-body js-comment-edit-hide">
                
                <span class="comment-copy">JavaScript treats decimals as <a href="http://en.wikipedia.org/wiki/Floating_point" rel="nofollow noreferrer">floating point numbers</a>, which means operations like addition might be subject to rounding error. You might want to take a look at this article: <a href="http://docs.sun.com/source/806-3568/ncg_goldberg.html" rel="nofollow noreferrer">What Every Computer Scientist Should Know About Floating-Point Arithmetic</a></span>
                
              <div class="d-inline-flex ai-center">
&nbsp;<a href="/users/4249/matt-b" title="135,686 reputation" class="comment-user">matt b</a>
                </div>
                <span class="comment-date" dir="ltr"><a class="comment-link" href="#comment21420903_588004"><span title="2009-02-25 21:42:49Z, License: CC BY-SA 2.5" class="relativetime-clean">Feb 25, 2009 at 21:42</span></a></span>
            </div>
        </div><span class="divide-x-8 "></span><div class="comment-item w-9/12 border-t divide-y-8">
            <div class="comment-body js-comment-edit-hide">
                
                <span class="comment-copy">Just for information, ALL numeric types in javascript are IEEE-754 Doubles.</span>
                
              <div class="d-inline-flex ai-center">
&nbsp;<a href="/users/13227/gary-willoughby" title="48,829 reputation" class="comment-user">Gary Willoughby</a>
                </div>
                <span class="comment-date" dir="ltr"><a class="comment-link" href="#comment21420906_588004"><span title="2010-04-11 13:01:33Z, License: CC BY-SA 2.5" class="relativetime-clean">Apr 11, 2010 at 13:01</span></a></span>
            </div>
        </div><span class="divide-x-8 "></span><button class="bg-blue-500 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded">See more</button></div><div class="sc-6e32c308-2 bFXWFO"><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 1</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Binary <a href="https://en.wikipedia.org/wiki/Double-precision_floating-point_format" rel="noreferrer">floating point</a> math is like this. In most programming languages, it is based on the <a href="https://en.wikipedia.org/wiki/IEEE_754#Basic_and_interchange_formats" rel="noreferrer">IEEE 754 standard</a>. The crux of the problem is that numbers are represented in this format as a whole number times a power of two; rational numbers (such as <code>0.1</code>, which is <code>1/10</code>) whose denominator is not a power of two cannot be exactly represented.</p>
<p>For <code>0.1</code> in the standard <code>binary64</code> format, the representation can be written exactly as</p>
<ul>
<li><code>0.1000000000000000055511151231257827021181583404541015625</code> in decimal, or</li>
<li><code>0x1.999999999999ap-4</code> in <a href="http://www.exploringbinary.com/hexadecimal-floating-point-constants/" rel="noreferrer">C99 hexfloat notation</a>.</li>
</ul>
<p>In contrast, the rational number <code>0.1</code>, which is <code>1/10</code>, can be written exactly as</p>
<ul>
<li><code>0.1</code> in decimal, or</li>
<li><code>0x1.99999999999999...p-4</code> in an analogue of C99 hexfloat notation, where the <code>...</code> represents an unending sequence of 9's.</li>
</ul>
<p>The constants <code>0.2</code> and <code>0.3</code> in your program will also be approximations to their true values.  It happens that the closest <code>double</code> to <code>0.2</code> is larger than the rational number <code>0.2</code> but that the closest <code>double</code> to <code>0.3</code> is smaller than the rational number <code>0.3</code>.  The sum of <code>0.1</code> and <code>0.2</code> winds up being larger than the rational number <code>0.3</code> and hence disagreeing with the constant in your code.</p>
<p>A fairly comprehensive treatment of floating-point arithmetic issues is <a href="http://download.oracle.com/docs/cd/E19957-01/806-3568/ncg_goldberg.html" rel="noreferrer"><em>What Every Computer Scientist Should Know About Floating-Point Arithmetic</em></a>. For an easier-to-digest explanation, see <a href="http://floating-point-gui.de" rel="noreferrer">floating-point-gui.de</a>.</p>
<p><strong>Side Note: All positional (base-N) number systems share this problem with precision</strong></p>
<p>Plain old decimal (base 10) numbers have the same issues, which is why numbers like 1/3 end up as 0.333333333...</p>
<p>You've just stumbled on a number (3/10) that happens to be easy to represent with the decimal system, but doesn't fit the binary system. It goes both ways (to some small degree) as well: 1/16 is an ugly number in decimal (0.0625), but in binary it looks as neat as a 10,000th does in decimal (0.0001)** - if we were in the habit of using a base-2 number system in our daily lives, you'd even look at that number and instinctively understand you could arrive there by halving something, halving it again, and again and again.</p>
<p>** Of course, that's not exactly how floating-point numbers are stored in memory (they use a form of scientific notation). However, it does illustrate the point that binary floating-point precision errors tend to crop up because the "real world" numbers we are usually interested in working with are so often powers of ten - but only because we use a decimal number system day-to-day. This is also why we'll say things like 71% instead of "5 out of every 7" (71% is an approximation, since 5/7 can't be represented exactly with any decimal number).</p>
<p>So no: binary floating point numbers are not broken, they just happen to be as imperfect as every other base-N number system :)</p>
<p><strong>Side Side Note: Working with Floats in Programming</strong></p>
<p>In practice, this problem of precision means you need to use rounding functions to round your floating point numbers off to however many decimal places you're interested in before you display them.</p>
<p>You also need to replace equality tests with comparisons that allow some amount of tolerance, which means:</p>
<p>Do <em>not</em> do <code>if (x == y) { ... }</code></p>
<p>Instead do <code>if (abs(x - y) &lt; myToleranceValue) { ... }</code>.</p>
<p>where <code>abs</code> is the absolute value. <code>myToleranceValue</code> needs to be chosen for your particular application - and it will have a lot to do with how much "wiggle room" you are prepared to allow, and what the largest number you are going to be comparing may be (due to loss of precision issues). Beware of "epsilon" style constants in your language of choice. These are <em>not</em> to be used as tolerance values.</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 2</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<h1><strong>A Hardware Designer's Perspective</strong></h1>

<p>I believe I should add a hardware designers perspective to this since I design and build floating point hardware. Knowing the origin of the error may help in understanding what is happening in the software, and ultimately, I hope this helps explain the reasons for why floating point errors happen and seem to accumulate over time.</p>

<h2>1. Overview</h2>

<p>From an engineering perspective, most floating point operations will have some element of error since the hardware that does the floating point computations is only required to have an error of less than one half of one unit in the last place. Therefore, much hardware will stop at a precision that's only necessary to yield an error of less than one half of one unit in the last place for a <em>single operation</em> which is especially problematic in floating point division. What constitutes a single operation depends upon how many operands the unit takes. For most, it is two, but some units take 3 or more operands. Because of this, there is no guarantee that repeated operations will result in a desirable error since the errors add up over time.</p>

<h2>2. Standards</h2>

<p>Most processors follow the <a href="http://en.wikipedia.org/wiki/IEEE_754-2008" rel="noreferrer">IEEE-754</a> standard but some use denormalized, or different standards
. For example, there is a denormalized mode in IEEE-754 which allows representation of very small floating point numbers at the expense of precision. The following, however, will cover the normalized mode of IEEE-754 which is the typical mode of operation.</p>

<p>In the IEEE-754 standard, hardware designers are allowed any value of error/epsilon as long as it's less than one half of one unit in the last place, and the result only has to be less than one half of one unit in the last place for one operation. This explains why when there are repeated operations, the errors add up. For IEEE-754 double precision, this is the 54th bit, since 53 bits are used to represent the numeric part (normalized), also called the mantissa, of the floating point number (e.g. the 5.3 in 5.3e5). The next sections go into more detail on the causes of hardware error on various floating point operations.</p>

<h2>3. Cause of Rounding Error in Division</h2>

<p>The main cause of the error in floating point division is the division algorithms used to calculate the quotient. Most computer systems calculate division using multiplication by an inverse, mainly in <code>Z=X/Y</code>, <code>Z = X * (1/Y)</code>.  A division is computed iteratively i.e. each cycle computes some bits of the quotient until the desired precision is reached, which for IEEE-754 is anything with an error of less than one unit in the last place. The table of reciprocals of Y (1/Y) is known as the quotient selection table (QST) in the slow division, and the size in bits of the quotient selection table is usually the width of the radix, or a number of bits of the quotient computed in each iteration,  plus a few guard bits. For the IEEE-754 standard, double precision (64-bit), it would be the size of the radix of the divider, plus a few guard bits k, where <code>k&gt;=2</code>. So for example, a typical Quotient Selection Table for a divider that computes 2 bits of the quotient at a time (radix 4) would be <code>2+2= 4</code> bits (plus a few optional bits). </p>

<p><strong>3.1 Division Rounding Error: Approximation of Reciprocal</strong></p>

<p>What reciprocals are in the quotient selection table depend on the <a href="http://en.wikipedia.org/wiki/Division_%28digital%29" rel="noreferrer">division method</a>: slow division such as SRT division, or fast division such as Goldschmidt division; each entry is modified according to the division algorithm in an attempt to yield the lowest possible error. In any case, though, all reciprocals are <em>approximations</em> of the actual reciprocal and introduce some element of error. Both slow division and fast division methods calculate the quotient iteratively, i.e. some number of bits of the quotient are calculated each step, then the result is subtracted from the dividend, and the divider repeats the steps until the error is less than one half of one unit in the last place. Slow division methods calculate a fixed number of digits of the quotient in each step and are usually less expensive to build, and fast division methods calculate a variable number of digits per step and are usually more expensive to build. The most important part of the division methods is that most of them rely upon repeated multiplication by an <em>approximation</em> of a reciprocal, so they are prone to error.</p>

<h2>4. Rounding Errors in Other Operations: Truncation</h2>

<p>Another cause of the rounding errors in all operations are the different modes of truncation of the final answer that IEEE-754 allows. There's truncate, round-towards-zero, <a href="http://en.wikipedia.org/wiki/Floating_point#Rounding_modes" rel="noreferrer">round-to-nearest (default),</a> round-down, and round-up. All methods introduce an element of error of less than one unit in the last place for a single operation. Over time and repeated operations, truncation also adds cumulatively to the resultant error. This truncation error is especially problematic in exponentiation, which involves some form of repeated multiplication.</p>

<h2>5. Repeated Operations</h2>

<p>Since the hardware that does the floating point calculations only needs to yield a result with an error of less than one half of one unit in the last place for a single operation, the error will grow over repeated operations if not watched. This is the reason that in computations that require a bounded error, mathematicians use methods such as using the round-to-nearest <a href="http://en.wikipedia.org/wiki/Floating_point#Rounding_modes" rel="noreferrer">even digit in the last place</a> of IEEE-754, because, over time, the errors are more likely to cancel each other out, and <a href="http://en.wikipedia.org/wiki/Interval_arithmetic" rel="noreferrer">Interval Arithmetic</a> combined with variations of the <a href="http://en.wikipedia.org/wiki/IEEE_754-2008#Rounding_rules" rel="noreferrer">IEEE 754 rounding modes</a> to predict rounding errors, and correct them. Because of its low relative error compared to other rounding modes, round to nearest even digit (in the last place), is the default rounding mode of IEEE-754.</p>

<p>Note that the default rounding mode, round-to-nearest <a href="http://en.wikipedia.org/wiki/Floating_point#Rounding_modes" rel="noreferrer">even digit in the last place</a>, guarantees an error of less than one half of one unit in the last place for one operation. Using the truncation, round-up, and round down alone may result in an error that is greater than one half of one unit in the last place, but less than one unit in the last place, so these modes are not recommended unless they are used in Interval Arithmetic. </p>

<h2>6. Summary</h2>

<p>In short, the fundamental reason for the errors in floating point operations is a combination of the truncation in hardware, and the truncation of a reciprocal in the case of division. Since the IEEE-754 standard only requires an error of less than one half of one unit in the last place for a single operation, the floating point errors over repeated operations will add up unless corrected.</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 3</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>It's broken in the exact same way the decimal (base-10) notation you learned in grade school and use every day is broken, just for base-2.</p>
<p>To understand, think about representing 1/3 as a decimal value. It's impossible to do exactly! The world will end before you finish writing the 3's after the decimal point, and so instead we write to some number of places and consider it sufficiently accurate.</p>
<p>In the same way, 1/10 (decimal 0.1) cannot be represented exactly in base 2 (binary) as a "decimal" value; a repeating pattern after the decimal point goes on forever. The value is not exact, and therefore you can't do exact math with it using normal floating point methods. Just like with base 10, there are other values that exhibit this problem as well.</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 4</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p><em>Most answers here address this question in very dry, technical terms. I'd like to address this in terms that normal human beings can understand.</em></p>

<p>Imagine that you are trying to slice up pizzas. You have a robotic pizza cutter that can cut pizza slices <em>exactly</em> in half. It can halve a whole pizza, or it can halve an existing slice, but in any case, the halving is always exact.</p>

<p>That pizza cutter has very fine movements, and if you start with a whole pizza, then halve that, and continue halving the smallest slice each time, you can do the halving <em>53 times</em> before the slice is too small for even its high-precision abilities. At that point, you can no longer halve that very thin slice, but must either include or exclude it as is.</p>

<p>Now, how would you piece all the slices in such a way that would add up to one-tenth (0.1) or one-fifth (0.2) of a pizza? Really think about it, and try working it out. You can even try to use a real pizza, if you have a mythical precision pizza cutter at hand. :-)</p>

<hr>

<p>Most experienced programmers, of course, know the real answer, which is that there is no way to piece together an <em>exact</em> tenth or fifth of the pizza using those slices, no matter how finely you slice them. You can do a pretty good approximation, and if you add up the approximation of 0.1 with the approximation of 0.2, you get a pretty good approximation of 0.3, but it's still just that, an approximation.</p>

<p>For double-precision numbers (which is the precision that allows you to halve your pizza 53 times), the numbers immediately less and greater than 0.1 are 0.09999999999999999167332731531132594682276248931884765625 and 0.1000000000000000055511151231257827021181583404541015625. The latter is quite a bit closer to 0.1 than the former, so a numeric parser will, given an input of 0.1, favour the latter.</p>

<p>(The difference between those two numbers is the "smallest slice" that we must decide to either include, which introduces an upward bias, or exclude, which introduces a downward bias. The technical term for that smallest slice is an <a href="https://en.wikipedia.org/wiki/Unit_in_the_last_place">ulp</a>.)</p>

<p>In the case of 0.2, the numbers are all the same, just scaled up by a factor of 2. Again, we favour the value that's slightly higher than 0.2.</p>

<p>Notice that in both cases, the approximations for 0.1 and 0.2 have a slight upward bias. If we add enough of these biases in, they will push the number further and further away from what we want, and in fact, in the case of 0.1 + 0.2, the bias is high enough that the resulting number is no longer the closest number to 0.3.</p>

<p>In particular, 0.1 + 0.2 is really 0.1000000000000000055511151231257827021181583404541015625 + 0.200000000000000011102230246251565404236316680908203125 = 0.3000000000000000444089209850062616169452667236328125, whereas the number closest to 0.3 is actually 0.299999999999999988897769753748434595763683319091796875.</p>

<hr>

<p>P.S. Some programming languages also provide pizza cutters that can <a href="https://en.wikipedia.org/wiki/Decimal_floating_point">split slices into exact tenths</a>. Although such pizza cutters are uncommon, if you do have access to one, you should use it when it's important to be able to get exactly one-tenth or one-fifth of a slice.</p>

<p><a href="http://qr.ae/mDcAq"><em>(Originally posted on Quora.)</em></a></p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 5</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Floating point rounding errors. 0.1 cannot be represented as accurately in base-2 as in base-10 due to the missing prime factor of 5. Just as 1/3 takes an infinite number of digits to represent in decimal, but is "0.1" in base-3, 0.1 takes an infinite number of digits in base-2 where it does not in base-10. And computers don't have an infinite amount of memory.</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 6</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p><em>My answer is quite long, so I've split it into three sections. Since the question is about floating point mathematics, I've put the emphasis on what the machine actually does. I've also made it specific to double (64 bit) precision, but the argument applies equally to any floating point arithmetic.</em></p>

<p><strong>Preamble</strong></p>

<p>An <a href="http://en.wikipedia.org/wiki/Double-precision_floating-point_format" rel="noreferrer">IEEE 754 double-precision binary floating-point format (binary64)</a> number represents a number of the form</p>

<blockquote>
  <p>value = (-1)^s * (1.m<sub>51</sub>m<sub>50</sub>...m<sub>2</sub>m<sub>1</sub>m<sub>0</sub>)<sub>2</sub> * 2<sup>e-1023</sup></p>
</blockquote>

<p>in 64 bits:</p>

<ul>
<li>The first bit is the <a href="http://en.wikipedia.org/wiki/Sign_bit" rel="noreferrer">sign bit</a>: <code>1</code> if the number is negative, <code>0</code> otherwise<sup>1</sup>.</li>
<li>The next 11 bits are the <a href="http://en.wikipedia.org/wiki/Exponentiation" rel="noreferrer">exponent</a>, which is <a href="http://en.wikipedia.org/wiki/Offset_binary" rel="noreferrer">offset</a> by 1023. In other words, after reading the exponent bits from a double-precision number, 1023 must be subtracted to obtain the power of two.</li>
<li>The remaining 52 bits are the <a href="http://en.wikipedia.org/wiki/Significand" rel="noreferrer">significand</a> (or mantissa). In the mantissa, an 'implied' <code>1.</code> is always<sup>2</sup> omitted since the most significant bit of any binary value is <code>1</code>.</li>
</ul>

<p><sup>1</sup> - IEEE 754 allows for the concept of a <a href="http://en.wikipedia.org/wiki/Signed_zero" rel="noreferrer">signed zero</a> - <code>+0</code> and <code>-0</code> are treated differently: <code>1 / (+0)</code> is positive infinity; <code>1 / (-0)</code> is negative infinity. For zero values, the mantissa and exponent bits are all zero. Note: zero values (+0 and -0) are explicitly not classed as denormal<sup>2</sup>.</p>

<p><sup>2</sup> - This is not the case for <a href="http://en.wikipedia.org/wiki/Denormal_number" rel="noreferrer">denormal numbers</a>, which have an offset exponent of zero (and an implied <code>0.</code>). The range of denormal double precision numbers is d<sub>min</sub>  |x|  d<sub>max</sub>, where d<sub>min</sub> (the smallest representable nonzero number) is 2<sup>-1023 - 51</sup> ( 4.94 * 10<sup>-324</sup>) and d<sub>max</sub> (the largest denormal number, for which the mantissa consists entirely of <code>1</code>s) is 2<sup>-1023 + 1</sup> - 2<sup>-1023 - 51</sup> ( 2.225 * 10<sup>-308</sup>).</p>

<hr>

<p><strong>Turning a double precision number to binary</strong></p>

<p>Many online converters exist to convert a double precision floating point number to binary (e.g. at <a href="http://www.binaryconvert.com/convert_double.html" rel="noreferrer">binaryconvert.com</a>), but here is some sample C# code to obtain the IEEE 754 representation for a double precision number (I separate the three parts with colons (<code>:</code>):</p>

<pre><code>public static string BinaryRepresentation(double value)
{
    long valueInLongType = BitConverter.DoubleToInt64Bits(value);
    string bits = Convert.ToString(valueInLongType, 2);
    string leadingZeros = new string('0', 64 - bits.Length);
    string binaryRepresentation = leadingZeros + bits;

    string sign = binaryRepresentation[0].ToString();
    string exponent = binaryRepresentation.Substring(1, 11);
    string mantissa = binaryRepresentation.Substring(12);

    return string.Format("{0}:{1}:{2}", sign, exponent, mantissa);
}
</code></pre>

<hr>

<p><strong>Getting to the point: the original question</strong></p>

<p>(Skip to the bottom for the TL;DR version)</p>

<p><a href="https://stackoverflow.com/users/62118/cato-johnston">Cato Johnston</a> (the question asker) asked why 0.1 + 0.2 != 0.3.</p>

<p>Written in binary (with colons separating the three parts), the IEEE 754 representations of the values are:</p>

<pre><code>0.1 =&gt; 0:01111111011:1001100110011001100110011001100110011001100110011010
0.2 =&gt; 0:01111111100:1001100110011001100110011001100110011001100110011010
</code></pre>

<p>Note that the mantissa is composed of recurring digits of <code>0011</code>. This is <strong>key</strong> to why there is any error to the calculations - 0.1, 0.2 and 0.3 cannot be represented in binary <strong>precisely</strong> in a <em>finite</em> number of binary bits any more than 1/9, 1/3 or 1/7 can be represented precisely in <em>decimal digits</em>.</p>

<p>Also note that we can decrease the power in the exponent by 52 and shift the point in the binary representation to the right by 52 places (much like 10<sup>-3</sup> * 1.23 == 10<sup>-5</sup> * 123). This then enables us to represent the binary representation as the exact value that it represents in the form a * 2<sup>p</sup>. where 'a' is an integer.</p>

<p>Converting the exponents to decimal, removing the offset, and re-adding the implied <code>1</code> (in square brackets), 0.1 and 0.2 are:</p>

<pre><code>0.1 =&gt; 2^-4 * [1].1001100110011001100110011001100110011001100110011010
0.2 =&gt; 2^-3 * [1].1001100110011001100110011001100110011001100110011010
or
0.1 =&gt; 2^-56 * 7205759403792794 = 0.1000000000000000055511151231257827021181583404541015625
0.2 =&gt; 2^-55 * 7205759403792794 = 0.200000000000000011102230246251565404236316680908203125
</code></pre>

<p>To add two numbers, the exponent needs to be the same, i.e.:</p>

<pre><code>0.1 =&gt; 2^-3 *  0.1100110011001100110011001100110011001100110011001101(0)
0.2 =&gt; 2^-3 *  1.1001100110011001100110011001100110011001100110011010
sum =  2^-3 * 10.0110011001100110011001100110011001100110011001100111
or
0.1 =&gt; 2^-55 * 3602879701896397  = 0.1000000000000000055511151231257827021181583404541015625
0.2 =&gt; 2^-55 * 7205759403792794  = 0.200000000000000011102230246251565404236316680908203125
sum =  2^-55 * 10808639105689191 = 0.3000000000000000166533453693773481063544750213623046875
</code></pre>

<p>Since the sum is not of the form 2<sup>n</sup> * 1.{bbb} we increase the exponent by one and shift the decimal (<em>binary</em>) point to get:</p>

<pre><code>sum = 2^-2  * 1.0011001100110011001100110011001100110011001100110011(1)
    = 2^-54 * 5404319552844595.5 = 0.3000000000000000166533453693773481063544750213623046875
</code></pre>

<p>There are now 53 bits in the mantissa (the 53rd is in square brackets in the line above). The default <a href="https://en.wikipedia.org/wiki/IEEE_754-1985#Rounding_floating-point_numbers" rel="noreferrer">rounding mode</a> for IEEE 754 is '<em>Round to Nearest</em>' - i.e. if a number <em>x</em> falls between two values <em>a</em> and <em>b</em>, the value where the least significant bit is zero is chosen.</p>

<pre><code>a = 2^-54 * 5404319552844595 = 0.299999999999999988897769753748434595763683319091796875
  = 2^-2  * 1.0011001100110011001100110011001100110011001100110011

x = 2^-2  * 1.0011001100110011001100110011001100110011001100110011(1)

b = 2^-2  * 1.0011001100110011001100110011001100110011001100110100
  = 2^-54 * 5404319552844596 = 0.3000000000000000444089209850062616169452667236328125
</code></pre>

<p>Note that <em>a</em> and <em>b</em> differ only in the last bit; <code>...0011</code> + <code>1</code> = <code>...0100</code>. In this case, the value with the least significant bit of zero is <em>b</em>, so the sum is:</p>

<pre><code>sum = 2^-2  * 1.0011001100110011001100110011001100110011001100110100
    = 2^-54 * 5404319552844596 = 0.3000000000000000444089209850062616169452667236328125
</code></pre>

<p>whereas the binary representation of 0.3 is:</p>

<pre><code>0.3 =&gt; 2^-2  * 1.0011001100110011001100110011001100110011001100110011
    =  2^-54 * 5404319552844595 = 0.299999999999999988897769753748434595763683319091796875
</code></pre>

<p>which only differs from the binary representation of the sum of 0.1 and 0.2 by 2<sup>-54</sup>.</p>

<p>The binary representation of 0.1 and 0.2 are the <em>most accurate</em> representations of the numbers allowable by IEEE 754. The addition of these representation, due to the default rounding mode, results in a value which differs only in the least-significant-bit.</p>

<p><strong>TL;DR</strong></p>

<p>Writing <code>0.1 + 0.2</code> in a IEEE 754 binary representation (with colons separating the three parts) and comparing it to <code>0.3</code>, this is (I've put the distinct bits in square brackets):</p>

<pre><code>0.1 + 0.2 =&gt; 0:01111111101:0011001100110011001100110011001100110011001100110[100]
0.3       =&gt; 0:01111111101:0011001100110011001100110011001100110011001100110[011]
</code></pre>

<p>Converted back to decimal, these values are:</p>

<pre><code>0.1 + 0.2 =&gt; 0.300000000000000044408920985006...
0.3       =&gt; 0.299999999999999988897769753748...
</code></pre>

<p>The difference is exactly 2<sup>-54</sup>, which is ~5.5511151231258 × 10<sup>-17</sup> - insignificant (for many applications) when compared to the original values.</p>

<p>Comparing the last few bits of a floating point number is inherently dangerous, as anyone who reads the famous "<a href="http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html" rel="noreferrer">What Every Computer Scientist Should Know About Floating-Point Arithmetic</a>" (which covers all the major parts of this answer) will know.</p>

<p>Most calculators use additional <a href="https://en.wikipedia.org/wiki/Guard_digit" rel="noreferrer">guard digits</a> to get around this problem, which is how <code>0.1 + 0.2</code> would give <code>0.3</code>: the final few bits are rounded.</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 7</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>In addition to the other correct answers, you may want to consider scaling your values to avoid problems with floating-point arithmetic. </p>

<p>For example: </p>

<pre><code>var result = 1.0 + 2.0;     // result === 3.0 returns true
</code></pre>

<p>... instead of:</p>

<pre><code>var result = 0.1 + 0.2;     // result === 0.3 returns false
</code></pre>

<p>The expression <code>0.1 + 0.2 === 0.3</code> returns <code>false</code> in JavaScript, but fortunately integer arithmetic in floating-point is exact, so decimal representation errors can be avoided by scaling.</p>

<p>As a practical example, to avoid floating-point problems where accuracy is paramount, it is recommended<sup>1</sup> to handle money as an integer representing the number of cents: <code>2550</code> cents instead of <code>25.50</code> dollars. </p>

<hr>

<p><sup>1</sup> Douglas Crockford: <a href="http://books.google.com/books?id=PXa2bby0oQ0C&amp;pg=PA105&amp;dq=douglas+crockford+javascript+good+parts+dollar+values+can+be+converted&amp;rview=1&amp;cd=1#v=onepage&amp;q=&amp;f=false" rel="noreferrer"><strong>JavaScript: The Good Parts</strong>: Appendix A - Awful Parts (page 105)</a>.</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 8</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Floating point numbers stored in the computer consist of two parts, an integer and an exponent that the base is taken to and multiplied by the integer part.</p>

<p>If the computer were working in base 10, <code>0.1</code> would be <code>1 x 10¹</code>, <code>0.2</code> would be <code>2 x 10¹</code>, and <code>0.3</code> would be <code>3 x 10¹</code>. Integer math is easy and exact, so adding <code>0.1 + 0.2</code> will obviously result in <code>0.3</code>.</p>

<p>Computers don't usually work in base 10, they work in base 2. You can still get exact results for some values, for example <code>0.5</code> is <code>1 x 2¹</code> and <code>0.25</code> is <code>1 x 2²</code>, and adding them results in <code>3 x 2²</code>, or <code>0.75</code>. Exactly.</p>

<p>The problem comes with numbers that can be represented exactly in base 10, but not in base 2. Those numbers need to be rounded to their closest equivalent. Assuming the very common IEEE 64-bit floating point format, the closest number to <code>0.1</code> is <code>3602879701896397 x 2</code>, and the closest number to <code>0.2</code> is <code>7205759403792794 x 2</code>; adding them together results in <code>10808639105689191 x 2</code>, or an exact decimal value of <code>0.3000000000000000444089209850062616169452667236328125</code>. Floating point numbers are generally rounded for display.</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 9</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Floating point rounding error.  From <a href="https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html" rel="noreferrer">What Every Computer Scientist Should Know About Floating-Point Arithmetic</a>:</p>

<blockquote>
  <p>Squeezing infinitely many real numbers into a finite number of bits requires an approximate representation. Although there are infinitely many integers, in most programs the result of integer computations can be stored in 32 bits. In contrast, given any fixed number of bits, most calculations with real numbers will produce quantities that cannot be exactly represented using that many bits. Therefore the result of a floating-point calculation must often be rounded in order to fit back into its finite representation. This rounding error is the characteristic feature of floating-point computation.</p>
</blockquote>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 10</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p><a href="https://stackoverflow.com/questions/1089018/why-cant-decimal-numbers-be-represented-exactly-in-binary">In short</a> it's because:</p>
<blockquote>
<p>Floating point numbers cannot represent all decimals precisely in binary</p>
</blockquote>
<p>So just like 10/3 which <a href="https://www.exploringbinary.com/why-0-point-1-does-not-exist-in-floating-point/" rel="noreferrer">does not exist</a> in base 10 precisely (it will be 3.33... recurring), in the same way 1/10 doesn't exist in binary.</p>
<p><strong>So what? How to deal with it?</strong> Is there any workaround?</p>
<p>In order to offer The <strong>best solution</strong> I can say I discovered following method:</p>
<pre><code>parseFloat((0.1 + 0.2).toFixed(10)) =&gt; Will return 0.3
</code></pre>
<p>Let me explain why it's the best solution.
As others mentioned in above answers it's a good idea to use ready to use Javascript toFixed() function to solve the problem. But most likely you'll encounter with some problems.</p>
<p>Imagine you are going to add up two float numbers like <code>0.2</code> and <code>0.7</code> here it is: <code>0.2 + 0.7 = 0.8999999999999999</code>.</p>
<p>Your expected result was <code>0.9</code> it means you need a result with 1 digit precision in this case.
So you should have used <code>(0.2 + 0.7).tofixed(1)</code>
but you can't just give a certain parameter to toFixed() since it depends on the given number, for instance</p>
<pre><code>0.22 + 0.7 = 0.9199999999999999
</code></pre>
<p>In this example you need 2 digits precision so it should be <code>toFixed(2)</code>, so what should be the paramter to fit every given float number?</p>
<p>You might say let it be 10 in every situation then:</p>
<pre><code>(0.2 + 0.7).toFixed(10) =&gt; Result will be "0.9000000000"
</code></pre>
<p>Damn! What are you going to do with those unwanted zeros after 9?
It's the time to convert it to float to make it as you desire:</p>
<pre><code>parseFloat((0.2 + 0.7).toFixed(10)) =&gt; Result will be 0.9
</code></pre>
<p>Now that you found the solution, it's better to offer it as a function like this:</p>
<pre><code>function floatify(number){
           return parseFloat((number).toFixed(10));
        }
    
</code></pre>
<p>Let's try it yourself:
</p><div class="snippet" data-lang="js" data-hide="false" data-console="true" data-babel="false">
<div class="snippet-code">
<pre class="snippet-code-js lang-js s-code-block"><code class="hljs language-javascript"><span class="hljs-keyword">function</span> <span class="hljs-title function_">floatify</span>(<span class="hljs-params">number</span>){
       <span class="hljs-keyword">return</span> <span class="hljs-built_in">parseFloat</span>((number).<span class="hljs-title function_">toFixed</span>(<span class="hljs-number">10</span>));
    }
 
<span class="hljs-keyword">function</span> <span class="hljs-title function_">addUp</span>(<span class="hljs-params"></span>){
  <span class="hljs-keyword">var</span> number1 = +$(<span class="hljs-string">"#number1"</span>).<span class="hljs-title function_">val</span>();
  <span class="hljs-keyword">var</span> number2 = +$(<span class="hljs-string">"#number2"</span>).<span class="hljs-title function_">val</span>();
  <span class="hljs-keyword">var</span> unexpectedResult = number1 + number2;
  <span class="hljs-keyword">var</span> expectedResult = <span class="hljs-title function_">floatify</span>(number1 + number2);
  $(<span class="hljs-string">"#unexpectedResult"</span>).<span class="hljs-title function_">text</span>(unexpectedResult);
  $(<span class="hljs-string">"#expectedResult"</span>).<span class="hljs-title function_">text</span>(expectedResult);
}
<span class="hljs-title function_">addUp</span>();</code></pre>
<pre class="snippet-code-css lang-css s-code-block"><code class="hljs language-css"><span class="hljs-selector-tag">input</span>{
  <span class="hljs-attribute">width</span>: <span class="hljs-number">50px</span>;
}
<span class="hljs-selector-id">#expectedResult</span>{
<span class="hljs-attribute">color</span>: green;
}
<span class="hljs-selector-id">#unexpectedResult</span>{
<span class="hljs-attribute">color</span>: red;
}</code></pre>
<pre class="snippet-code-html lang-html s-code-block"><code class="hljs language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">script</span> <span class="hljs-attr">src</span>=<span class="hljs-string">"https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">script</span>&gt;</span>
<span class="hljs-tag">&lt;<span class="hljs-name">input</span> <span class="hljs-attr">id</span>=<span class="hljs-string">"number1"</span> <span class="hljs-attr">value</span>=<span class="hljs-string">"0.2"</span> <span class="hljs-attr">onclick</span>=<span class="hljs-string">"addUp()"</span> <span class="hljs-attr">onkeyup</span>=<span class="hljs-string">"addUp()"</span>/&gt;</span> +
<span class="hljs-tag">&lt;<span class="hljs-name">input</span> <span class="hljs-attr">id</span>=<span class="hljs-string">"number2"</span> <span class="hljs-attr">value</span>=<span class="hljs-string">"0.7"</span> <span class="hljs-attr">onclick</span>=<span class="hljs-string">"addUp()"</span> <span class="hljs-attr">onkeyup</span>=<span class="hljs-string">"addUp()"</span>/&gt;</span> =
<span class="hljs-tag">&lt;<span class="hljs-name">p</span>&gt;</span>Expected Result: <span class="hljs-tag">&lt;<span class="hljs-name">span</span> <span class="hljs-attr">id</span>=<span class="hljs-string">"expectedResult"</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">span</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">p</span>&gt;</span>
<span class="hljs-tag">&lt;<span class="hljs-name">p</span>&gt;</span>Unexpected Result: <span class="hljs-tag">&lt;<span class="hljs-name">span</span> <span class="hljs-attr">id</span>=<span class="hljs-string">"unexpectedResult"</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">span</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">p</span>&gt;</span></code></pre>
<div class="snippet-result"><div class="snippet-ctas"><button type="button" class="s-btn s-btn__primary"><span class="icon-play-white _hover"></span><span> Run code snippet</span></button><input class="copySnippet s-btn s-btn__filled" type="button" value="Copy snippet to answer" style="display: none;"><button type="button" class="s-btn hideResults" style="display: none;">Hide results</button><div class="popout-code"><a class="snippet-expand-link">Expand snippet</a></div></div><div class="snippet-result-code" style="display: none;"><iframe name="sif1" sandbox="allow-forms allow-modals allow-scripts" class="snippet-box-edit snippet-box-result" frameborder="0"></iframe></div></div></div>
</div>
<p></p>
<p>You can use it this way:</p>
<pre><code>var x = 0.2 + 0.7;
floatify(x);  =&gt; Result: 0.9
</code></pre>
<p>As <a href="https://www.w3schools.com/js/js_numbers.asp" rel="noreferrer">W3SCHOOLS</a> suggests there is another solution too, you can multiply and divide to solve the problem above:</p>
<pre><code>var x = (0.2 * 10 + 0.1 * 10) / 10;       // x will be 0.3
</code></pre>
<p>Keep in mind that <code>(0.2 + 0.1) * 10 / 10</code> won't work at all although it seems the same!
I prefer the first solution since I can apply it as a function which converts the input float to accurate output float.</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 11</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>My workaround:</p>

<pre><code>function add(a, b, precision) {
    var x = Math.pow(10, precision || 2);
    return (Math.round(a * x) + Math.round(b * x)) / x;
}
</code></pre>

<p><em>precision</em> refers to the number of digits you want to preserve after the decimal point during addition.</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 12</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<h3>No, not broken, but most decimal fractions must be approximated</h3>

<blockquote>
  <p><em>Summary</em></p>
</blockquote>

<p>Floating point arithmetic <em>is</em> exact, unfortunately, it doesn't match up well with our usual base-10 number representation, so it turns out we are often giving it input that is slightly off from what we wrote.</p>

<p>Even simple numbers like 0.01, 0.02, 0.03, 0.04 ... 0.24 are not representable exactly as binary fractions. If you count up 0.01, .02, .03 ..., not until you get to 0.25 will you get the first fraction representable in base<sub>2</sub>.  If you tried that using FP, your 0.01 would have been slightly off, so the only way to add 25 of them up to a nice exact 0.25 would have required a long chain of causality involving guard bits and rounding. It's hard to predict so we throw up our hands and say <em>"FP is inexact",</em> but that's not really true. </p>

<p>We constantly give the FP hardware something that seems simple in base 10 but is a repeating fraction in base 2.</p>

<blockquote>
  <p><em>How did this happen?</em></p>
</blockquote>

<p>When we write in decimal, every fraction (specifically, every <em>terminating decimal)</em> is a rational number of the form </p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<em>a / (2<sup>n</sup> x 5<sup>m</sup>)</em></p>

<p>In binary, we only get the <em>2<sup>n</sup></em> term, that is:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <em>a / 2<sup>n</sup></em></p>

<p>So in decimal, we can't represent <sup>1</sup>/<sub>3</sub>. Because base 10 includes 2 as a prime factor, every number we can write as a binary fraction <em>also</em> can be written as a base 10 fraction. However, hardly anything we write as a base<sub>10</sub> fraction is representable in binary. In the range from 0.01, 0.02, 0.03 ... 0.99, only <em>three</em> numbers can be represented in our FP format: 0.25, 0.50, and 0.75, because they are 1/4, 1/2, and 3/4, all numbers with a prime factor using only the 2<sup>n</sup> term.</p>

<p>In base<sub>10</sub> we can't represent <sup>1</sup>/<sub>3</sub>. But in binary, we can't do <sup>1</sup>/<sub>10</sub> <em>or</em> <sup>1</sup>/<sub>3</sub>.</p>

<p>So while every binary fraction can be written in decimal, the reverse is not true. And in fact most decimal fractions repeat in binary.</p>

<blockquote>
  <p><em>Dealing with it</em></p>
</blockquote>

<p>Developers are usually instructed to do <em>&lt; epsilon</em> comparisons, better advice might be to round to integral values (in the C library: round() and roundf(), i.e., stay in the FP format) and then compare. Rounding to a specific decimal fraction length solves most problems with output.</p>

<p>Also, on real number-crunching problems (the problems that FP was invented for on early, frightfully expensive computers) the physical constants of the universe and all other measurements are only known to a relatively small number of significant figures, so the entire problem space was "inexact" anyway. FP "accuracy" isn't a problem in this kind of application.</p>

<p>The whole issue really arises when people try to use FP for bean counting. It does work for that, but only if you stick to integral values, which kind of defeats the point of using it. <em>This is why we have all those decimal fraction software libraries.</em></p>

<p>I love the Pizza answer by <a href="https://stackoverflow.com/users/13/chris-jester-young">Chris</a>, because it describes the actual problem, not just the usual handwaving about "inaccuracy". If FP were simply "inaccurate", we could <em>fix</em> that and would have done it decades ago. The reason we haven't is because the FP format is compact and fast and it's the best way to crunch a lot of numbers. Also, it's a legacy from the space age and arms race and early attempts to solve big problems with very slow computers using small memory systems. (Sometimes, individual <em>magnetic cores</em> for 1-bit storage, but that's <a href="https://en.wikipedia.org/wiki/Magnetic-core_memory" rel="noreferrer">another story.</a>)</p>

<blockquote>
  <p>Conclusion</p>
</blockquote>

<p>If you are just counting beans at a bank, software solutions that use decimal string representations in the first place work perfectly well. But you can't do quantum chromodynamics or aerodynamics that way.</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 13</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>A lot of good answers have been posted, but I'd like to append one more.</p>

<p>Not all numbers can be represented via <strong>floats</strong>/<strong>doubles</strong>
For example, the number "0.2" will be represented as "0.200000003" in single precision in IEEE754 float point standard.</p>

<p>Model for store real numbers under the hood represent float numbers as</p>

<p><a href="https://i.stack.imgur.com/BRvSp.png" rel="noreferrer"><img src="https://i.stack.imgur.com/BRvSp.png" alt="enter image description here"></a></p>

<p>Even though you can type <code>0.2</code> easily, <code>FLT_RADIX</code> and <code>DBL_RADIX</code> is 2; not 10 for a computer with FPU which uses "IEEE Standard for Binary Floating-Point Arithmetic (ISO/IEEE Std 754-1985)".</p>

<p>So it is a bit hard to represent such numbers exactly. Even if you specify this variable explicitly without any intermediate calculation.</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 14</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Some statistics related to this famous double precision question.</p>

<p>When adding all values (<em>a + b</em>) using a step of 0.1 (from 0.1 to 100) we have <strong>~15% chance of precision error</strong>. Note that the error could result in slightly bigger or smaller values.
Here are some examples:</p>

<pre><code>0.1 + 0.2 = 0.30000000000000004 (BIGGER)
0.1 + 0.7 = 0.7999999999999999 (SMALLER)
...
1.7 + 1.9 = 3.5999999999999996 (SMALLER)
1.7 + 2.2 = 3.9000000000000004 (BIGGER)
...
3.2 + 3.6 = 6.800000000000001 (BIGGER)
3.2 + 4.4 = 7.6000000000000005 (BIGGER)
</code></pre>

<p>When subtracting all values (<em>a - b</em> where <em>a &gt; b</em>) using a step of 0.1 (from 100 to 0.1) we have <strong>~34% chance of precision error</strong>.
Here are some examples:</p>

<pre><code>0.6 - 0.2 = 0.39999999999999997 (SMALLER)
0.5 - 0.4 = 0.09999999999999998 (SMALLER)
...
2.1 - 0.2 = 1.9000000000000001 (BIGGER)
2.0 - 1.9 = 0.10000000000000009 (BIGGER)
...
100 - 99.9 = 0.09999999999999432 (SMALLER)
100 - 99.8 = 0.20000000000000284 (BIGGER)
</code></pre>

<p>*15% and 34% are indeed huge, so always use BigDecimal when precision is of big importance. With 2 decimal digits (step 0.01) the situation worsens a bit more (18% and 36%).</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 15</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Given that nobody has mentioned this...</p>

<p>Some high level languages such as Python and Java come with tools to overcome binary floating point limitations. For example:</p>

<ul>
<li><p>Python's <a href="https://docs.python.org/dev/library/decimal.html" rel="noreferrer"><code>decimal</code> module</a> and Java's <a href="http://docs.oracle.com/javase/8/docs/api/java/math/BigDecimal.html" rel="noreferrer"><code>BigDecimal</code> class</a>, that represent numbers internally with decimal notation (as opposed to binary notation). Both have limited precision, so they are still error prone, however they solve most common problems with binary floating point arithmetic.</p>

<p>Decimals are very nice when dealing with money: ten cents plus twenty cents are always exactly thirty cents:</p>

<pre><code>&gt;&gt;&gt; 0.1 + 0.2 == 0.3
False
&gt;&gt;&gt; Decimal('0.1') + Decimal('0.2') == Decimal('0.3')
True
</code></pre>

<p>Python's <code>decimal</code> module is based on <a href="https://en.wikipedia.org/wiki/IEEE_854-1987" rel="noreferrer">IEEE standard 854-1987</a>.</p></li>
<li><p>Python's <a href="https://docs.python.org/dev/library/fractions.html" rel="noreferrer"><code>fractions</code> module</a> and Apache Common's <a href="https://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math3/fraction/BigFraction.html" rel="noreferrer"><code>BigFraction</code> class</a>. Both represent rational numbers as <code>(numerator, denominator)</code> pairs and they may give more accurate results than decimal floating point arithmetic.</p></li>
</ul>

<p>Neither of these solutions is perfect (especially if we look at performances, or if we require a very high precision), but still they solve a great number of problems with binary floating point arithmetic.</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 16</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Did you try the duct tape solution?</p>

<p>Try to determine when errors occur and fix them with short if statements, it's not pretty but for some problems it is the only solution and this is one of them.</p>

<pre><code> if( (n * 0.1) &lt; 100.0 ) { return n * 0.1 - 0.000000000000001 ;}
                    else { return n * 0.1 + 0.000000000000001 ;}    
</code></pre>

<p>I had the same problem in a scientific simulation project in c#, and I can tell you that if you ignore the butterfly effect it's gonna turn to a big fat dragon and bite you in the a**</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 17</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Those weird numbers appear because computers use binary(base 2) number system for calculation purposes, while we use decimal(base 10).</p>

<p>There are a majority of fractional numbers that cannot be represented precisely either in binary or in decimal or both. Result - A rounded up (but precise) number results.</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 18</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Many of this question's numerous duplicates ask about the effects of floating point rounding on specific numbers. In practice, it is easier to get a feeling for how it works by looking at exact results of calculations of interest rather than by just reading about it. Some languages provide ways of doing that - such as converting a <code>float</code> or <code>double</code> to <code>BigDecimal</code> in Java.</p>

<p>Since this is a language-agnostic question, it needs language-agnostic tools, such as a <a href="http://www.exploringbinary.com/floating-point-converter/" rel="noreferrer">Decimal to Floating-Point Converter</a>.</p>

<p>Applying it to the numbers in the question, treated as doubles: </p>

<p>0.1 converts to 0.1000000000000000055511151231257827021181583404541015625, </p>

<p>0.2 converts to 0.200000000000000011102230246251565404236316680908203125, </p>

<p>0.3 converts to 0.299999999999999988897769753748434595763683319091796875, and </p>

<p>0.30000000000000004 converts to 0.3000000000000000444089209850062616169452667236328125.</p>

<p>Adding the first two numbers manually or in a decimal calculator such as <a href="https://www.mathsisfun.com/calculator-precision.html" rel="noreferrer">Full Precision Calculator</a>, shows the exact sum of the actual inputs is 0.3000000000000000166533453693773481063544750213623046875. </p>

<p>If it were rounded down to the equivalent of 0.3 the rounding error would be 0.0000000000000000277555756156289135105907917022705078125. Rounding up to the equivalent of 0.30000000000000004 also gives rounding error 0.0000000000000000277555756156289135105907917022705078125. The round-to-even tie breaker applies.</p>

<p>Returning to the floating point converter, the raw hexadecimal for 0.30000000000000004 is 3fd3333333333334, which ends in an even digit and therefore is the correct result.</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 19</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Can I just add; people always assume this to be a computer problem, but if you count with your hands (base 10), you can't get <code>(1/3+1/3=2/3)=true</code> unless you have infinity to add 0.333... to 0.333... so just as with the <code>(1/10+2/10)!==3/10</code> problem in base 2, you truncate it to 0.333 + 0.333 = 0.666 and probably round it to 0.667 which would be also be technically inaccurate.</p>

<p>Count in ternary, and thirds are not a problem though - maybe some race with 15 fingers on each hand would ask why your decimal math was broken...</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 20</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>The kind of floating-point math that can be implemented in a digital computer necessarily uses an approximation of the real numbers and operations on them. (The <em>standard</em> version runs to over fifty pages of documentation and has a committee to deal with its errata and further refinement.)</p>

<p>This approximation is a mixture of approximations of different kinds, each of which can either be ignored or carefully accounted for due to its specific manner of deviation from exactitude. It also involves a number of explicit exceptional cases at both the hardware and software levels that most people walk right past while pretending not to notice.</p>

<p>If you need infinite precision (using the number π, for example, instead of one of its many shorter stand-ins), you should write or use a symbolic math program instead.</p>

<p>But if you're okay with the idea that sometimes floating-point math is fuzzy in value and logic and errors can accumulate quickly, and you can write your requirements and tests to allow for that, then your code can frequently get by with what's in your FPU.</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 21</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Just for fun, I played with the representation of floats, following the definitions from the Standard C99 and I wrote the code below.</p>

<p>The code prints the binary representation of floats in 3 separated groups</p>

<pre><code>SIGN EXPONENT FRACTION
</code></pre>

<p>and after that it prints a sum, that, when summed with enough precision, it will show the value that really exists in hardware.</p>

<p>So when you write <code>float x = 999...</code>, the compiler will transform that number in a bit representation printed by the function <code>xx</code> such that the sum printed by the function <code>yy</code> be equal to the given number.</p>

<p>In reality, this sum is only an approximation.  For the number 999,999,999  the compiler will insert in bit representation of the float the number 1,000,000,000</p>

<p>After the code I attach a console session, in which I compute the sum of terms for both constants (minus PI and 999999999) that really exists in hardware, inserted there by the compiler.</p>

<pre><code>#include &lt;stdio.h&gt;
#include &lt;limits.h&gt;

void
xx(float *x)
{
    unsigned char i = sizeof(*x)*CHAR_BIT-1;
    do {
        switch (i) {
        case 31:
             printf("sign:");
             break;
        case 30:
             printf("exponent:");
             break;
        case 23:
             printf("fraction:");
             break;

        }
        char b=(*(unsigned long long*)x&amp;((unsigned long long)1&lt;&lt;i))!=0;
        printf("%d ", b);
    } while (i--);
    printf("\n");
}

void
yy(float a)
{
    int sign=!(*(unsigned long long*)&amp;a&amp;((unsigned long long)1&lt;&lt;31));
    int fraction = ((1&lt;&lt;23)-1)&amp;(*(int*)&amp;a);
    int exponent = (255&amp;((*(int*)&amp;a)&gt;&gt;23))-127;

    printf(sign?"positive" " ( 1+":"negative" " ( 1+");
    unsigned int i = 1&lt;&lt;22;
    unsigned int j = 1;
    do {
        char b=(fraction&amp;i)!=0;
        b&amp;&amp;(printf("1/(%d) %c", 1&lt;&lt;j, (fraction&amp;(i-1))?'+':')' ), 0);
    } while (j++, i&gt;&gt;=1);

    printf("*2^%d", exponent);
    printf("\n");
}

void
main()
{
    float x=-3.14;
    float y=999999999;
    printf("%lu\n", sizeof(x));
    xx(&amp;x);
    xx(&amp;y);
    yy(x);
    yy(y);
}
</code></pre>

<hr>

<p>Here is a console session in which I compute the real value of the float that exists in hardware.  I used <code>bc</code> to print the sum of terms outputted by the main program.  One can insert that sum in python <code>repl</code> or something similar also.</p>

<pre><code>-- .../terra1/stub
@ qemacs f.c
-- .../terra1/stub
@ gcc f.c
-- .../terra1/stub
@ ./a.out
sign:1 exponent:1 0 0 0 0 0 0 fraction:0 1 0 0 1 0 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 1
sign:0 exponent:1 0 0 1 1 1 0 fraction:0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 0
negative ( 1+1/(2) +1/(16) +1/(256) +1/(512) +1/(1024) +1/(2048) +1/(8192) +1/(32768) +1/(65536) +1/(131072) +1/(4194304) +1/(8388608) )*2^1
positive ( 1+1/(2) +1/(4) +1/(16) +1/(32) +1/(64) +1/(512) +1/(1024) +1/(4096) +1/(16384) +1/(32768) +1/(262144) +1/(1048576) )*2^29
-- .../terra1/stub
@ bc
scale=15
( 1+1/(2) +1/(4) +1/(16) +1/(32) +1/(64) +1/(512) +1/(1024) +1/(4096) +1/(16384) +1/(32768) +1/(262144) +1/(1048576) )*2^29
999999999.999999446351872
</code></pre>

<p>That's it.  The value of 999999999 is in fact</p>

<pre><code>999999999.999999446351872
</code></pre>

<p>You can also check with <code>bc</code> that -3.14 is also perturbed.  Do not forget to set a <code>scale</code> factor in <code>bc</code>.</p>

<p>The displayed sum is what inside the hardware.  The value you obtain by computing it depends on the scale you set.  I did set the <code>scale</code> factor to 15.  Mathematically, with infinite precision, it seems it is 1,000,000,000.</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 22</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p><a href="https://www.python.org/dev/peps/pep-0485/" rel="nofollow noreferrer">Since Python 3.5</a> you can use <a href="https://docs.python.org/3/library/math.html#math.isclose" rel="nofollow noreferrer"><code>math.isclose()</code></a> function for testing approximate equality:</p>
<pre><code>&gt;&gt;&gt; import math
&gt;&gt;&gt; math.isclose(0.1 + 0.2, 0.3)
True
&gt;&gt;&gt; 0.1 + 0.2 == 0.3
False
</code></pre>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 23</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>The trap with floating point numbers is that they look like decimal but they work in binary.</p>
<p>The only prime factor of 2 is 2, while 10 has prime factors of 2 and 5. The result of this is that every number that can be written exactly as a binary fraction can also be written exactly as a decimal fraction but only a subset of numbers that can be written as decimal fractions can be written as binary fractions.</p>
<p>A floating point number is essentially a binary fraction with a limited number of significant digits. If you go past those significant digits then the results will be rounded.</p>
<p>When you type a literal in your code or call the function to parse a floating point number to a string, it expects a decimal number and it stores a binary approximation of that decimal number in the variable.</p>
<p>When you print a floating point number or call the function to convert one to a string it prints a decimal approximation of the floating point number. It <em>is</em> possible to convert a binary number to decimal exactly, but no language I'm aware of does that by default when converting to a string*. Some languages use a fixed number of significant digits, others use the shortest string that will "round trip" back to the same floating point value.</p>
<p>* Python <em>does</em> convert exactly when converting a floating point number to a "decimal.Decimal". This is the easiest way I know of to obtain the exact decimal equivalent of a floating point number.</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 24</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Another way to look at this: Used are 64 bits to represent numbers. As consequence there is no way more than 2**64 = 18,446,744,073,709,551,616 different numbers can be precisely represented. </p>

<p>However, Math says there are already infinitely many decimals between 0 and 1. IEE 754 defines an encoding to use these 64 bits efficiently for a much larger number space plus NaN and +/- Infinity, so there are gaps between accurately represented numbers filled with numbers only approximated. </p>

<p>Unfortunately 0.3 sits in a gap.</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 25</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Floating point numbers are represented, at the hardware level, as fractions of binary numbers (base 2). For example, the decimal fraction:</p>
<pre><code>0.125
</code></pre>
<p>has the value 1/10 + 2/100 + 5/1000 and, in the same way, the binary fraction:</p>
<pre><code>0.001
</code></pre>
<p>has the value 0/2 + 0/4 + 1/8. These two fractions have the same value, the only difference is that the first is a decimal fraction, the second is a binary fraction.</p>
<p>Unfortunately, most decimal fractions cannot have exact representation in binary fractions. Therefore, in general, the floating point numbers you give are only approximated to binary fractions to be stored in the machine.</p>
<p>The problem is easier to approach in base 10. Take for example, the fraction 1/3. You can approximate it to a decimal fraction:</p>
<pre><code>0.3
</code></pre>
<p>or better,</p>
<pre><code>0.33
</code></pre>
<p>or better,</p>
<pre><code>0.333
</code></pre>
<p>etc. No matter how many decimal places you write, the result is never exactly 1/3, but it is an estimate that always comes closer.</p>
<p>Likewise, no matter how many base 2 decimal places you use, the decimal value 0.1 cannot be represented exactly as a binary fraction. In base 2, 1/10 is the following periodic number:</p>
<pre><code>0.0001100110011001100110011001100110011001100110011 ...
</code></pre>
<p>Stop at any finite amount of bits, and you'll get an approximation.</p>
<p>For Python, on a typical machine, 53 bits are used for the precision of a float, so the value stored when you enter the decimal 0.1 is the binary fraction.</p>
<pre><code>0.00011001100110011001100110011001100110011001100110011010
</code></pre>
<p>which is close, but not exactly equal, to 1/10.</p>
<p>It's easy to forget that the stored value is an approximation of the original decimal fraction, due to the way floats are displayed in the interpreter. Python only displays a decimal approximation of the value stored in binary. If Python were to output the true decimal value of the binary approximation stored for 0.1, it would output:</p>
<pre class="lang-py s-code-block"><code class="hljs language-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-number">0.1</span>
<span class="hljs-number">0.1000000000000000055511151231257827021181583404541015625</span>
</code></pre>
<p>This is a lot more decimal places than most people would expect, so Python displays a rounded value to improve readability:</p>
<pre class="lang-py s-code-block"><code class="hljs language-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-number">0.1</span>
<span class="hljs-number">0.1</span>
</code></pre>
<p>It is important to understand that in reality this is an illusion: the stored value is not exactly 1/10, it is simply on the display that the stored value is rounded. This becomes evident as soon as you perform arithmetic operations with these values:</p>
<pre class="lang-py s-code-block"><code class="hljs language-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-number">0.1</span> + <span class="hljs-number">0.2</span>
<span class="hljs-number">0.30000000000000004</span>
</code></pre>
<p>This behavior is inherent to the very nature of the machine's floating-point representation: it is not a bug in Python, nor is it a bug in your code. You can observe the same type of behavior in all other languages that use hardware support for calculating floating point numbers (although some languages do not make the difference visible by default, or not in all display modes).</p>
<p>Another surprise is inherent in this one. For example, if you try to round the value 2.675 to two decimal places, you will get</p>
<pre class="lang-py s-code-block"><code class="hljs language-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span> (<span class="hljs-number">2.675</span>, <span class="hljs-number">2</span>)
<span class="hljs-number">2.67</span>
</code></pre>
<p>The documentation for the round() primitive indicates that it rounds to the nearest value away from zero. Since the decimal fraction is exactly halfway between 2.67 and 2.68, you should expect to get (a binary approximation of) 2.68. This is not the case, however, because when the decimal fraction 2.675 is converted to a float, it is stored by an approximation whose exact value is :</p>
<pre><code>2.67499999999999982236431605997495353221893310546875
</code></pre>
<p>Since the approximation is slightly closer to 2.67 than 2.68, the rounding is down.</p>
<p>If you are in a situation where rounding decimal numbers halfway down matters, you should use the decimal module. By the way, the decimal module also provides a convenient way to "see" the exact value stored for any float.</p>
<pre class="lang-py s-code-block"><code class="hljs language-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> decimal <span class="hljs-keyword">import</span> Decimal
<span class="hljs-meta">&gt;&gt;&gt; </span>Decimal (<span class="hljs-number">2.675</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>Decimal (<span class="hljs-string">'2.67499999999999982236431605997495353221893310546875'</span>)
</code></pre>
<p>Another consequence of the fact that 0.1 is not exactly stored in 1/10 is that the sum of ten values of 0.1 does not give 1.0 either:</p>
<pre class="lang-py s-code-block"><code class="hljs language-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">sum</span> = <span class="hljs-number">0.0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span> (<span class="hljs-number">10</span>):
<span class="hljs-meta">... </span><span class="hljs-built_in">sum</span> + = <span class="hljs-number">0.1</span>
...&gt;&gt;&gt; <span class="hljs-built_in">sum</span>
<span class="hljs-number">0.9999999999999999</span>
</code></pre>
<p>The arithmetic of binary floating point numbers holds many such surprises. The problem with "0.1" is explained in detail below, in the section "Representation errors". See The Perils of Floating Point for a more complete list of such surprises.</p>
<p>It is true that there is no simple answer, however do not be overly suspicious of floating virtula numbers! Errors, in Python, in floating-point number operations are due to the underlying hardware, and on most machines are no more than 1 in 2 ** 53 per operation. This is more than necessary for most tasks, but you should keep in mind that these are not decimal operations, and every operation on floating point numbers may suffer from a new error.</p>
<p>Although pathological cases exist, for most common use cases you will get the expected result at the end by simply rounding up to the number of decimal places you want on the display. For fine control over how floats are displayed, see String Formatting Syntax for the formatting specifications of the str.format () method.</p>
<p>This part of the answer explains in detail the example of "0.1" and shows how you can perform an exact analysis of this type of case on your own. We assume that you are familiar with the binary representation of floating point numbers.The term Representation error means that most decimal fractions cannot be represented exactly in binary. This is the main reason why Python (or Perl, C, C ++, Java, Fortran, and many others) usually doesn't display the exact result in decimal:</p>
<pre><code>&gt;&gt;&gt; 0.1 + 0.2
0.30000000000000004
</code></pre>
<p>Why ? 1/10 and 2/10 are not representable exactly in binary fractions. However, all machines today (July 2010) follow the IEEE-754 standard for the arithmetic of floating point numbers. and most platforms use an "IEEE-754 double precision" to represent Python floats. Double precision IEEE-754 uses 53 bits of precision, so on reading the computer tries to convert 0.1 to the nearest fraction of the form J / 2 ** N with J an integer of exactly 53 bits. Rewrite :</p>
<pre><code>1/10 ~ = J / (2 ** N)
</code></pre>
<p>in :</p>
<pre><code>J ~ = 2 ** N / 10
</code></pre>
<p>remembering that J is exactly 53 bits (so&gt; = 2 ** 52 but &lt;2 ** 53), the best possible value for N is 56:</p>
<pre class="lang-py s-code-block"><code class="hljs language-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-number">2</span> ** <span class="hljs-number">52</span>
<span class="hljs-number">4503599627370496</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-number">2</span> ** <span class="hljs-number">53</span>
<span class="hljs-number">9007199254740992</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-number">2</span> ** <span class="hljs-number">56</span>/<span class="hljs-number">10</span>
<span class="hljs-number">7205759403792793</span>
</code></pre>
<p>So 56 is the only possible value for N which leaves exactly 53 bits for J. The best possible value for J is therefore this quotient, rounded:</p>
<pre class="lang-py s-code-block"><code class="hljs language-python"><span class="hljs-meta">&gt;&gt;&gt; </span>q, r = <span class="hljs-built_in">divmod</span> (<span class="hljs-number">2</span> ** <span class="hljs-number">56</span>, <span class="hljs-number">10</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>r
<span class="hljs-number">6</span>
</code></pre>
<p>Since the carry is greater than half of 10, the best approximation is obtained by rounding up:</p>
<pre><code>&gt;&gt;&gt; q + 1
7205759403792794
</code></pre>
<p>Therefore the best possible approximation for 1/10 in "IEEE-754 double precision" is this above 2 ** 56, that is:</p>
<pre><code>7205759403792794/72057594037927936
</code></pre>
<p>Note that since the rounding was done upward, the result is actually slightly greater than 1/10; if we hadn't rounded up, the quotient would have been slightly less than 1/10. But in no case is it exactly 1/10!</p>
<p>So the computer never "sees" 1/10: what it sees is the exact fraction given above, the best approximation using the double precision floating point numbers from the "" IEEE-754 ":</p>
<pre><code>&gt;&gt;&gt;. 1 * 2 ** 56
7205759403792794.0
</code></pre>
<p>If we multiply this fraction by 10 ** 30, we can observe the values of its 30 decimal places of strong weight.</p>
<pre><code>&gt;&gt;&gt; 7205759403792794 * 10 ** 30 // 2 ** 56
100000000000000005551115123125L
</code></pre>
<p>meaning that the exact value stored in the computer is approximately equal to the decimal value 0.100000000000000005551115123125. In versions prior to Python 2.7 and Python 3.1, Python rounded these values to 17 significant decimal places, displaying 0.10000000000000001. In current versions of Python, the displayed value is the value whose fraction is as short as possible while giving exactly the same representation when converted back to binary, simply displaying 0.1.</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 26</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Imagine working in base ten with, say, 8 digits of accuracy.  You check whether </p>

<pre><code>1/3 + 2 / 3 == 1
</code></pre>

<p>and learn that this returns <code>false</code>.  Why?  Well, as real numbers we have</p>

<p><em>1/3 = 0.333....</em> and <em>2/3 = 0.666....</em></p>

<p>Truncating at eight decimal places, we get</p>

<pre><code>0.33333333 + 0.66666666 = 0.99999999
</code></pre>

<p>which is, of course, different from <code>1.00000000</code> by exactly <code>0.00000001</code>.</p>

<hr>

<p>The situation for binary numbers with a fixed number of bits is exactly analogous. As real numbers, we have</p>

<p><em>1/10 = 0.0001100110011001100... (base 2)</em></p>

<p>and</p>

<p><em>1/5 = 0.0011001100110011001... (base 2)</em></p>

<p>If we truncated these to, say, seven bits, then we'd get</p>

<pre><code>0.0001100 + 0.0011001 = 0.0100101
</code></pre>

<p>while on the other hand,</p>

<p><em>3/10 = 0.01001100110011... (base 2)</em></p>

<p>which, truncated to seven bits, is <code>0.0100110</code>, and these differ by exactly <code>0.0000001</code>.</p>

<hr>

<p>The exact situation is slightly more subtle because these numbers are typically stored in scientific notation.  So, for instance, instead of storing 1/10 as <code>0.0001100</code> we may store it as something like <code>1.10011 * 2^-4</code>, depending on how many bits we've allocated for the exponent and the mantissa.  This affects how many digits of precision you get for your calculations.</p>

<p>The upshot is that because of these rounding errors you essentially never want to use == on floating-point numbers.  Instead, you can check if the absolute value of their difference is smaller than some fixed small number.</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 27</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<blockquote>
  <p>It's actually pretty simple. When you have a base 10 system (like ours), it can only express fractions that use a prime factor of the base. The prime factors of 10 are 2 and 5. So 1/2, 1/4, 1/5, 1/8, and 1/10 can all be expressed cleanly because the denominators all use prime factors of 10. In contrast, 1/3, 1/6, and 1/7 are all repeating decimals because their denominators use a prime factor of 3 or 7. In binary (or base 2), the only prime factor is 2. So you can only express fractions cleanly which only contain 2 as a prime factor. In binary, 1/2, 1/4, 1/8 would all be expressed cleanly as decimals. While, 1/5 or 1/10 would be repeating decimals. So 0.1 and 0.2 (1/10 and 1/5) while clean decimals in a base 10 system, are repeating decimals in the base 2 system the computer is operating in. When you do math on these repeating decimals, you end up with leftovers which carry over when you convert the computer's base 2 (binary) number into a more human readable base 10 number.</p>
</blockquote>

<p>From <a href="https://0.30000000000000004.com/" rel="noreferrer">https://0.30000000000000004.com/</a></p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 28</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Decimal numbers such as <code>0.1</code>, <code>0.2</code>, and <code>0.3</code> are not represented exactly in binary encoded floating point types. The sum of the approximations for <code>0.1</code> and <code>0.2</code> differs from the approximation used for <code>0.3</code>, hence the falsehood of <code>0.1 + 0.2 == 0.3</code> as can be seen more clearly here:</p>

<pre class="lang-c s-code-block"><code class="hljs language-c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span></span>

<span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span> {
    <span class="hljs-built_in">printf</span>(<span class="hljs-string">"0.1 + 0.2 == 0.3 is %s\n"</span>, <span class="hljs-number">0.1</span> + <span class="hljs-number">0.2</span> == <span class="hljs-number">0.3</span> ? <span class="hljs-string">"true"</span> : <span class="hljs-string">"false"</span>);
    <span class="hljs-built_in">printf</span>(<span class="hljs-string">"0.1 is %.23f\n"</span>, <span class="hljs-number">0.1</span>);
    <span class="hljs-built_in">printf</span>(<span class="hljs-string">"0.2 is %.23f\n"</span>, <span class="hljs-number">0.2</span>);
    <span class="hljs-built_in">printf</span>(<span class="hljs-string">"0.1 + 0.2 is %.23f\n"</span>, <span class="hljs-number">0.1</span> + <span class="hljs-number">0.2</span>);
    <span class="hljs-built_in">printf</span>(<span class="hljs-string">"0.3 is %.23f\n"</span>, <span class="hljs-number">0.3</span>);
    <span class="hljs-built_in">printf</span>(<span class="hljs-string">"0.3 - (0.1 + 0.2) is %g\n"</span>, <span class="hljs-number">0.3</span> - (<span class="hljs-number">0.1</span> + <span class="hljs-number">0.2</span>));
    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;
}
</code></pre>

<p>Output:</p>

<pre><code>0.1 + 0.2 == 0.3 is false
0.1 is 0.10000000000000000555112
0.2 is 0.20000000000000001110223
0.1 + 0.2 is 0.30000000000000004440892
0.3 is 0.29999999999999998889777
0.3 - (0.1 + 0.2) is -5.55112e-17
</code></pre>

<p>For these computations to be evaluated more reliably, you would need to use a decimal-based representation for floating point values. The C Standard does not specify such types by default but as an extension described in a <a href="http://www.open-std.org/JTC1/SC22/WG14/www/docs/n1312.pdf" rel="noreferrer">technical Report</a>.</p>

<p>The <code>_Decimal32</code>, <code>_Decimal64</code> and <code>_Decimal128</code> types might be available on your system (for example, <a href="http://en.wikipedia.org/wiki/GNU_Compiler_Collection" rel="noreferrer">GCC</a> supports them on <a href="https://gcc.gnu.org/onlinedocs/gcc-4.2.4/gcc/Decimal-Float.html" rel="noreferrer">selected targets</a>, but <a href="http://en.wikipedia.org/wiki/Clang" rel="noreferrer">Clang</a> does not support them on <a href="http://en.wikipedia.org/wiki/OS_X" rel="noreferrer">OS&nbsp;X</a>).</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 29</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Since this thread branched off a bit into a general discussion over current floating point implementations I'd add that there are projects on fixing their issues.</p>

<p>Take a look at <a href="https://posithub.org/" rel="nofollow noreferrer">https://posithub.org/</a> for example, which showcases a number type called posit (and its predecessor unum) that promises to offer better accuracy with fewer bits. If my understanding is correct, it also fixes the kind of problems in the question. Quite interesting project, the person behind it is a mathematician it <a href="http://www.johngustafson.net/" rel="nofollow noreferrer">Dr. John Gustafson</a>. The whole thing is open source, with many actual implementations in C/C++, Python, Julia and C# (<a href="https://hastlayer.com/arithmetics" rel="nofollow noreferrer">https://hastlayer.com/arithmetics</a>).</p>
    </div></div></div></div><div class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl border p-10 shadow-md w-9/12 bg-white"><h4 class="text-4xl font-semibold mb-5">Solution 30</h4><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Normal arithmetic is base-10, so decimals represent tenths, hundredths, etc.  When you try to represent a floating-point number in binary base-2 arithmetic, you are dealing with halves, fourths, eighths, etc.</p>
<p>In the hardware, floating points are stored as integer mantissas and exponents.  Mantissa represents the significant digits.  Exponent is like scientific notation but it uses a base of 2 instead of 10.  For example 64.0 would be represented with a mantissa of 1 and exponent of 6.  0.125 would be represented with a mantissa of 1 and an exponent of -3.</p>
<p>Floating point decimals have to add up negative powers of 2</p>
<pre><code>0.1b = 0.5d
0.01b = 0.25d
0.001b = 0.125d
0.0001b = 0.0625d
0.00001b = 0.03125d
</code></pre>
<p>and so on.</p>
<p>It is common to use a error delta instead of using equality operators when dealing with floating point arithmetic.  Instead of</p>
<pre><code>if(a==b) ...
</code></pre>
<p>you would use</p>
<pre><code>delta = 0.0001; // or some arbitrarily small amount
if(a - b &gt; -delta &amp;&amp; a - b &lt; delta) ...
</code></pre>
    </div></div></div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"data":{"answer":["\n\u0026lt;p\u0026gt;Binary \u0026lt;a href=\u0026quot;https://en.wikipedia.org/wiki/Double-precision_floating-point_format\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;floating point\u0026lt;/a\u0026gt; math is like this. In most programming languages, it is based on the \u0026lt;a href=\u0026quot;https://en.wikipedia.org/wiki/IEEE_754#Basic_and_interchange_formats\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;IEEE 754 standard\u0026lt;/a\u0026gt;. The crux of the problem is that numbers are represented in this format as a whole number times a power of two; rational numbers (such as \u0026lt;code\u0026gt;0.1\u0026lt;/code\u0026gt;, which is \u0026lt;code\u0026gt;1/10\u0026lt;/code\u0026gt;) whose denominator is not a power of two cannot be exactly represented.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;For \u0026lt;code\u0026gt;0.1\u0026lt;/code\u0026gt; in the standard \u0026lt;code\u0026gt;binary64\u0026lt;/code\u0026gt; format, the representation can be written exactly as\u0026lt;/p\u0026gt;\n\u0026lt;ul\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;code\u0026gt;0.1000000000000000055511151231257827021181583404541015625\u0026lt;/code\u0026gt; in decimal, or\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;code\u0026gt;0x1.999999999999ap-4\u0026lt;/code\u0026gt; in \u0026lt;a href=\u0026quot;http://www.exploringbinary.com/hexadecimal-floating-point-constants/\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;C99 hexfloat notation\u0026lt;/a\u0026gt;.\u0026lt;/li\u0026gt;\n\u0026lt;/ul\u0026gt;\n\u0026lt;p\u0026gt;In contrast, the rational number \u0026lt;code\u0026gt;0.1\u0026lt;/code\u0026gt;, which is \u0026lt;code\u0026gt;1/10\u0026lt;/code\u0026gt;, can be written exactly as\u0026lt;/p\u0026gt;\n\u0026lt;ul\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;code\u0026gt;0.1\u0026lt;/code\u0026gt; in decimal, or\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;code\u0026gt;0x1.99999999999999...p-4\u0026lt;/code\u0026gt; in an analogue of C99 hexfloat notation, where the \u0026lt;code\u0026gt;...\u0026lt;/code\u0026gt; represents an unending sequence of 9\u0026apos;s.\u0026lt;/li\u0026gt;\n\u0026lt;/ul\u0026gt;\n\u0026lt;p\u0026gt;The constants \u0026lt;code\u0026gt;0.2\u0026lt;/code\u0026gt; and \u0026lt;code\u0026gt;0.3\u0026lt;/code\u0026gt; in your program will also be approximations to their true values.  It happens that the closest \u0026lt;code\u0026gt;double\u0026lt;/code\u0026gt; to \u0026lt;code\u0026gt;0.2\u0026lt;/code\u0026gt; is larger than the rational number \u0026lt;code\u0026gt;0.2\u0026lt;/code\u0026gt; but that the closest \u0026lt;code\u0026gt;double\u0026lt;/code\u0026gt; to \u0026lt;code\u0026gt;0.3\u0026lt;/code\u0026gt; is smaller than the rational number \u0026lt;code\u0026gt;0.3\u0026lt;/code\u0026gt;.  The sum of \u0026lt;code\u0026gt;0.1\u0026lt;/code\u0026gt; and \u0026lt;code\u0026gt;0.2\u0026lt;/code\u0026gt; winds up being larger than the rational number \u0026lt;code\u0026gt;0.3\u0026lt;/code\u0026gt; and hence disagreeing with the constant in your code.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;A fairly comprehensive treatment of floating-point arithmetic issues is \u0026lt;a href=\u0026quot;http://download.oracle.com/docs/cd/E19957-01/806-3568/ncg_goldberg.html\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;\u0026lt;em\u0026gt;What Every Computer Scientist Should Know About Floating-Point Arithmetic\u0026lt;/em\u0026gt;\u0026lt;/a\u0026gt;. For an easier-to-digest explanation, see \u0026lt;a href=\u0026quot;http://floating-point-gui.de\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;floating-point-gui.de\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Side Note: All positional (base-N) number systems share this problem with precision\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Plain old decimal (base 10) numbers have the same issues, which is why numbers like 1/3 end up as 0.333333333...\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;You\u0026apos;ve just stumbled on a number (3/10) that happens to be easy to represent with the decimal system, but doesn\u0026apos;t fit the binary system. It goes both ways (to some small degree) as well: 1/16 is an ugly number in decimal (0.0625), but in binary it looks as neat as a 10,000th does in decimal (0.0001)** - if we were in the habit of using a base-2 number system in our daily lives, you\u0026apos;d even look at that number and instinctively understand you could arrive there by halving something, halving it again, and again and again.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;** Of course, that\u0026apos;s not exactly how floating-point numbers are stored in memory (they use a form of scientific notation). However, it does illustrate the point that binary floating-point precision errors tend to crop up because the \u0026quot;real world\u0026quot; numbers we are usually interested in working with are so often powers of ten - but only because we use a decimal number system day-to-day. This is also why we\u0026apos;ll say things like 71% instead of \u0026quot;5 out of every 7\u0026quot; (71% is an approximation, since 5/7 can\u0026apos;t be represented exactly with any decimal number).\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;So no: binary floating point numbers are not broken, they just happen to be as imperfect as every other base-N number system :)\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Side Side Note: Working with Floats in Programming\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;In practice, this problem of precision means you need to use rounding functions to round your floating point numbers off to however many decimal places you\u0026apos;re interested in before you display them.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;You also need to replace equality tests with comparisons that allow some amount of tolerance, which means:\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Do \u0026lt;em\u0026gt;not\u0026lt;/em\u0026gt; do \u0026lt;code\u0026gt;if (x == y) { ... }\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Instead do \u0026lt;code\u0026gt;if (abs(x - y) \u0026amp;lt; myToleranceValue) { ... }\u0026lt;/code\u0026gt;.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;where \u0026lt;code\u0026gt;abs\u0026lt;/code\u0026gt; is the absolute value. \u0026lt;code\u0026gt;myToleranceValue\u0026lt;/code\u0026gt; needs to be chosen for your particular application - and it will have a lot to do with how much \u0026quot;wiggle room\u0026quot; you are prepared to allow, and what the largest number you are going to be comparing may be (due to loss of precision issues). Beware of \u0026quot;epsilon\u0026quot; style constants in your language of choice. These are \u0026lt;em\u0026gt;not\u0026lt;/em\u0026gt; to be used as tolerance values.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;h1\u0026gt;\u0026lt;strong\u0026gt;A Hardware Designer\u0026apos;s Perspective\u0026lt;/strong\u0026gt;\u0026lt;/h1\u0026gt;\n\n\u0026lt;p\u0026gt;I believe I should add a hardware designers perspective to this since I design and build floating point hardware. Knowing the origin of the error may help in understanding what is happening in the software, and ultimately, I hope this helps explain the reasons for why floating point errors happen and seem to accumulate over time.\u0026lt;/p\u0026gt;\n\n\u0026lt;h2\u0026gt;1. Overview\u0026lt;/h2\u0026gt;\n\n\u0026lt;p\u0026gt;From an engineering perspective, most floating point operations will have some element of error since the hardware that does the floating point computations is only required to have an error of less than one half of one unit in the last place. Therefore, much hardware will stop at a precision that\u0026apos;s only necessary to yield an error of less than one half of one unit in the last place for a \u0026lt;em\u0026gt;single operation\u0026lt;/em\u0026gt; which is especially problematic in floating point division. What constitutes a single operation depends upon how many operands the unit takes. For most, it is two, but some units take 3 or more operands. Because of this, there is no guarantee that repeated operations will result in a desirable error since the errors add up over time.\u0026lt;/p\u0026gt;\n\n\u0026lt;h2\u0026gt;2. Standards\u0026lt;/h2\u0026gt;\n\n\u0026lt;p\u0026gt;Most processors follow the \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/IEEE_754-2008\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;IEEE-754\u0026lt;/a\u0026gt; standard but some use denormalized, or different standards\n. For example, there is a denormalized mode in IEEE-754 which allows representation of very small floating point numbers at the expense of precision. The following, however, will cover the normalized mode of IEEE-754 which is the typical mode of operation.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;In the IEEE-754 standard, hardware designers are allowed any value of error/epsilon as long as it\u0026apos;s less than one half of one unit in the last place, and the result only has to be less than one half of one unit in the last place for one operation. This explains why when there are repeated operations, the errors add up. For IEEE-754 double precision, this is the 54th bit, since 53 bits are used to represent the numeric part (normalized), also called the mantissa, of the floating point number (e.g. the 5.3 in 5.3e5). The next sections go into more detail on the causes of hardware error on various floating point operations.\u0026lt;/p\u0026gt;\n\n\u0026lt;h2\u0026gt;3. Cause of Rounding Error in Division\u0026lt;/h2\u0026gt;\n\n\u0026lt;p\u0026gt;The main cause of the error in floating point division is the division algorithms used to calculate the quotient. Most computer systems calculate division using multiplication by an inverse, mainly in \u0026lt;code\u0026gt;Z=X/Y\u0026lt;/code\u0026gt;, \u0026lt;code\u0026gt;Z = X * (1/Y)\u0026lt;/code\u0026gt;.  A division is computed iteratively i.e. each cycle computes some bits of the quotient until the desired precision is reached, which for IEEE-754 is anything with an error of less than one unit in the last place. The table of reciprocals of Y (1/Y) is known as the quotient selection table (QST) in the slow division, and the size in bits of the quotient selection table is usually the width of the radix, or a number of bits of the quotient computed in each iteration,  plus a few guard bits. For the IEEE-754 standard, double precision (64-bit), it would be the size of the radix of the divider, plus a few guard bits k, where \u0026lt;code\u0026gt;k\u0026amp;gt;=2\u0026lt;/code\u0026gt;. So for example, a typical Quotient Selection Table for a divider that computes 2 bits of the quotient at a time (radix 4) would be \u0026lt;code\u0026gt;2+2= 4\u0026lt;/code\u0026gt; bits (plus a few optional bits). \u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;3.1 Division Rounding Error: Approximation of Reciprocal\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;What reciprocals are in the quotient selection table depend on the \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Division_%28digital%29\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;division method\u0026lt;/a\u0026gt;: slow division such as SRT division, or fast division such as Goldschmidt division; each entry is modified according to the division algorithm in an attempt to yield the lowest possible error. In any case, though, all reciprocals are \u0026lt;em\u0026gt;approximations\u0026lt;/em\u0026gt; of the actual reciprocal and introduce some element of error. Both slow division and fast division methods calculate the quotient iteratively, i.e. some number of bits of the quotient are calculated each step, then the result is subtracted from the dividend, and the divider repeats the steps until the error is less than one half of one unit in the last place. Slow division methods calculate a fixed number of digits of the quotient in each step and are usually less expensive to build, and fast division methods calculate a variable number of digits per step and are usually more expensive to build. The most important part of the division methods is that most of them rely upon repeated multiplication by an \u0026lt;em\u0026gt;approximation\u0026lt;/em\u0026gt; of a reciprocal, so they are prone to error.\u0026lt;/p\u0026gt;\n\n\u0026lt;h2\u0026gt;4. Rounding Errors in Other Operations: Truncation\u0026lt;/h2\u0026gt;\n\n\u0026lt;p\u0026gt;Another cause of the rounding errors in all operations are the different modes of truncation of the final answer that IEEE-754 allows. There\u0026apos;s truncate, round-towards-zero, \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Floating_point#Rounding_modes\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;round-to-nearest (default),\u0026lt;/a\u0026gt; round-down, and round-up. All methods introduce an element of error of less than one unit in the last place for a single operation. Over time and repeated operations, truncation also adds cumulatively to the resultant error. This truncation error is especially problematic in exponentiation, which involves some form of repeated multiplication.\u0026lt;/p\u0026gt;\n\n\u0026lt;h2\u0026gt;5. Repeated Operations\u0026lt;/h2\u0026gt;\n\n\u0026lt;p\u0026gt;Since the hardware that does the floating point calculations only needs to yield a result with an error of less than one half of one unit in the last place for a single operation, the error will grow over repeated operations if not watched. This is the reason that in computations that require a bounded error, mathematicians use methods such as using the round-to-nearest \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Floating_point#Rounding_modes\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;even digit in the last place\u0026lt;/a\u0026gt; of IEEE-754, because, over time, the errors are more likely to cancel each other out, and \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Interval_arithmetic\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;Interval Arithmetic\u0026lt;/a\u0026gt; combined with variations of the \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/IEEE_754-2008#Rounding_rules\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;IEEE 754 rounding modes\u0026lt;/a\u0026gt; to predict rounding errors, and correct them. Because of its low relative error compared to other rounding modes, round to nearest even digit (in the last place), is the default rounding mode of IEEE-754.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Note that the default rounding mode, round-to-nearest \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Floating_point#Rounding_modes\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;even digit in the last place\u0026lt;/a\u0026gt;, guarantees an error of less than one half of one unit in the last place for one operation. Using the truncation, round-up, and round down alone may result in an error that is greater than one half of one unit in the last place, but less than one unit in the last place, so these modes are not recommended unless they are used in Interval Arithmetic. \u0026lt;/p\u0026gt;\n\n\u0026lt;h2\u0026gt;6. Summary\u0026lt;/h2\u0026gt;\n\n\u0026lt;p\u0026gt;In short, the fundamental reason for the errors in floating point operations is a combination of the truncation in hardware, and the truncation of a reciprocal in the case of division. Since the IEEE-754 standard only requires an error of less than one half of one unit in the last place for a single operation, the floating point errors over repeated operations will add up unless corrected.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;It\u0026apos;s broken in the exact same way the decimal (base-10) notation you learned in grade school and use every day is broken, just for base-2.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;To understand, think about representing 1/3 as a decimal value. It\u0026apos;s impossible to do exactly! The world will end before you finish writing the 3\u0026apos;s after the decimal point, and so instead we write to some number of places and consider it sufficiently accurate.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;In the same way, 1/10 (decimal 0.1) cannot be represented exactly in base 2 (binary) as a \u0026quot;decimal\u0026quot; value; a repeating pattern after the decimal point goes on forever. The value is not exact, and therefore you can\u0026apos;t do exact math with it using normal floating point methods. Just like with base 10, there are other values that exhibit this problem as well.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Most answers here address this question in very dry, technical terms. I\u0026apos;d like to address this in terms that normal human beings can understand.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Imagine that you are trying to slice up pizzas. You have a robotic pizza cutter that can cut pizza slices \u0026lt;em\u0026gt;exactly\u0026lt;/em\u0026gt; in half. It can halve a whole pizza, or it can halve an existing slice, but in any case, the halving is always exact.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;That pizza cutter has very fine movements, and if you start with a whole pizza, then halve that, and continue halving the smallest slice each time, you can do the halving \u0026lt;em\u0026gt;53 times\u0026lt;/em\u0026gt; before the slice is too small for even its high-precision abilities. At that point, you can no longer halve that very thin slice, but must either include or exclude it as is.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Now, how would you piece all the slices in such a way that would add up to one-tenth (0.1) or one-fifth (0.2) of a pizza? Really think about it, and try working it out. You can even try to use a real pizza, if you have a mythical precision pizza cutter at hand. :-)\u0026lt;/p\u0026gt;\n\n\u0026lt;hr\u0026gt;\n\n\u0026lt;p\u0026gt;Most experienced programmers, of course, know the real answer, which is that there is no way to piece together an \u0026lt;em\u0026gt;exact\u0026lt;/em\u0026gt; tenth or fifth of the pizza using those slices, no matter how finely you slice them. You can do a pretty good approximation, and if you add up the approximation of 0.1 with the approximation of 0.2, you get a pretty good approximation of 0.3, but it\u0026apos;s still just that, an approximation.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;For double-precision numbers (which is the precision that allows you to halve your pizza 53 times), the numbers immediately less and greater than 0.1 are 0.09999999999999999167332731531132594682276248931884765625 and 0.1000000000000000055511151231257827021181583404541015625. The latter is quite a bit closer to 0.1 than the former, so a numeric parser will, given an input of 0.1, favour the latter.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;(The difference between those two numbers is the \u0026quot;smallest slice\u0026quot; that we must decide to either include, which introduces an upward bias, or exclude, which introduces a downward bias. The technical term for that smallest slice is an \u0026lt;a href=\u0026quot;https://en.wikipedia.org/wiki/Unit_in_the_last_place\u0026quot;\u0026gt;ulp\u0026lt;/a\u0026gt;.)\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;In the case of 0.2, the numbers are all the same, just scaled up by a factor of 2. Again, we favour the value that\u0026apos;s slightly higher than 0.2.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Notice that in both cases, the approximations for 0.1 and 0.2 have a slight upward bias. If we add enough of these biases in, they will push the number further and further away from what we want, and in fact, in the case of 0.1 + 0.2, the bias is high enough that the resulting number is no longer the closest number to 0.3.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;In particular, 0.1 + 0.2 is really 0.1000000000000000055511151231257827021181583404541015625 + 0.200000000000000011102230246251565404236316680908203125 = 0.3000000000000000444089209850062616169452667236328125, whereas the number closest to 0.3 is actually 0.299999999999999988897769753748434595763683319091796875.\u0026lt;/p\u0026gt;\n\n\u0026lt;hr\u0026gt;\n\n\u0026lt;p\u0026gt;P.S. Some programming languages also provide pizza cutters that can \u0026lt;a href=\u0026quot;https://en.wikipedia.org/wiki/Decimal_floating_point\u0026quot;\u0026gt;split slices into exact tenths\u0026lt;/a\u0026gt;. Although such pizza cutters are uncommon, if you do have access to one, you should use it when it\u0026apos;s important to be able to get exactly one-tenth or one-fifth of a slice.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;http://qr.ae/mDcAq\u0026quot;\u0026gt;\u0026lt;em\u0026gt;(Originally posted on Quora.)\u0026lt;/em\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Floating point rounding errors. 0.1 cannot be represented as accurately in base-2 as in base-10 due to the missing prime factor of 5. Just as 1/3 takes an infinite number of digits to represent in decimal, but is \u0026quot;0.1\u0026quot; in base-3, 0.1 takes an infinite number of digits in base-2 where it does not in base-10. And computers don\u0026apos;t have an infinite amount of memory.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;My answer is quite long, so I\u0026apos;ve split it into three sections. Since the question is about floating point mathematics, I\u0026apos;ve put the emphasis on what the machine actually does. I\u0026apos;ve also made it specific to double (64 bit) precision, but the argument applies equally to any floating point arithmetic.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Preamble\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;An \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Double-precision_floating-point_format\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;IEEE 754 double-precision binary floating-point format (binary64)\u0026lt;/a\u0026gt; number represents a number of the form\u0026lt;/p\u0026gt;\n\n\u0026lt;blockquote\u0026gt;\n  \u0026lt;p\u0026gt;value = (-1)^s * (1.m\u0026lt;sub\u0026gt;51\u0026lt;/sub\u0026gt;m\u0026lt;sub\u0026gt;50\u0026lt;/sub\u0026gt;...m\u0026lt;sub\u0026gt;2\u0026lt;/sub\u0026gt;m\u0026lt;sub\u0026gt;1\u0026lt;/sub\u0026gt;m\u0026lt;sub\u0026gt;0\u0026lt;/sub\u0026gt;)\u0026lt;sub\u0026gt;2\u0026lt;/sub\u0026gt; * 2\u0026lt;sup\u0026gt;e-1023\u0026lt;/sup\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;/blockquote\u0026gt;\n\n\u0026lt;p\u0026gt;in 64 bits:\u0026lt;/p\u0026gt;\n\n\u0026lt;ul\u0026gt;\n\u0026lt;li\u0026gt;The first bit is the \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Sign_bit\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;sign bit\u0026lt;/a\u0026gt;: \u0026lt;code\u0026gt;1\u0026lt;/code\u0026gt; if the number is negative, \u0026lt;code\u0026gt;0\u0026lt;/code\u0026gt; otherwise\u0026lt;sup\u0026gt;1\u0026lt;/sup\u0026gt;.\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;The next 11 bits are the \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Exponentiation\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;exponent\u0026lt;/a\u0026gt;, which is \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Offset_binary\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;offset\u0026lt;/a\u0026gt; by 1023. In other words, after reading the exponent bits from a double-precision number, 1023 must be subtracted to obtain the power of two.\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;The remaining 52 bits are the \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Significand\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;significand\u0026lt;/a\u0026gt; (or mantissa). In the mantissa, an \u0026apos;implied\u0026apos; \u0026lt;code\u0026gt;1.\u0026lt;/code\u0026gt; is always\u0026lt;sup\u0026gt;2\u0026lt;/sup\u0026gt; omitted since the most significant bit of any binary value is \u0026lt;code\u0026gt;1\u0026lt;/code\u0026gt;.\u0026lt;/li\u0026gt;\n\u0026lt;/ul\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;sup\u0026gt;1\u0026lt;/sup\u0026gt; - IEEE 754 allows for the concept of a \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Signed_zero\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;signed zero\u0026lt;/a\u0026gt; - \u0026lt;code\u0026gt;+0\u0026lt;/code\u0026gt; and \u0026lt;code\u0026gt;-0\u0026lt;/code\u0026gt; are treated differently: \u0026lt;code\u0026gt;1 / (+0)\u0026lt;/code\u0026gt; is positive infinity; \u0026lt;code\u0026gt;1 / (-0)\u0026lt;/code\u0026gt; is negative infinity. For zero values, the mantissa and exponent bits are all zero. Note: zero values (+0 and -0) are explicitly not classed as denormal\u0026lt;sup\u0026gt;2\u0026lt;/sup\u0026gt;.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;sup\u0026gt;2\u0026lt;/sup\u0026gt; - This is not the case for \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Denormal_number\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;denormal numbers\u0026lt;/a\u0026gt;, which have an offset exponent of zero (and an implied \u0026lt;code\u0026gt;0.\u0026lt;/code\u0026gt;). The range of denormal double precision numbers is d\u0026lt;sub\u0026gt;min\u0026lt;/sub\u0026gt;  |x|  d\u0026lt;sub\u0026gt;max\u0026lt;/sub\u0026gt;, where d\u0026lt;sub\u0026gt;min\u0026lt;/sub\u0026gt; (the smallest representable nonzero number) is 2\u0026lt;sup\u0026gt;-1023 - 51\u0026lt;/sup\u0026gt; ( 4.94 * 10\u0026lt;sup\u0026gt;-324\u0026lt;/sup\u0026gt;) and d\u0026lt;sub\u0026gt;max\u0026lt;/sub\u0026gt; (the largest denormal number, for which the mantissa consists entirely of \u0026lt;code\u0026gt;1\u0026lt;/code\u0026gt;s) is 2\u0026lt;sup\u0026gt;-1023 + 1\u0026lt;/sup\u0026gt; - 2\u0026lt;sup\u0026gt;-1023 - 51\u0026lt;/sup\u0026gt; ( 2.225 * 10\u0026lt;sup\u0026gt;-308\u0026lt;/sup\u0026gt;).\u0026lt;/p\u0026gt;\n\n\u0026lt;hr\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Turning a double precision number to binary\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Many online converters exist to convert a double precision floating point number to binary (e.g. at \u0026lt;a href=\u0026quot;http://www.binaryconvert.com/convert_double.html\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;binaryconvert.com\u0026lt;/a\u0026gt;), but here is some sample C# code to obtain the IEEE 754 representation for a double precision number (I separate the three parts with colons (\u0026lt;code\u0026gt;:\u0026lt;/code\u0026gt;):\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;public static string BinaryRepresentation(double value)\n{\n    long valueInLongType = BitConverter.DoubleToInt64Bits(value);\n    string bits = Convert.ToString(valueInLongType, 2);\n    string leadingZeros = new string(\u0026apos;0\u0026apos;, 64 - bits.Length);\n    string binaryRepresentation = leadingZeros + bits;\n\n    string sign = binaryRepresentation[0].ToString();\n    string exponent = binaryRepresentation.Substring(1, 11);\n    string mantissa = binaryRepresentation.Substring(12);\n\n    return string.Format(\u0026quot;{0}:{1}:{2}\u0026quot;, sign, exponent, mantissa);\n}\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;hr\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;Getting to the point: the original question\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;(Skip to the bottom for the TL;DR version)\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;https://stackoverflow.com/users/62118/cato-johnston\u0026quot;\u0026gt;Cato Johnston\u0026lt;/a\u0026gt; (the question asker) asked why 0.1 + 0.2 != 0.3.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Written in binary (with colons separating the three parts), the IEEE 754 representations of the values are:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;0.1 =\u0026amp;gt; 0:01111111011:1001100110011001100110011001100110011001100110011010\n0.2 =\u0026amp;gt; 0:01111111100:1001100110011001100110011001100110011001100110011010\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;Note that the mantissa is composed of recurring digits of \u0026lt;code\u0026gt;0011\u0026lt;/code\u0026gt;. This is \u0026lt;strong\u0026gt;key\u0026lt;/strong\u0026gt; to why there is any error to the calculations - 0.1, 0.2 and 0.3 cannot be represented in binary \u0026lt;strong\u0026gt;precisely\u0026lt;/strong\u0026gt; in a \u0026lt;em\u0026gt;finite\u0026lt;/em\u0026gt; number of binary bits any more than 1/9, 1/3 or 1/7 can be represented precisely in \u0026lt;em\u0026gt;decimal digits\u0026lt;/em\u0026gt;.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Also note that we can decrease the power in the exponent by 52 and shift the point in the binary representation to the right by 52 places (much like 10\u0026lt;sup\u0026gt;-3\u0026lt;/sup\u0026gt; * 1.23 == 10\u0026lt;sup\u0026gt;-5\u0026lt;/sup\u0026gt; * 123). This then enables us to represent the binary representation as the exact value that it represents in the form a * 2\u0026lt;sup\u0026gt;p\u0026lt;/sup\u0026gt;. where \u0026apos;a\u0026apos; is an integer.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Converting the exponents to decimal, removing the offset, and re-adding the implied \u0026lt;code\u0026gt;1\u0026lt;/code\u0026gt; (in square brackets), 0.1 and 0.2 are:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;0.1 =\u0026amp;gt; 2^-4 * [1].1001100110011001100110011001100110011001100110011010\n0.2 =\u0026amp;gt; 2^-3 * [1].1001100110011001100110011001100110011001100110011010\nor\n0.1 =\u0026amp;gt; 2^-56 * 7205759403792794 = 0.1000000000000000055511151231257827021181583404541015625\n0.2 =\u0026amp;gt; 2^-55 * 7205759403792794 = 0.200000000000000011102230246251565404236316680908203125\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;To add two numbers, the exponent needs to be the same, i.e.:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;0.1 =\u0026amp;gt; 2^-3 *  0.1100110011001100110011001100110011001100110011001101(0)\n0.2 =\u0026amp;gt; 2^-3 *  1.1001100110011001100110011001100110011001100110011010\nsum =  2^-3 * 10.0110011001100110011001100110011001100110011001100111\nor\n0.1 =\u0026amp;gt; 2^-55 * 3602879701896397  = 0.1000000000000000055511151231257827021181583404541015625\n0.2 =\u0026amp;gt; 2^-55 * 7205759403792794  = 0.200000000000000011102230246251565404236316680908203125\nsum =  2^-55 * 10808639105689191 = 0.3000000000000000166533453693773481063544750213623046875\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;Since the sum is not of the form 2\u0026lt;sup\u0026gt;n\u0026lt;/sup\u0026gt; * 1.{bbb} we increase the exponent by one and shift the decimal (\u0026lt;em\u0026gt;binary\u0026lt;/em\u0026gt;) point to get:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;sum = 2^-2  * 1.0011001100110011001100110011001100110011001100110011(1)\n    = 2^-54 * 5404319552844595.5 = 0.3000000000000000166533453693773481063544750213623046875\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;There are now 53 bits in the mantissa (the 53rd is in square brackets in the line above). The default \u0026lt;a href=\u0026quot;https://en.wikipedia.org/wiki/IEEE_754-1985#Rounding_floating-point_numbers\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;rounding mode\u0026lt;/a\u0026gt; for IEEE 754 is \u0026apos;\u0026lt;em\u0026gt;Round to Nearest\u0026lt;/em\u0026gt;\u0026apos; - i.e. if a number \u0026lt;em\u0026gt;x\u0026lt;/em\u0026gt; falls between two values \u0026lt;em\u0026gt;a\u0026lt;/em\u0026gt; and \u0026lt;em\u0026gt;b\u0026lt;/em\u0026gt;, the value where the least significant bit is zero is chosen.\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;a = 2^-54 * 5404319552844595 = 0.299999999999999988897769753748434595763683319091796875\n  = 2^-2  * 1.0011001100110011001100110011001100110011001100110011\n\nx = 2^-2  * 1.0011001100110011001100110011001100110011001100110011(1)\n\nb = 2^-2  * 1.0011001100110011001100110011001100110011001100110100\n  = 2^-54 * 5404319552844596 = 0.3000000000000000444089209850062616169452667236328125\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;Note that \u0026lt;em\u0026gt;a\u0026lt;/em\u0026gt; and \u0026lt;em\u0026gt;b\u0026lt;/em\u0026gt; differ only in the last bit; \u0026lt;code\u0026gt;...0011\u0026lt;/code\u0026gt; + \u0026lt;code\u0026gt;1\u0026lt;/code\u0026gt; = \u0026lt;code\u0026gt;...0100\u0026lt;/code\u0026gt;. In this case, the value with the least significant bit of zero is \u0026lt;em\u0026gt;b\u0026lt;/em\u0026gt;, so the sum is:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;sum = 2^-2  * 1.0011001100110011001100110011001100110011001100110100\n    = 2^-54 * 5404319552844596 = 0.3000000000000000444089209850062616169452667236328125\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;whereas the binary representation of 0.3 is:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;0.3 =\u0026amp;gt; 2^-2  * 1.0011001100110011001100110011001100110011001100110011\n    =  2^-54 * 5404319552844595 = 0.299999999999999988897769753748434595763683319091796875\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;which only differs from the binary representation of the sum of 0.1 and 0.2 by 2\u0026lt;sup\u0026gt;-54\u0026lt;/sup\u0026gt;.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;The binary representation of 0.1 and 0.2 are the \u0026lt;em\u0026gt;most accurate\u0026lt;/em\u0026gt; representations of the numbers allowable by IEEE 754. The addition of these representation, due to the default rounding mode, results in a value which differs only in the least-significant-bit.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;TL;DR\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Writing \u0026lt;code\u0026gt;0.1 + 0.2\u0026lt;/code\u0026gt; in a IEEE 754 binary representation (with colons separating the three parts) and comparing it to \u0026lt;code\u0026gt;0.3\u0026lt;/code\u0026gt;, this is (I\u0026apos;ve put the distinct bits in square brackets):\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;0.1 + 0.2 =\u0026amp;gt; 0:01111111101:0011001100110011001100110011001100110011001100110[100]\n0.3       =\u0026amp;gt; 0:01111111101:0011001100110011001100110011001100110011001100110[011]\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;Converted back to decimal, these values are:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;0.1 + 0.2 =\u0026amp;gt; 0.300000000000000044408920985006...\n0.3       =\u0026amp;gt; 0.299999999999999988897769753748...\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;The difference is exactly 2\u0026lt;sup\u0026gt;-54\u0026lt;/sup\u0026gt;, which is ~5.5511151231258 × 10\u0026lt;sup\u0026gt;-17\u0026lt;/sup\u0026gt; - insignificant (for many applications) when compared to the original values.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Comparing the last few bits of a floating point number is inherently dangerous, as anyone who reads the famous \u0026quot;\u0026lt;a href=\u0026quot;http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;What Every Computer Scientist Should Know About Floating-Point Arithmetic\u0026lt;/a\u0026gt;\u0026quot; (which covers all the major parts of this answer) will know.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Most calculators use additional \u0026lt;a href=\u0026quot;https://en.wikipedia.org/wiki/Guard_digit\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;guard digits\u0026lt;/a\u0026gt; to get around this problem, which is how \u0026lt;code\u0026gt;0.1 + 0.2\u0026lt;/code\u0026gt; would give \u0026lt;code\u0026gt;0.3\u0026lt;/code\u0026gt;: the final few bits are rounded.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;In addition to the other correct answers, you may want to consider scaling your values to avoid problems with floating-point arithmetic. \u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;For example: \u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;var result = 1.0 + 2.0;     // result === 3.0 returns true\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;... instead of:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;var result = 0.1 + 0.2;     // result === 0.3 returns false\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;The expression \u0026lt;code\u0026gt;0.1 + 0.2 === 0.3\u0026lt;/code\u0026gt; returns \u0026lt;code\u0026gt;false\u0026lt;/code\u0026gt; in JavaScript, but fortunately integer arithmetic in floating-point is exact, so decimal representation errors can be avoided by scaling.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;As a practical example, to avoid floating-point problems where accuracy is paramount, it is recommended\u0026lt;sup\u0026gt;1\u0026lt;/sup\u0026gt; to handle money as an integer representing the number of cents: \u0026lt;code\u0026gt;2550\u0026lt;/code\u0026gt; cents instead of \u0026lt;code\u0026gt;25.50\u0026lt;/code\u0026gt; dollars. \u0026lt;/p\u0026gt;\n\n\u0026lt;hr\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;sup\u0026gt;1\u0026lt;/sup\u0026gt; Douglas Crockford: \u0026lt;a href=\u0026quot;http://books.google.com/books?id=PXa2bby0oQ0C\u0026amp;amp;pg=PA105\u0026amp;amp;dq=douglas+crockford+javascript+good+parts+dollar+values+can+be+converted\u0026amp;amp;rview=1\u0026amp;amp;cd=1#v=onepage\u0026amp;amp;q=\u0026amp;amp;f=false\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;\u0026lt;strong\u0026gt;JavaScript: The Good Parts\u0026lt;/strong\u0026gt;: Appendix A - Awful Parts (page 105)\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Floating point numbers stored in the computer consist of two parts, an integer and an exponent that the base is taken to and multiplied by the integer part.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;If the computer were working in base 10, \u0026lt;code\u0026gt;0.1\u0026lt;/code\u0026gt; would be \u0026lt;code\u0026gt;1 x 10¹\u0026lt;/code\u0026gt;, \u0026lt;code\u0026gt;0.2\u0026lt;/code\u0026gt; would be \u0026lt;code\u0026gt;2 x 10¹\u0026lt;/code\u0026gt;, and \u0026lt;code\u0026gt;0.3\u0026lt;/code\u0026gt; would be \u0026lt;code\u0026gt;3 x 10¹\u0026lt;/code\u0026gt;. Integer math is easy and exact, so adding \u0026lt;code\u0026gt;0.1 + 0.2\u0026lt;/code\u0026gt; will obviously result in \u0026lt;code\u0026gt;0.3\u0026lt;/code\u0026gt;.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Computers don\u0026apos;t usually work in base 10, they work in base 2. You can still get exact results for some values, for example \u0026lt;code\u0026gt;0.5\u0026lt;/code\u0026gt; is \u0026lt;code\u0026gt;1 x 2¹\u0026lt;/code\u0026gt; and \u0026lt;code\u0026gt;0.25\u0026lt;/code\u0026gt; is \u0026lt;code\u0026gt;1 x 2²\u0026lt;/code\u0026gt;, and adding them results in \u0026lt;code\u0026gt;3 x 2²\u0026lt;/code\u0026gt;, or \u0026lt;code\u0026gt;0.75\u0026lt;/code\u0026gt;. Exactly.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;The problem comes with numbers that can be represented exactly in base 10, but not in base 2. Those numbers need to be rounded to their closest equivalent. Assuming the very common IEEE 64-bit floating point format, the closest number to \u0026lt;code\u0026gt;0.1\u0026lt;/code\u0026gt; is \u0026lt;code\u0026gt;3602879701896397 x 2\u0026lt;/code\u0026gt;, and the closest number to \u0026lt;code\u0026gt;0.2\u0026lt;/code\u0026gt; is \u0026lt;code\u0026gt;7205759403792794 x 2\u0026lt;/code\u0026gt;; adding them together results in \u0026lt;code\u0026gt;10808639105689191 x 2\u0026lt;/code\u0026gt;, or an exact decimal value of \u0026lt;code\u0026gt;0.3000000000000000444089209850062616169452667236328125\u0026lt;/code\u0026gt;. Floating point numbers are generally rounded for display.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Floating point rounding error.  From \u0026lt;a href=\u0026quot;https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;What Every Computer Scientist Should Know About Floating-Point Arithmetic\u0026lt;/a\u0026gt;:\u0026lt;/p\u0026gt;\n\n\u0026lt;blockquote\u0026gt;\n  \u0026lt;p\u0026gt;Squeezing infinitely many real numbers into a finite number of bits requires an approximate representation. Although there are infinitely many integers, in most programs the result of integer computations can be stored in 32 bits. In contrast, given any fixed number of bits, most calculations with real numbers will produce quantities that cannot be exactly represented using that many bits. Therefore the result of a floating-point calculation must often be rounded in order to fit back into its finite representation. This rounding error is the characteristic feature of floating-point computation.\u0026lt;/p\u0026gt;\n\u0026lt;/blockquote\u0026gt;\n    ","\n\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;https://stackoverflow.com/questions/1089018/why-cant-decimal-numbers-be-represented-exactly-in-binary\u0026quot;\u0026gt;In short\u0026lt;/a\u0026gt; it\u0026apos;s because:\u0026lt;/p\u0026gt;\n\u0026lt;blockquote\u0026gt;\n\u0026lt;p\u0026gt;Floating point numbers cannot represent all decimals precisely in binary\u0026lt;/p\u0026gt;\n\u0026lt;/blockquote\u0026gt;\n\u0026lt;p\u0026gt;So just like 10/3 which \u0026lt;a href=\u0026quot;https://www.exploringbinary.com/why-0-point-1-does-not-exist-in-floating-point/\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;does not exist\u0026lt;/a\u0026gt; in base 10 precisely (it will be 3.33... recurring), in the same way 1/10 doesn\u0026apos;t exist in binary.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;So what? How to deal with it?\u0026lt;/strong\u0026gt; Is there any workaround?\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;In order to offer The \u0026lt;strong\u0026gt;best solution\u0026lt;/strong\u0026gt; I can say I discovered following method:\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;parseFloat((0.1 + 0.2).toFixed(10)) =\u0026amp;gt; Will return 0.3\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;Let me explain why it\u0026apos;s the best solution.\nAs others mentioned in above answers it\u0026apos;s a good idea to use ready to use Javascript toFixed() function to solve the problem. But most likely you\u0026apos;ll encounter with some problems.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Imagine you are going to add up two float numbers like \u0026lt;code\u0026gt;0.2\u0026lt;/code\u0026gt; and \u0026lt;code\u0026gt;0.7\u0026lt;/code\u0026gt; here it is: \u0026lt;code\u0026gt;0.2 + 0.7 = 0.8999999999999999\u0026lt;/code\u0026gt;.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Your expected result was \u0026lt;code\u0026gt;0.9\u0026lt;/code\u0026gt; it means you need a result with 1 digit precision in this case.\nSo you should have used \u0026lt;code\u0026gt;(0.2 + 0.7).tofixed(1)\u0026lt;/code\u0026gt;\nbut you can\u0026apos;t just give a certain parameter to toFixed() since it depends on the given number, for instance\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;0.22 + 0.7 = 0.9199999999999999\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;In this example you need 2 digits precision so it should be \u0026lt;code\u0026gt;toFixed(2)\u0026lt;/code\u0026gt;, so what should be the paramter to fit every given float number?\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;You might say let it be 10 in every situation then:\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;(0.2 + 0.7).toFixed(10) =\u0026amp;gt; Result will be \u0026quot;0.9000000000\u0026quot;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;Damn! What are you going to do with those unwanted zeros after 9?\nIt\u0026apos;s the time to convert it to float to make it as you desire:\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;parseFloat((0.2 + 0.7).toFixed(10)) =\u0026amp;gt; Result will be 0.9\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;Now that you found the solution, it\u0026apos;s better to offer it as a function like this:\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;function floatify(number){\n           return parseFloat((number).toFixed(10));\n        }\n    \n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;Let\u0026apos;s try it yourself:\n\u0026lt;/p\u0026gt;\u0026lt;div class=\u0026quot;snippet\u0026quot; data-lang=\u0026quot;js\u0026quot; data-hide=\u0026quot;false\u0026quot; data-console=\u0026quot;true\u0026quot; data-babel=\u0026quot;false\u0026quot;\u0026gt;\n\u0026lt;div class=\u0026quot;snippet-code\u0026quot;\u0026gt;\n\u0026lt;pre class=\u0026quot;snippet-code-js lang-js s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-javascript\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;function\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-title function_\u0026quot;\u0026gt;floatify\u0026lt;/span\u0026gt;(\u0026lt;span class=\u0026quot;hljs-params\u0026quot;\u0026gt;number\u0026lt;/span\u0026gt;){\n       \u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;return\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;parseFloat\u0026lt;/span\u0026gt;((number).\u0026lt;span class=\u0026quot;hljs-title function_\u0026quot;\u0026gt;toFixed\u0026lt;/span\u0026gt;(\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;10\u0026lt;/span\u0026gt;));\n    }\n \n\u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;function\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-title function_\u0026quot;\u0026gt;addUp\u0026lt;/span\u0026gt;(\u0026lt;span class=\u0026quot;hljs-params\u0026quot;\u0026gt;\u0026lt;/span\u0026gt;){\n  \u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;var\u0026lt;/span\u0026gt; number1 = +$(\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;#number1\u0026quot;\u0026lt;/span\u0026gt;).\u0026lt;span class=\u0026quot;hljs-title function_\u0026quot;\u0026gt;val\u0026lt;/span\u0026gt;();\n  \u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;var\u0026lt;/span\u0026gt; number2 = +$(\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;#number2\u0026quot;\u0026lt;/span\u0026gt;).\u0026lt;span class=\u0026quot;hljs-title function_\u0026quot;\u0026gt;val\u0026lt;/span\u0026gt;();\n  \u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;var\u0026lt;/span\u0026gt; unexpectedResult = number1 + number2;\n  \u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;var\u0026lt;/span\u0026gt; expectedResult = \u0026lt;span class=\u0026quot;hljs-title function_\u0026quot;\u0026gt;floatify\u0026lt;/span\u0026gt;(number1 + number2);\n  $(\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;#unexpectedResult\u0026quot;\u0026lt;/span\u0026gt;).\u0026lt;span class=\u0026quot;hljs-title function_\u0026quot;\u0026gt;text\u0026lt;/span\u0026gt;(unexpectedResult);\n  $(\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;#expectedResult\u0026quot;\u0026lt;/span\u0026gt;).\u0026lt;span class=\u0026quot;hljs-title function_\u0026quot;\u0026gt;text\u0026lt;/span\u0026gt;(expectedResult);\n}\n\u0026lt;span class=\u0026quot;hljs-title function_\u0026quot;\u0026gt;addUp\u0026lt;/span\u0026gt;();\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;pre class=\u0026quot;snippet-code-css lang-css s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-css\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-selector-tag\u0026quot;\u0026gt;input\u0026lt;/span\u0026gt;{\n  \u0026lt;span class=\u0026quot;hljs-attribute\u0026quot;\u0026gt;width\u0026lt;/span\u0026gt;: \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;50px\u0026lt;/span\u0026gt;;\n}\n\u0026lt;span class=\u0026quot;hljs-selector-id\u0026quot;\u0026gt;#expectedResult\u0026lt;/span\u0026gt;{\n\u0026lt;span class=\u0026quot;hljs-attribute\u0026quot;\u0026gt;color\u0026lt;/span\u0026gt;: green;\n}\n\u0026lt;span class=\u0026quot;hljs-selector-id\u0026quot;\u0026gt;#unexpectedResult\u0026lt;/span\u0026gt;{\n\u0026lt;span class=\u0026quot;hljs-attribute\u0026quot;\u0026gt;color\u0026lt;/span\u0026gt;: red;\n}\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;pre class=\u0026quot;snippet-code-html lang-html s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-xml\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-tag\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;span class=\u0026quot;hljs-name\u0026quot;\u0026gt;script\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-attr\u0026quot;\u0026gt;src\u0026lt;/span\u0026gt;=\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js\u0026quot;\u0026lt;/span\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-tag\u0026quot;\u0026gt;\u0026amp;lt;/\u0026lt;span class=\u0026quot;hljs-name\u0026quot;\u0026gt;script\u0026lt;/span\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt;\n\u0026lt;span class=\u0026quot;hljs-tag\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;span class=\u0026quot;hljs-name\u0026quot;\u0026gt;input\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-attr\u0026quot;\u0026gt;id\u0026lt;/span\u0026gt;=\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;number1\u0026quot;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-attr\u0026quot;\u0026gt;value\u0026lt;/span\u0026gt;=\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;0.2\u0026quot;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-attr\u0026quot;\u0026gt;onclick\u0026lt;/span\u0026gt;=\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;addUp()\u0026quot;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-attr\u0026quot;\u0026gt;onkeyup\u0026lt;/span\u0026gt;=\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;addUp()\u0026quot;\u0026lt;/span\u0026gt;/\u0026amp;gt;\u0026lt;/span\u0026gt; +\n\u0026lt;span class=\u0026quot;hljs-tag\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;span class=\u0026quot;hljs-name\u0026quot;\u0026gt;input\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-attr\u0026quot;\u0026gt;id\u0026lt;/span\u0026gt;=\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;number2\u0026quot;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-attr\u0026quot;\u0026gt;value\u0026lt;/span\u0026gt;=\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;0.7\u0026quot;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-attr\u0026quot;\u0026gt;onclick\u0026lt;/span\u0026gt;=\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;addUp()\u0026quot;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-attr\u0026quot;\u0026gt;onkeyup\u0026lt;/span\u0026gt;=\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;addUp()\u0026quot;\u0026lt;/span\u0026gt;/\u0026amp;gt;\u0026lt;/span\u0026gt; =\n\u0026lt;span class=\u0026quot;hljs-tag\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;span class=\u0026quot;hljs-name\u0026quot;\u0026gt;p\u0026lt;/span\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt;Expected Result: \u0026lt;span class=\u0026quot;hljs-tag\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;span class=\u0026quot;hljs-name\u0026quot;\u0026gt;span\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-attr\u0026quot;\u0026gt;id\u0026lt;/span\u0026gt;=\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;expectedResult\u0026quot;\u0026lt;/span\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-tag\u0026quot;\u0026gt;\u0026amp;lt;/\u0026lt;span class=\u0026quot;hljs-name\u0026quot;\u0026gt;span\u0026lt;/span\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-tag\u0026quot;\u0026gt;\u0026amp;lt;/\u0026lt;span class=\u0026quot;hljs-name\u0026quot;\u0026gt;p\u0026lt;/span\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt;\n\u0026lt;span class=\u0026quot;hljs-tag\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;span class=\u0026quot;hljs-name\u0026quot;\u0026gt;p\u0026lt;/span\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt;Unexpected Result: \u0026lt;span class=\u0026quot;hljs-tag\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;span class=\u0026quot;hljs-name\u0026quot;\u0026gt;span\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-attr\u0026quot;\u0026gt;id\u0026lt;/span\u0026gt;=\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;unexpectedResult\u0026quot;\u0026lt;/span\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-tag\u0026quot;\u0026gt;\u0026amp;lt;/\u0026lt;span class=\u0026quot;hljs-name\u0026quot;\u0026gt;span\u0026lt;/span\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-tag\u0026quot;\u0026gt;\u0026amp;lt;/\u0026lt;span class=\u0026quot;hljs-name\u0026quot;\u0026gt;p\u0026lt;/span\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt;\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;div class=\u0026quot;snippet-result\u0026quot;\u0026gt;\u0026lt;div class=\u0026quot;snippet-ctas\u0026quot;\u0026gt;\u0026lt;button type=\u0026quot;button\u0026quot; class=\u0026quot;s-btn s-btn__primary\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;icon-play-white _hover\u0026quot;\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span\u0026gt; Run code snippet\u0026lt;/span\u0026gt;\u0026lt;/button\u0026gt;\u0026lt;input class=\u0026quot;copySnippet s-btn s-btn__filled\u0026quot; type=\u0026quot;button\u0026quot; value=\u0026quot;Copy snippet to answer\u0026quot; style=\u0026quot;display: none;\u0026quot;\u0026gt;\u0026lt;button type=\u0026quot;button\u0026quot; class=\u0026quot;s-btn hideResults\u0026quot; style=\u0026quot;display: none;\u0026quot;\u0026gt;Hide results\u0026lt;/button\u0026gt;\u0026lt;div class=\u0026quot;popout-code\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;snippet-expand-link\u0026quot;\u0026gt;Expand snippet\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;div class=\u0026quot;snippet-result-code\u0026quot; style=\u0026quot;display: none;\u0026quot;\u0026gt;\u0026lt;iframe name=\u0026quot;sif1\u0026quot; sandbox=\u0026quot;allow-forms allow-modals allow-scripts\u0026quot; class=\u0026quot;snippet-box-edit snippet-box-result\u0026quot; frameborder=\u0026quot;0\u0026quot;\u0026gt;\u0026lt;/iframe\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/div\u0026gt;\n\u0026lt;/div\u0026gt;\n\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;You can use it this way:\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;var x = 0.2 + 0.7;\nfloatify(x);  =\u0026amp;gt; Result: 0.9\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;As \u0026lt;a href=\u0026quot;https://www.w3schools.com/js/js_numbers.asp\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;W3SCHOOLS\u0026lt;/a\u0026gt; suggests there is another solution too, you can multiply and divide to solve the problem above:\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;var x = (0.2 * 10 + 0.1 * 10) / 10;       // x will be 0.3\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;Keep in mind that \u0026lt;code\u0026gt;(0.2 + 0.1) * 10 / 10\u0026lt;/code\u0026gt; won\u0026apos;t work at all although it seems the same!\nI prefer the first solution since I can apply it as a function which converts the input float to accurate output float.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;My workaround:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;function add(a, b, precision) {\n    var x = Math.pow(10, precision || 2);\n    return (Math.round(a * x) + Math.round(b * x)) / x;\n}\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;precision\u0026lt;/em\u0026gt; refers to the number of digits you want to preserve after the decimal point during addition.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;h3\u0026gt;No, not broken, but most decimal fractions must be approximated\u0026lt;/h3\u0026gt;\n\n\u0026lt;blockquote\u0026gt;\n  \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Summary\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;/blockquote\u0026gt;\n\n\u0026lt;p\u0026gt;Floating point arithmetic \u0026lt;em\u0026gt;is\u0026lt;/em\u0026gt; exact, unfortunately, it doesn\u0026apos;t match up well with our usual base-10 number representation, so it turns out we are often giving it input that is slightly off from what we wrote.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Even simple numbers like 0.01, 0.02, 0.03, 0.04 ... 0.24 are not representable exactly as binary fractions. If you count up 0.01, .02, .03 ..., not until you get to 0.25 will you get the first fraction representable in base\u0026lt;sub\u0026gt;2\u0026lt;/sub\u0026gt;.  If you tried that using FP, your 0.01 would have been slightly off, so the only way to add 25 of them up to a nice exact 0.25 would have required a long chain of causality involving guard bits and rounding. It\u0026apos;s hard to predict so we throw up our hands and say \u0026lt;em\u0026gt;\u0026quot;FP is inexact\u0026quot;,\u0026lt;/em\u0026gt; but that\u0026apos;s not really true. \u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;We constantly give the FP hardware something that seems simple in base 10 but is a repeating fraction in base 2.\u0026lt;/p\u0026gt;\n\n\u0026lt;blockquote\u0026gt;\n  \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;How did this happen?\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;/blockquote\u0026gt;\n\n\u0026lt;p\u0026gt;When we write in decimal, every fraction (specifically, every \u0026lt;em\u0026gt;terminating decimal)\u0026lt;/em\u0026gt; is a rational number of the form \u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\n\u0026lt;em\u0026gt;a / (2\u0026lt;sup\u0026gt;n\u0026lt;/sup\u0026gt; x 5\u0026lt;sup\u0026gt;m\u0026lt;/sup\u0026gt;)\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;In binary, we only get the \u0026lt;em\u0026gt;2\u0026lt;sup\u0026gt;n\u0026lt;/sup\u0026gt;\u0026lt;/em\u0026gt; term, that is:\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp; \u0026lt;em\u0026gt;a / 2\u0026lt;sup\u0026gt;n\u0026lt;/sup\u0026gt;\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;So in decimal, we can\u0026apos;t represent \u0026lt;sup\u0026gt;1\u0026lt;/sup\u0026gt;/\u0026lt;sub\u0026gt;3\u0026lt;/sub\u0026gt;. Because base 10 includes 2 as a prime factor, every number we can write as a binary fraction \u0026lt;em\u0026gt;also\u0026lt;/em\u0026gt; can be written as a base 10 fraction. However, hardly anything we write as a base\u0026lt;sub\u0026gt;10\u0026lt;/sub\u0026gt; fraction is representable in binary. In the range from 0.01, 0.02, 0.03 ... 0.99, only \u0026lt;em\u0026gt;three\u0026lt;/em\u0026gt; numbers can be represented in our FP format: 0.25, 0.50, and 0.75, because they are 1/4, 1/2, and 3/4, all numbers with a prime factor using only the 2\u0026lt;sup\u0026gt;n\u0026lt;/sup\u0026gt; term.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;In base\u0026lt;sub\u0026gt;10\u0026lt;/sub\u0026gt; we can\u0026apos;t represent \u0026lt;sup\u0026gt;1\u0026lt;/sup\u0026gt;/\u0026lt;sub\u0026gt;3\u0026lt;/sub\u0026gt;. But in binary, we can\u0026apos;t do \u0026lt;sup\u0026gt;1\u0026lt;/sup\u0026gt;/\u0026lt;sub\u0026gt;10\u0026lt;/sub\u0026gt; \u0026lt;em\u0026gt;or\u0026lt;/em\u0026gt; \u0026lt;sup\u0026gt;1\u0026lt;/sup\u0026gt;/\u0026lt;sub\u0026gt;3\u0026lt;/sub\u0026gt;.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;So while every binary fraction can be written in decimal, the reverse is not true. And in fact most decimal fractions repeat in binary.\u0026lt;/p\u0026gt;\n\n\u0026lt;blockquote\u0026gt;\n  \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Dealing with it\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt;\n\u0026lt;/blockquote\u0026gt;\n\n\u0026lt;p\u0026gt;Developers are usually instructed to do \u0026lt;em\u0026gt;\u0026amp;lt; epsilon\u0026lt;/em\u0026gt; comparisons, better advice might be to round to integral values (in the C library: round() and roundf(), i.e., stay in the FP format) and then compare. Rounding to a specific decimal fraction length solves most problems with output.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Also, on real number-crunching problems (the problems that FP was invented for on early, frightfully expensive computers) the physical constants of the universe and all other measurements are only known to a relatively small number of significant figures, so the entire problem space was \u0026quot;inexact\u0026quot; anyway. FP \u0026quot;accuracy\u0026quot; isn\u0026apos;t a problem in this kind of application.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;The whole issue really arises when people try to use FP for bean counting. It does work for that, but only if you stick to integral values, which kind of defeats the point of using it. \u0026lt;em\u0026gt;This is why we have all those decimal fraction software libraries.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;I love the Pizza answer by \u0026lt;a href=\u0026quot;https://stackoverflow.com/users/13/chris-jester-young\u0026quot;\u0026gt;Chris\u0026lt;/a\u0026gt;, because it describes the actual problem, not just the usual handwaving about \u0026quot;inaccuracy\u0026quot;. If FP were simply \u0026quot;inaccurate\u0026quot;, we could \u0026lt;em\u0026gt;fix\u0026lt;/em\u0026gt; that and would have done it decades ago. The reason we haven\u0026apos;t is because the FP format is compact and fast and it\u0026apos;s the best way to crunch a lot of numbers. Also, it\u0026apos;s a legacy from the space age and arms race and early attempts to solve big problems with very slow computers using small memory systems. (Sometimes, individual \u0026lt;em\u0026gt;magnetic cores\u0026lt;/em\u0026gt; for 1-bit storage, but that\u0026apos;s \u0026lt;a href=\u0026quot;https://en.wikipedia.org/wiki/Magnetic-core_memory\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;another story.\u0026lt;/a\u0026gt;)\u0026lt;/p\u0026gt;\n\n\u0026lt;blockquote\u0026gt;\n  \u0026lt;p\u0026gt;Conclusion\u0026lt;/p\u0026gt;\n\u0026lt;/blockquote\u0026gt;\n\n\u0026lt;p\u0026gt;If you are just counting beans at a bank, software solutions that use decimal string representations in the first place work perfectly well. But you can\u0026apos;t do quantum chromodynamics or aerodynamics that way.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;A lot of good answers have been posted, but I\u0026apos;d like to append one more.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Not all numbers can be represented via \u0026lt;strong\u0026gt;floats\u0026lt;/strong\u0026gt;/\u0026lt;strong\u0026gt;doubles\u0026lt;/strong\u0026gt;\nFor example, the number \u0026quot;0.2\u0026quot; will be represented as \u0026quot;0.200000003\u0026quot; in single precision in IEEE754 float point standard.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Model for store real numbers under the hood represent float numbers as\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;https://i.stack.imgur.com/BRvSp.png\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;\u0026lt;img src=\u0026quot;https://i.stack.imgur.com/BRvSp.png\u0026quot; alt=\u0026quot;enter image description here\u0026quot;\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Even though you can type \u0026lt;code\u0026gt;0.2\u0026lt;/code\u0026gt; easily, \u0026lt;code\u0026gt;FLT_RADIX\u0026lt;/code\u0026gt; and \u0026lt;code\u0026gt;DBL_RADIX\u0026lt;/code\u0026gt; is 2; not 10 for a computer with FPU which uses \u0026quot;IEEE Standard for Binary Floating-Point Arithmetic (ISO/IEEE Std 754-1985)\u0026quot;.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;So it is a bit hard to represent such numbers exactly. Even if you specify this variable explicitly without any intermediate calculation.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Some statistics related to this famous double precision question.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;When adding all values (\u0026lt;em\u0026gt;a + b\u0026lt;/em\u0026gt;) using a step of 0.1 (from 0.1 to 100) we have \u0026lt;strong\u0026gt;~15% chance of precision error\u0026lt;/strong\u0026gt;. Note that the error could result in slightly bigger or smaller values.\nHere are some examples:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;0.1 + 0.2 = 0.30000000000000004 (BIGGER)\n0.1 + 0.7 = 0.7999999999999999 (SMALLER)\n...\n1.7 + 1.9 = 3.5999999999999996 (SMALLER)\n1.7 + 2.2 = 3.9000000000000004 (BIGGER)\n...\n3.2 + 3.6 = 6.800000000000001 (BIGGER)\n3.2 + 4.4 = 7.6000000000000005 (BIGGER)\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;When subtracting all values (\u0026lt;em\u0026gt;a - b\u0026lt;/em\u0026gt; where \u0026lt;em\u0026gt;a \u0026amp;gt; b\u0026lt;/em\u0026gt;) using a step of 0.1 (from 100 to 0.1) we have \u0026lt;strong\u0026gt;~34% chance of precision error\u0026lt;/strong\u0026gt;.\nHere are some examples:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;0.6 - 0.2 = 0.39999999999999997 (SMALLER)\n0.5 - 0.4 = 0.09999999999999998 (SMALLER)\n...\n2.1 - 0.2 = 1.9000000000000001 (BIGGER)\n2.0 - 1.9 = 0.10000000000000009 (BIGGER)\n...\n100 - 99.9 = 0.09999999999999432 (SMALLER)\n100 - 99.8 = 0.20000000000000284 (BIGGER)\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;*15% and 34% are indeed huge, so always use BigDecimal when precision is of big importance. With 2 decimal digits (step 0.01) the situation worsens a bit more (18% and 36%).\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Given that nobody has mentioned this...\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Some high level languages such as Python and Java come with tools to overcome binary floating point limitations. For example:\u0026lt;/p\u0026gt;\n\n\u0026lt;ul\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Python\u0026apos;s \u0026lt;a href=\u0026quot;https://docs.python.org/dev/library/decimal.html\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;\u0026lt;code\u0026gt;decimal\u0026lt;/code\u0026gt; module\u0026lt;/a\u0026gt; and Java\u0026apos;s \u0026lt;a href=\u0026quot;http://docs.oracle.com/javase/8/docs/api/java/math/BigDecimal.html\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;\u0026lt;code\u0026gt;BigDecimal\u0026lt;/code\u0026gt; class\u0026lt;/a\u0026gt;, that represent numbers internally with decimal notation (as opposed to binary notation). Both have limited precision, so they are still error prone, however they solve most common problems with binary floating point arithmetic.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Decimals are very nice when dealing with money: ten cents plus twenty cents are always exactly thirty cents:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt; 0.1 + 0.2 == 0.3\nFalse\n\u0026amp;gt;\u0026amp;gt;\u0026amp;gt; Decimal(\u0026apos;0.1\u0026apos;) + Decimal(\u0026apos;0.2\u0026apos;) == Decimal(\u0026apos;0.3\u0026apos;)\nTrue\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;Python\u0026apos;s \u0026lt;code\u0026gt;decimal\u0026lt;/code\u0026gt; module is based on \u0026lt;a href=\u0026quot;https://en.wikipedia.org/wiki/IEEE_854-1987\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;IEEE standard 854-1987\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Python\u0026apos;s \u0026lt;a href=\u0026quot;https://docs.python.org/dev/library/fractions.html\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;\u0026lt;code\u0026gt;fractions\u0026lt;/code\u0026gt; module\u0026lt;/a\u0026gt; and Apache Common\u0026apos;s \u0026lt;a href=\u0026quot;https://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math3/fraction/BigFraction.html\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;\u0026lt;code\u0026gt;BigFraction\u0026lt;/code\u0026gt; class\u0026lt;/a\u0026gt;. Both represent rational numbers as \u0026lt;code\u0026gt;(numerator, denominator)\u0026lt;/code\u0026gt; pairs and they may give more accurate results than decimal floating point arithmetic.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;/ul\u0026gt;\n\n\u0026lt;p\u0026gt;Neither of these solutions is perfect (especially if we look at performances, or if we require a very high precision), but still they solve a great number of problems with binary floating point arithmetic.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Did you try the duct tape solution?\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Try to determine when errors occur and fix them with short if statements, it\u0026apos;s not pretty but for some problems it is the only solution and this is one of them.\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt; if( (n * 0.1) \u0026amp;lt; 100.0 ) { return n * 0.1 - 0.000000000000001 ;}\n                    else { return n * 0.1 + 0.000000000000001 ;}    \n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;I had the same problem in a scientific simulation project in c#, and I can tell you that if you ignore the butterfly effect it\u0026apos;s gonna turn to a big fat dragon and bite you in the a**\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Those weird numbers appear because computers use binary(base 2) number system for calculation purposes, while we use decimal(base 10).\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;There are a majority of fractional numbers that cannot be represented precisely either in binary or in decimal or both. Result - A rounded up (but precise) number results.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Many of this question\u0026apos;s numerous duplicates ask about the effects of floating point rounding on specific numbers. In practice, it is easier to get a feeling for how it works by looking at exact results of calculations of interest rather than by just reading about it. Some languages provide ways of doing that - such as converting a \u0026lt;code\u0026gt;float\u0026lt;/code\u0026gt; or \u0026lt;code\u0026gt;double\u0026lt;/code\u0026gt; to \u0026lt;code\u0026gt;BigDecimal\u0026lt;/code\u0026gt; in Java.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Since this is a language-agnostic question, it needs language-agnostic tools, such as a \u0026lt;a href=\u0026quot;http://www.exploringbinary.com/floating-point-converter/\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;Decimal to Floating-Point Converter\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Applying it to the numbers in the question, treated as doubles: \u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;0.1 converts to 0.1000000000000000055511151231257827021181583404541015625, \u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;0.2 converts to 0.200000000000000011102230246251565404236316680908203125, \u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;0.3 converts to 0.299999999999999988897769753748434595763683319091796875, and \u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;0.30000000000000004 converts to 0.3000000000000000444089209850062616169452667236328125.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Adding the first two numbers manually or in a decimal calculator such as \u0026lt;a href=\u0026quot;https://www.mathsisfun.com/calculator-precision.html\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;Full Precision Calculator\u0026lt;/a\u0026gt;, shows the exact sum of the actual inputs is 0.3000000000000000166533453693773481063544750213623046875. \u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;If it were rounded down to the equivalent of 0.3 the rounding error would be 0.0000000000000000277555756156289135105907917022705078125. Rounding up to the equivalent of 0.30000000000000004 also gives rounding error 0.0000000000000000277555756156289135105907917022705078125. The round-to-even tie breaker applies.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Returning to the floating point converter, the raw hexadecimal for 0.30000000000000004 is 3fd3333333333334, which ends in an even digit and therefore is the correct result.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Can I just add; people always assume this to be a computer problem, but if you count with your hands (base 10), you can\u0026apos;t get \u0026lt;code\u0026gt;(1/3+1/3=2/3)=true\u0026lt;/code\u0026gt; unless you have infinity to add 0.333... to 0.333... so just as with the \u0026lt;code\u0026gt;(1/10+2/10)!==3/10\u0026lt;/code\u0026gt; problem in base 2, you truncate it to 0.333 + 0.333 = 0.666 and probably round it to 0.667 which would be also be technically inaccurate.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Count in ternary, and thirds are not a problem though - maybe some race with 15 fingers on each hand would ask why your decimal math was broken...\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;The kind of floating-point math that can be implemented in a digital computer necessarily uses an approximation of the real numbers and operations on them. (The \u0026lt;em\u0026gt;standard\u0026lt;/em\u0026gt; version runs to over fifty pages of documentation and has a committee to deal with its errata and further refinement.)\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;This approximation is a mixture of approximations of different kinds, each of which can either be ignored or carefully accounted for due to its specific manner of deviation from exactitude. It also involves a number of explicit exceptional cases at both the hardware and software levels that most people walk right past while pretending not to notice.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;If you need infinite precision (using the number π, for example, instead of one of its many shorter stand-ins), you should write or use a symbolic math program instead.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;But if you\u0026apos;re okay with the idea that sometimes floating-point math is fuzzy in value and logic and errors can accumulate quickly, and you can write your requirements and tests to allow for that, then your code can frequently get by with what\u0026apos;s in your FPU.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Just for fun, I played with the representation of floats, following the definitions from the Standard C99 and I wrote the code below.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;The code prints the binary representation of floats in 3 separated groups\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;SIGN EXPONENT FRACTION\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;and after that it prints a sum, that, when summed with enough precision, it will show the value that really exists in hardware.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;So when you write \u0026lt;code\u0026gt;float x = 999...\u0026lt;/code\u0026gt;, the compiler will transform that number in a bit representation printed by the function \u0026lt;code\u0026gt;xx\u0026lt;/code\u0026gt; such that the sum printed by the function \u0026lt;code\u0026gt;yy\u0026lt;/code\u0026gt; be equal to the given number.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;In reality, this sum is only an approximation.  For the number 999,999,999  the compiler will insert in bit representation of the float the number 1,000,000,000\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;After the code I attach a console session, in which I compute the sum of terms for both constants (minus PI and 999999999) that really exists in hardware, inserted there by the compiler.\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;#include \u0026amp;lt;stdio.h\u0026amp;gt;\n#include \u0026amp;lt;limits.h\u0026amp;gt;\n\nvoid\nxx(float *x)\n{\n    unsigned char i = sizeof(*x)*CHAR_BIT-1;\n    do {\n        switch (i) {\n        case 31:\n             printf(\u0026quot;sign:\u0026quot;);\n             break;\n        case 30:\n             printf(\u0026quot;exponent:\u0026quot;);\n             break;\n        case 23:\n             printf(\u0026quot;fraction:\u0026quot;);\n             break;\n\n        }\n        char b=(*(unsigned long long*)x\u0026amp;amp;((unsigned long long)1\u0026amp;lt;\u0026amp;lt;i))!=0;\n        printf(\u0026quot;%d \u0026quot;, b);\n    } while (i--);\n    printf(\u0026quot;\\n\u0026quot;);\n}\n\nvoid\nyy(float a)\n{\n    int sign=!(*(unsigned long long*)\u0026amp;amp;a\u0026amp;amp;((unsigned long long)1\u0026amp;lt;\u0026amp;lt;31));\n    int fraction = ((1\u0026amp;lt;\u0026amp;lt;23)-1)\u0026amp;amp;(*(int*)\u0026amp;amp;a);\n    int exponent = (255\u0026amp;amp;((*(int*)\u0026amp;amp;a)\u0026amp;gt;\u0026amp;gt;23))-127;\n\n    printf(sign?\u0026quot;positive\u0026quot; \u0026quot; ( 1+\u0026quot;:\u0026quot;negative\u0026quot; \u0026quot; ( 1+\u0026quot;);\n    unsigned int i = 1\u0026amp;lt;\u0026amp;lt;22;\n    unsigned int j = 1;\n    do {\n        char b=(fraction\u0026amp;amp;i)!=0;\n        b\u0026amp;amp;\u0026amp;amp;(printf(\u0026quot;1/(%d) %c\u0026quot;, 1\u0026amp;lt;\u0026amp;lt;j, (fraction\u0026amp;amp;(i-1))?\u0026apos;+\u0026apos;:\u0026apos;)\u0026apos; ), 0);\n    } while (j++, i\u0026amp;gt;\u0026amp;gt;=1);\n\n    printf(\u0026quot;*2^%d\u0026quot;, exponent);\n    printf(\u0026quot;\\n\u0026quot;);\n}\n\nvoid\nmain()\n{\n    float x=-3.14;\n    float y=999999999;\n    printf(\u0026quot;%lu\\n\u0026quot;, sizeof(x));\n    xx(\u0026amp;amp;x);\n    xx(\u0026amp;amp;y);\n    yy(x);\n    yy(y);\n}\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;hr\u0026gt;\n\n\u0026lt;p\u0026gt;Here is a console session in which I compute the real value of the float that exists in hardware.  I used \u0026lt;code\u0026gt;bc\u0026lt;/code\u0026gt; to print the sum of terms outputted by the main program.  One can insert that sum in python \u0026lt;code\u0026gt;repl\u0026lt;/code\u0026gt; or something similar also.\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;-- .../terra1/stub\n@ qemacs f.c\n-- .../terra1/stub\n@ gcc f.c\n-- .../terra1/stub\n@ ./a.out\nsign:1 exponent:1 0 0 0 0 0 0 fraction:0 1 0 0 1 0 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 1\nsign:0 exponent:1 0 0 1 1 1 0 fraction:0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 0\nnegative ( 1+1/(2) +1/(16) +1/(256) +1/(512) +1/(1024) +1/(2048) +1/(8192) +1/(32768) +1/(65536) +1/(131072) +1/(4194304) +1/(8388608) )*2^1\npositive ( 1+1/(2) +1/(4) +1/(16) +1/(32) +1/(64) +1/(512) +1/(1024) +1/(4096) +1/(16384) +1/(32768) +1/(262144) +1/(1048576) )*2^29\n-- .../terra1/stub\n@ bc\nscale=15\n( 1+1/(2) +1/(4) +1/(16) +1/(32) +1/(64) +1/(512) +1/(1024) +1/(4096) +1/(16384) +1/(32768) +1/(262144) +1/(1048576) )*2^29\n999999999.999999446351872\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;That\u0026apos;s it.  The value of 999999999 is in fact\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;999999999.999999446351872\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;You can also check with \u0026lt;code\u0026gt;bc\u0026lt;/code\u0026gt; that -3.14 is also perturbed.  Do not forget to set a \u0026lt;code\u0026gt;scale\u0026lt;/code\u0026gt; factor in \u0026lt;code\u0026gt;bc\u0026lt;/code\u0026gt;.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;The displayed sum is what inside the hardware.  The value you obtain by computing it depends on the scale you set.  I did set the \u0026lt;code\u0026gt;scale\u0026lt;/code\u0026gt; factor to 15.  Mathematically, with infinite precision, it seems it is 1,000,000,000.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;https://www.python.org/dev/peps/pep-0485/\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;Since Python 3.5\u0026lt;/a\u0026gt; you can use \u0026lt;a href=\u0026quot;https://docs.python.org/3/library/math.html#math.isclose\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;\u0026lt;code\u0026gt;math.isclose()\u0026lt;/code\u0026gt;\u0026lt;/a\u0026gt; function for testing approximate equality:\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt; import math\n\u0026amp;gt;\u0026amp;gt;\u0026amp;gt; math.isclose(0.1 + 0.2, 0.3)\nTrue\n\u0026amp;gt;\u0026amp;gt;\u0026amp;gt; 0.1 + 0.2 == 0.3\nFalse\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n    ","\n\u0026lt;p\u0026gt;The trap with floating point numbers is that they look like decimal but they work in binary.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;The only prime factor of 2 is 2, while 10 has prime factors of 2 and 5. The result of this is that every number that can be written exactly as a binary fraction can also be written exactly as a decimal fraction but only a subset of numbers that can be written as decimal fractions can be written as binary fractions.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;A floating point number is essentially a binary fraction with a limited number of significant digits. If you go past those significant digits then the results will be rounded.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;When you type a literal in your code or call the function to parse a floating point number to a string, it expects a decimal number and it stores a binary approximation of that decimal number in the variable.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;When you print a floating point number or call the function to convert one to a string it prints a decimal approximation of the floating point number. It \u0026lt;em\u0026gt;is\u0026lt;/em\u0026gt; possible to convert a binary number to decimal exactly, but no language I\u0026apos;m aware of does that by default when converting to a string*. Some languages use a fixed number of significant digits, others use the shortest string that will \u0026quot;round trip\u0026quot; back to the same floating point value.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;* Python \u0026lt;em\u0026gt;does\u0026lt;/em\u0026gt; convert exactly when converting a floating point number to a \u0026quot;decimal.Decimal\u0026quot;. This is the easiest way I know of to obtain the exact decimal equivalent of a floating point number.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Another way to look at this: Used are 64 bits to represent numbers. As consequence there is no way more than 2**64 = 18,446,744,073,709,551,616 different numbers can be precisely represented. \u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;However, Math says there are already infinitely many decimals between 0 and 1. IEE 754 defines an encoding to use these 64 bits efficiently for a much larger number space plus NaN and +/- Infinity, so there are gaps between accurately represented numbers filled with numbers only approximated. \u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Unfortunately 0.3 sits in a gap.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Floating point numbers are represented, at the hardware level, as fractions of binary numbers (base 2). For example, the decimal fraction:\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;0.125\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;has the value 1/10 + 2/100 + 5/1000 and, in the same way, the binary fraction:\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;0.001\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;has the value 0/2 + 0/4 + 1/8. These two fractions have the same value, the only difference is that the first is a decimal fraction, the second is a binary fraction.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Unfortunately, most decimal fractions cannot have exact representation in binary fractions. Therefore, in general, the floating point numbers you give are only approximated to binary fractions to be stored in the machine.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;The problem is easier to approach in base 10. Take for example, the fraction 1/3. You can approximate it to a decimal fraction:\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;0.3\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;or better,\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;0.33\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;or better,\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;0.333\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;etc. No matter how many decimal places you write, the result is never exactly 1/3, but it is an estimate that always comes closer.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Likewise, no matter how many base 2 decimal places you use, the decimal value 0.1 cannot be represented exactly as a binary fraction. In base 2, 1/10 is the following periodic number:\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;0.0001100110011001100110011001100110011001100110011 ...\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;Stop at any finite amount of bits, and you\u0026apos;ll get an approximation.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;For Python, on a typical machine, 53 bits are used for the precision of a float, so the value stored when you enter the decimal 0.1 is the binary fraction.\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;0.00011001100110011001100110011001100110011001100110011010\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;which is close, but not exactly equal, to 1/10.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;It\u0026apos;s easy to forget that the stored value is an approximation of the original decimal fraction, due to the way floats are displayed in the interpreter. Python only displays a decimal approximation of the value stored in binary. If Python were to output the true decimal value of the binary approximation stored for 0.1, it would output:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-py s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-python\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-meta\u0026quot;\u0026gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt; \u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.1\u0026lt;/span\u0026gt;\n\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.1000000000000000055511151231257827021181583404541015625\u0026lt;/span\u0026gt;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;This is a lot more decimal places than most people would expect, so Python displays a rounded value to improve readability:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-py s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-python\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-meta\u0026quot;\u0026gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt; \u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.1\u0026lt;/span\u0026gt;\n\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.1\u0026lt;/span\u0026gt;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;It is important to understand that in reality this is an illusion: the stored value is not exactly 1/10, it is simply on the display that the stored value is rounded. This becomes evident as soon as you perform arithmetic operations with these values:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-py s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-python\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-meta\u0026quot;\u0026gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt; \u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.1\u0026lt;/span\u0026gt; + \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.2\u0026lt;/span\u0026gt;\n\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.30000000000000004\u0026lt;/span\u0026gt;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;This behavior is inherent to the very nature of the machine\u0026apos;s floating-point representation: it is not a bug in Python, nor is it a bug in your code. You can observe the same type of behavior in all other languages that use hardware support for calculating floating point numbers (although some languages do not make the difference visible by default, or not in all display modes).\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Another surprise is inherent in this one. For example, if you try to round the value 2.675 to two decimal places, you will get\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-py s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-python\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-meta\u0026quot;\u0026gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt; \u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;round\u0026lt;/span\u0026gt; (\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2.675\u0026lt;/span\u0026gt;, \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt;)\n\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2.67\u0026lt;/span\u0026gt;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;The documentation for the round() primitive indicates that it rounds to the nearest value away from zero. Since the decimal fraction is exactly halfway between 2.67 and 2.68, you should expect to get (a binary approximation of) 2.68. This is not the case, however, because when the decimal fraction 2.675 is converted to a float, it is stored by an approximation whose exact value is :\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;2.67499999999999982236431605997495353221893310546875\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;Since the approximation is slightly closer to 2.67 than 2.68, the rounding is down.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;If you are in a situation where rounding decimal numbers halfway down matters, you should use the decimal module. By the way, the decimal module also provides a convenient way to \u0026quot;see\u0026quot; the exact value stored for any float.\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-py s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-python\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-meta\u0026quot;\u0026gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt; \u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;from\u0026lt;/span\u0026gt; decimal \u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;import\u0026lt;/span\u0026gt; Decimal\n\u0026lt;span class=\u0026quot;hljs-meta\u0026quot;\u0026gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt; \u0026lt;/span\u0026gt;Decimal (\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2.675\u0026lt;/span\u0026gt;)\n\u0026lt;span class=\u0026quot;hljs-meta\u0026quot;\u0026gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt; \u0026lt;/span\u0026gt;Decimal (\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026apos;2.67499999999999982236431605997495353221893310546875\u0026apos;\u0026lt;/span\u0026gt;)\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;Another consequence of the fact that 0.1 is not exactly stored in 1/10 is that the sum of ten values of 0.1 does not give 1.0 either:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-py s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-python\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-meta\u0026quot;\u0026gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt; \u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;sum\u0026lt;/span\u0026gt; = \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.0\u0026lt;/span\u0026gt;\n\u0026lt;span class=\u0026quot;hljs-meta\u0026quot;\u0026gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt; \u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;for\u0026lt;/span\u0026gt; i \u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;in\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;range\u0026lt;/span\u0026gt; (\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;10\u0026lt;/span\u0026gt;):\n\u0026lt;span class=\u0026quot;hljs-meta\u0026quot;\u0026gt;... \u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;sum\u0026lt;/span\u0026gt; + = \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.1\u0026lt;/span\u0026gt;\n...\u0026amp;gt;\u0026amp;gt;\u0026amp;gt; \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;sum\u0026lt;/span\u0026gt;\n\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.9999999999999999\u0026lt;/span\u0026gt;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;The arithmetic of binary floating point numbers holds many such surprises. The problem with \u0026quot;0.1\u0026quot; is explained in detail below, in the section \u0026quot;Representation errors\u0026quot;. See The Perils of Floating Point for a more complete list of such surprises.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;It is true that there is no simple answer, however do not be overly suspicious of floating virtula numbers! Errors, in Python, in floating-point number operations are due to the underlying hardware, and on most machines are no more than 1 in 2 ** 53 per operation. This is more than necessary for most tasks, but you should keep in mind that these are not decimal operations, and every operation on floating point numbers may suffer from a new error.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Although pathological cases exist, for most common use cases you will get the expected result at the end by simply rounding up to the number of decimal places you want on the display. For fine control over how floats are displayed, see String Formatting Syntax for the formatting specifications of the str.format () method.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;This part of the answer explains in detail the example of \u0026quot;0.1\u0026quot; and shows how you can perform an exact analysis of this type of case on your own. We assume that you are familiar with the binary representation of floating point numbers.The term Representation error means that most decimal fractions cannot be represented exactly in binary. This is the main reason why Python (or Perl, C, C ++, Java, Fortran, and many others) usually doesn\u0026apos;t display the exact result in decimal:\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt; 0.1 + 0.2\n0.30000000000000004\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;Why ? 1/10 and 2/10 are not representable exactly in binary fractions. However, all machines today (July 2010) follow the IEEE-754 standard for the arithmetic of floating point numbers. and most platforms use an \u0026quot;IEEE-754 double precision\u0026quot; to represent Python floats. Double precision IEEE-754 uses 53 bits of precision, so on reading the computer tries to convert 0.1 to the nearest fraction of the form J / 2 ** N with J an integer of exactly 53 bits. Rewrite :\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;1/10 ~ = J / (2 ** N)\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;in :\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;J ~ = 2 ** N / 10\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;remembering that J is exactly 53 bits (so\u0026amp;gt; = 2 ** 52 but \u0026amp;lt;2 ** 53), the best possible value for N is 56:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-py s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-python\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-meta\u0026quot;\u0026gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt; \u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt; ** \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;52\u0026lt;/span\u0026gt;\n\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;4503599627370496\u0026lt;/span\u0026gt;\n\u0026lt;span class=\u0026quot;hljs-meta\u0026quot;\u0026gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt; \u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt; ** \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;53\u0026lt;/span\u0026gt;\n\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;9007199254740992\u0026lt;/span\u0026gt;\n\u0026lt;span class=\u0026quot;hljs-meta\u0026quot;\u0026gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt; \u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt; ** \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;56\u0026lt;/span\u0026gt;/\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;10\u0026lt;/span\u0026gt;\n\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;7205759403792793\u0026lt;/span\u0026gt;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;So 56 is the only possible value for N which leaves exactly 53 bits for J. The best possible value for J is therefore this quotient, rounded:\u0026lt;/p\u0026gt;\n\u0026lt;pre class=\u0026quot;lang-py s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-python\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-meta\u0026quot;\u0026gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt; \u0026lt;/span\u0026gt;q, r = \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;divmod\u0026lt;/span\u0026gt; (\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;2\u0026lt;/span\u0026gt; ** \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;56\u0026lt;/span\u0026gt;, \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;10\u0026lt;/span\u0026gt;)\n\u0026lt;span class=\u0026quot;hljs-meta\u0026quot;\u0026gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt; \u0026lt;/span\u0026gt;r\n\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;6\u0026lt;/span\u0026gt;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;Since the carry is greater than half of 10, the best approximation is obtained by rounding up:\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt; q + 1\n7205759403792794\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;Therefore the best possible approximation for 1/10 in \u0026quot;IEEE-754 double precision\u0026quot; is this above 2 ** 56, that is:\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;7205759403792794/72057594037927936\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;Note that since the rounding was done upward, the result is actually slightly greater than 1/10; if we hadn\u0026apos;t rounded up, the quotient would have been slightly less than 1/10. But in no case is it exactly 1/10!\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;So the computer never \u0026quot;sees\u0026quot; 1/10: what it sees is the exact fraction given above, the best approximation using the double precision floating point numbers from the \u0026quot;\u0026quot; IEEE-754 \u0026quot;:\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;. 1 * 2 ** 56\n7205759403792794.0\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;If we multiply this fraction by 10 ** 30, we can observe the values of its 30 decimal places of strong weight.\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt; 7205759403792794 * 10 ** 30 // 2 ** 56\n100000000000000005551115123125L\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;meaning that the exact value stored in the computer is approximately equal to the decimal value 0.100000000000000005551115123125. In versions prior to Python 2.7 and Python 3.1, Python rounded these values to 17 significant decimal places, displaying 0.10000000000000001. In current versions of Python, the displayed value is the value whose fraction is as short as possible while giving exactly the same representation when converted back to binary, simply displaying 0.1.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Imagine working in base ten with, say, 8 digits of accuracy.  You check whether \u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;1/3 + 2 / 3 == 1\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;and learn that this returns \u0026lt;code\u0026gt;false\u0026lt;/code\u0026gt;.  Why?  Well, as real numbers we have\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;1/3 = 0.333....\u0026lt;/em\u0026gt; and \u0026lt;em\u0026gt;2/3 = 0.666....\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Truncating at eight decimal places, we get\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;0.33333333 + 0.66666666 = 0.99999999\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;which is, of course, different from \u0026lt;code\u0026gt;1.00000000\u0026lt;/code\u0026gt; by exactly \u0026lt;code\u0026gt;0.00000001\u0026lt;/code\u0026gt;.\u0026lt;/p\u0026gt;\n\n\u0026lt;hr\u0026gt;\n\n\u0026lt;p\u0026gt;The situation for binary numbers with a fixed number of bits is exactly analogous. As real numbers, we have\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;1/10 = 0.0001100110011001100... (base 2)\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;and\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;1/5 = 0.0011001100110011001... (base 2)\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;If we truncated these to, say, seven bits, then we\u0026apos;d get\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;0.0001100 + 0.0011001 = 0.0100101\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;while on the other hand,\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;3/10 = 0.01001100110011... (base 2)\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;which, truncated to seven bits, is \u0026lt;code\u0026gt;0.0100110\u0026lt;/code\u0026gt;, and these differ by exactly \u0026lt;code\u0026gt;0.0000001\u0026lt;/code\u0026gt;.\u0026lt;/p\u0026gt;\n\n\u0026lt;hr\u0026gt;\n\n\u0026lt;p\u0026gt;The exact situation is slightly more subtle because these numbers are typically stored in scientific notation.  So, for instance, instead of storing 1/10 as \u0026lt;code\u0026gt;0.0001100\u0026lt;/code\u0026gt; we may store it as something like \u0026lt;code\u0026gt;1.10011 * 2^-4\u0026lt;/code\u0026gt;, depending on how many bits we\u0026apos;ve allocated for the exponent and the mantissa.  This affects how many digits of precision you get for your calculations.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;The upshot is that because of these rounding errors you essentially never want to use == on floating-point numbers.  Instead, you can check if the absolute value of their difference is smaller than some fixed small number.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;blockquote\u0026gt;\n  \u0026lt;p\u0026gt;It\u0026apos;s actually pretty simple. When you have a base 10 system (like ours), it can only express fractions that use a prime factor of the base. The prime factors of 10 are 2 and 5. So 1/2, 1/4, 1/5, 1/8, and 1/10 can all be expressed cleanly because the denominators all use prime factors of 10. In contrast, 1/3, 1/6, and 1/7 are all repeating decimals because their denominators use a prime factor of 3 or 7. In binary (or base 2), the only prime factor is 2. So you can only express fractions cleanly which only contain 2 as a prime factor. In binary, 1/2, 1/4, 1/8 would all be expressed cleanly as decimals. While, 1/5 or 1/10 would be repeating decimals. So 0.1 and 0.2 (1/10 and 1/5) while clean decimals in a base 10 system, are repeating decimals in the base 2 system the computer is operating in. When you do math on these repeating decimals, you end up with leftovers which carry over when you convert the computer\u0026apos;s base 2 (binary) number into a more human readable base 10 number.\u0026lt;/p\u0026gt;\n\u0026lt;/blockquote\u0026gt;\n\n\u0026lt;p\u0026gt;From \u0026lt;a href=\u0026quot;https://0.30000000000000004.com/\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;https://0.30000000000000004.com/\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Decimal numbers such as \u0026lt;code\u0026gt;0.1\u0026lt;/code\u0026gt;, \u0026lt;code\u0026gt;0.2\u0026lt;/code\u0026gt;, and \u0026lt;code\u0026gt;0.3\u0026lt;/code\u0026gt; are not represented exactly in binary encoded floating point types. The sum of the approximations for \u0026lt;code\u0026gt;0.1\u0026lt;/code\u0026gt; and \u0026lt;code\u0026gt;0.2\u0026lt;/code\u0026gt; differs from the approximation used for \u0026lt;code\u0026gt;0.3\u0026lt;/code\u0026gt;, hence the falsehood of \u0026lt;code\u0026gt;0.1 + 0.2 == 0.3\u0026lt;/code\u0026gt; as can be seen more clearly here:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre class=\u0026quot;lang-c s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-c\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-meta\u0026quot;\u0026gt;#\u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;include\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026amp;lt;stdio.h\u0026amp;gt;\u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\n\n\u0026lt;span class=\u0026quot;hljs-type\u0026quot;\u0026gt;int\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-title function_\u0026quot;\u0026gt;main\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;hljs-params\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; {\n    \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;printf\u0026lt;/span\u0026gt;(\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;0.1 + 0.2 == 0.3 is %s\\n\u0026quot;\u0026lt;/span\u0026gt;, \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.1\u0026lt;/span\u0026gt; + \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.2\u0026lt;/span\u0026gt; == \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.3\u0026lt;/span\u0026gt; ? \u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;true\u0026quot;\u0026lt;/span\u0026gt; : \u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;false\u0026quot;\u0026lt;/span\u0026gt;);\n    \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;printf\u0026lt;/span\u0026gt;(\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;0.1 is %.23f\\n\u0026quot;\u0026lt;/span\u0026gt;, \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.1\u0026lt;/span\u0026gt;);\n    \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;printf\u0026lt;/span\u0026gt;(\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;0.2 is %.23f\\n\u0026quot;\u0026lt;/span\u0026gt;, \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.2\u0026lt;/span\u0026gt;);\n    \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;printf\u0026lt;/span\u0026gt;(\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;0.1 + 0.2 is %.23f\\n\u0026quot;\u0026lt;/span\u0026gt;, \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.1\u0026lt;/span\u0026gt; + \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.2\u0026lt;/span\u0026gt;);\n    \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;printf\u0026lt;/span\u0026gt;(\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;0.3 is %.23f\\n\u0026quot;\u0026lt;/span\u0026gt;, \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.3\u0026lt;/span\u0026gt;);\n    \u0026lt;span class=\u0026quot;hljs-built_in\u0026quot;\u0026gt;printf\u0026lt;/span\u0026gt;(\u0026lt;span class=\u0026quot;hljs-string\u0026quot;\u0026gt;\u0026quot;0.3 - (0.1 + 0.2) is %g\\n\u0026quot;\u0026lt;/span\u0026gt;, \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.3\u0026lt;/span\u0026gt; - (\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.1\u0026lt;/span\u0026gt; + \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.2\u0026lt;/span\u0026gt;));\n    \u0026lt;span class=\u0026quot;hljs-keyword\u0026quot;\u0026gt;return\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0\u0026lt;/span\u0026gt;;\n}\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;Output:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;0.1 + 0.2 == 0.3 is false\n0.1 is 0.10000000000000000555112\n0.2 is 0.20000000000000001110223\n0.1 + 0.2 is 0.30000000000000004440892\n0.3 is 0.29999999999999998889777\n0.3 - (0.1 + 0.2) is -5.55112e-17\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;For these computations to be evaluated more reliably, you would need to use a decimal-based representation for floating point values. The C Standard does not specify such types by default but as an extension described in a \u0026lt;a href=\u0026quot;http://www.open-std.org/JTC1/SC22/WG14/www/docs/n1312.pdf\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;technical Report\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;The \u0026lt;code\u0026gt;_Decimal32\u0026lt;/code\u0026gt;, \u0026lt;code\u0026gt;_Decimal64\u0026lt;/code\u0026gt; and \u0026lt;code\u0026gt;_Decimal128\u0026lt;/code\u0026gt; types might be available on your system (for example, \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/GNU_Compiler_Collection\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;GCC\u0026lt;/a\u0026gt; supports them on \u0026lt;a href=\u0026quot;https://gcc.gnu.org/onlinedocs/gcc-4.2.4/gcc/Decimal-Float.html\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;selected targets\u0026lt;/a\u0026gt;, but \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Clang\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;Clang\u0026lt;/a\u0026gt; does not support them on \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/OS_X\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;OS\u0026amp;nbsp;X\u0026lt;/a\u0026gt;).\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Since this thread branched off a bit into a general discussion over current floating point implementations I\u0026apos;d add that there are projects on fixing their issues.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Take a look at \u0026lt;a href=\u0026quot;https://posithub.org/\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;https://posithub.org/\u0026lt;/a\u0026gt; for example, which showcases a number type called posit (and its predecessor unum) that promises to offer better accuracy with fewer bits. If my understanding is correct, it also fixes the kind of problems in the question. Quite interesting project, the person behind it is a mathematician it \u0026lt;a href=\u0026quot;http://www.johngustafson.net/\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;Dr. John Gustafson\u0026lt;/a\u0026gt;. The whole thing is open source, with many actual implementations in C/C++, Python, Julia and C# (\u0026lt;a href=\u0026quot;https://hastlayer.com/arithmetics\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;https://hastlayer.com/arithmetics\u0026lt;/a\u0026gt;).\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Normal arithmetic is base-10, so decimals represent tenths, hundredths, etc.  When you try to represent a floating-point number in binary base-2 arithmetic, you are dealing with halves, fourths, eighths, etc.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;In the hardware, floating points are stored as integer mantissas and exponents.  Mantissa represents the significant digits.  Exponent is like scientific notation but it uses a base of 2 instead of 10.  For example 64.0 would be represented with a mantissa of 1 and exponent of 6.  0.125 would be represented with a mantissa of 1 and an exponent of -3.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;Floating point decimals have to add up negative powers of 2\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;0.1b = 0.5d\n0.01b = 0.25d\n0.001b = 0.125d\n0.0001b = 0.0625d\n0.00001b = 0.03125d\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;and so on.\u0026lt;/p\u0026gt;\n\u0026lt;p\u0026gt;It is common to use a error delta instead of using equality operators when dealing with floating point arithmetic.  Instead of\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;if(a==b) ...\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\u0026lt;p\u0026gt;you would use\u0026lt;/p\u0026gt;\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;delta = 0.0001; // or some arbitrarily small amount\nif(a - b \u0026amp;gt; -delta \u0026amp;amp;\u0026amp;amp; a - b \u0026amp;lt; delta) ...\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n    "],"comment":["\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;Floating point variables typically have this behaviour. It\u0026apos;s caused by how they are stored in hardware. For more info check out the \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Floating-point\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;Wikipedia article on floating point numbers\u0026lt;/a\u0026gt;.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/68507/ben-s\u0026quot; title=\u0026quot;67,275 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Ben S\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment21420890_588004\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2009-02-25 21:41:51Z, License: CC BY-SA 2.5\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Feb 25, 2009 at 21:41\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;JavaScript treats decimals as \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Floating_point\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;floating point numbers\u0026lt;/a\u0026gt;, which means operations like addition might be subject to rounding error. You might want to take a look at this article: \u0026lt;a href=\u0026quot;http://docs.sun.com/source/806-3568/ncg_goldberg.html\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;What Every Computer Scientist Should Know About Floating-Point Arithmetic\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/4249/matt-b\u0026quot; title=\u0026quot;135,686 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;matt b\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment21420903_588004\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2009-02-25 21:42:49Z, License: CC BY-SA 2.5\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Feb 25, 2009 at 21:42\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;Just for information, ALL numeric types in javascript are IEEE-754 Doubles.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/13227/gary-willoughby\u0026quot; title=\u0026quot;48,829 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Gary Willoughby\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment21420906_588004\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2010-04-11 13:01:33Z, License: CC BY-SA 2.5\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Apr 11, 2010 at 13:01\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;Because JavaScript uses the IEEE 754 standard for Math, it makes use of \u0026lt;b\u0026gt;64-bit\u0026lt;/b\u0026gt; floating numbers. This causes precision errors when doing floating point (decimal) calculations, in short, due to computers working in \u0026lt;b\u0026gt;Base 2\u0026lt;/b\u0026gt; while decimal is \u0026lt;b\u0026gt;Base 10\u0026lt;/b\u0026gt;.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/5043867/pardeep-jain\u0026quot; title=\u0026quot;79,013 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Pardeep Jain\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment87431553_588004\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2018-05-07 04:57:28Z, License: CC BY-SA 4.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;May 7, 2018 at 4:57\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n                        \u0026lt;span title=\u0026quot;this comment was edited 2 times\u0026quot;\u0026gt;\n                            \u0026lt;svg aria-hidden=\u0026quot;true\u0026quot; class=\u0026quot;va-text-bottom o50 svg-icon iconPencilSm\u0026quot; width=\u0026quot;14\u0026quot; height=\u0026quot;14\u0026quot; viewBox=\u0026quot;0 0 14 14\u0026quot;\u0026gt;\u0026lt;path d=\u0026quot;m11.1 1.71 1.13 1.12c.2.2.2.51 0 .71L11.1 4.7 9.21 2.86l1.17-1.15c.2-.2.51-.2.71 0ZM2 10.12l6.37-6.43 1.88 1.88L3.88 12H2v-1.88Z\u0026quot;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;/svg\u0026gt;\n                        \u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;Simple explanation: 1/10 is periodic in binary (0.0 0011 0011 0011...) just like 1/3 is periodic in decimal (0.333...), so 1/10 can\u0026apos;t be accurately represented by a floating point number.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/589924/ikegami\u0026quot; title=\u0026quot;346,218 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;ikegami\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment105431564_588004\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2020-01-07 19:14:07Z, License: CC BY-SA 4.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Jan 7, 2020 at 19:14\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;I think \u0026quot;some error constant\u0026quot; is more correct than \u0026quot;The Epsilon\u0026quot; because there is no \u0026quot;The Epsilon\u0026quot; which could be used in all cases. Different epsilons need to be used in different situations. And the machine epsilon is almost never a good constant to use.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/166235/rotsor\u0026quot; title=\u0026quot;13,305 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Rotsor\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment3831071_588014\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2010-09-04 23:33:57Z, License: CC BY-SA 2.5\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Sep 4, 2010 at 23:33\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;It\u0026apos;s not \u0026lt;i\u0026gt;quite\u0026lt;/i\u0026gt; true that all floating-point math is based on the IEEE [754] standard.  There are still some systems in use that have the old IBM hexadecimal FP, for example, and there are still graphics cards that don\u0026apos;t support IEEE-754 arithmetic.  It\u0026apos;s true to a reasonably approximation, however.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/142434/stephen-canon\u0026quot; title=\u0026quot;101,076 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Stephen Canon\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment19596486_588014\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2013-01-03 23:36:29Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Jan 3, 2013 at 23:36\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;Cray ditched IEEE-754 compliance for speed.  Java loosened its adherence as an optimization as well.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/230979/art-taylor\u0026quot; title=\u0026quot;1,158 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Art Taylor\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment20769481_588014\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2013-02-12 03:12:57Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Feb 12, 2013 at 3:12\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;I think you should add something to this answer about how computations on money should always, always be done with fixed-point arithmetic on \u0026lt;i\u0026gt;integers\u0026lt;/i\u0026gt;, because money is quantized.  (It may make sense to do internal accounting computations in tiny fractions of a cent, or whatever your smallest currency unit is - this often helps with e.g. reducing round-off error when converting \u0026quot;$29.99 a month\u0026quot; to a daily rate - but it should still be fixed-point arithmetic.)\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/388520/zwol\u0026quot; title=\u0026quot;129,617 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;zwol\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment36264082_588014\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2014-05-12 22:23:27Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;May 12, 2014 at 22:23\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n                        \u0026lt;span title=\u0026quot;this comment was edited 1 time\u0026quot;\u0026gt;\n                            \u0026lt;svg aria-hidden=\u0026quot;true\u0026quot; class=\u0026quot;va-text-bottom o50 svg-icon iconPencilSm\u0026quot; width=\u0026quot;14\u0026quot; height=\u0026quot;14\u0026quot; viewBox=\u0026quot;0 0 14 14\u0026quot;\u0026gt;\u0026lt;path d=\u0026quot;m11.1 1.71 1.13 1.12c.2.2.2.51 0 .71L11.1 4.7 9.21 2.86l1.17-1.15c.2-.2.51-.2.71 0ZM2 10.12l6.37-6.43 1.88 1.88L3.88 12H2v-1.88Z\u0026quot;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;/svg\u0026gt;\n                        \u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;Interesting fact: this very 0.1 not being exactly represented in binary floating-point caused an infamous \u0026lt;a href=\u0026quot;https://web.archive.org/web/20151121063711/http://sydney.edu.au/engineering/it/~alum/patriot_bug.html\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;Patriot missile software bug\u0026lt;/a\u0026gt; which resulted in 28 people killed during the first Iraq war.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/4328188/hdl\u0026quot; title=\u0026quot;968 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;hdl\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment53362895_588014\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2015-09-24 12:57:18Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Sep 24, 2015 at 12:57\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n                        \u0026lt;span title=\u0026quot;this comment was edited 1 time\u0026quot;\u0026gt;\n                            \u0026lt;svg aria-hidden=\u0026quot;true\u0026quot; class=\u0026quot;va-text-bottom o50 svg-icon iconPencilSm\u0026quot; width=\u0026quot;14\u0026quot; height=\u0026quot;14\u0026quot; viewBox=\u0026quot;0 0 14 14\u0026quot;\u0026gt;\u0026lt;path d=\u0026quot;m11.1 1.71 1.13 1.12c.2.2.2.51 0 .71L11.1 4.7 9.21 2.86l1.17-1.15c.2-.2.51-.2.71 0ZM2 10.12l6.37-6.43 1.88 1.88L3.88 12H2v-1.88Z\u0026quot;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;/svg\u0026gt;\n                        \u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;(3) is wrong. The rounding error in a division is not less than \u0026lt;i\u0026gt;one\u0026lt;/i\u0026gt; unit in the last place, but at most \u0026lt;i\u0026gt;half\u0026lt;/i\u0026gt; a unit in the last place.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/3255455/gnasher729\u0026quot; title=\u0026quot;49,941 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;gnasher729\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment35588095_16082201\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2014-04-23 22:31:22Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Apr 23, 2014 at 22:31\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;@gnasher729 Good catch. Most basic operations also have en error of less than 1/2 of one unit in the last place using the default IEEE rounding mode. Edited the explanation, and also noted that the error may be greater than 1/2 of one ulp but less than 1 ulp if the user overrides the default rounding mode (this is especially true in embedded systems).\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/2244559/kernelpanik\u0026quot; title=\u0026quot;7,829 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;KernelPanik\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment35607440_16082201\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2014-04-24 11:17:14Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Apr 24, 2014 at 11:17\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;(1) Floating point \u0026lt;i\u0026gt;numbers\u0026lt;/i\u0026gt; do not have error.  Every floating point value is exactly what it is.  Most (but not all) floating point \u0026lt;i\u0026gt;operations\u0026lt;/i\u0026gt; give inexact results.  For example, there is no binary floating point value that is exactly equal to 1.0/10.0.  Some operations (e.g., 1.0 + 1.0) \u0026lt;i\u0026gt;do\u0026lt;/i\u0026gt; give exact results on the other hand.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/801894/solomon-slow\u0026quot; title=\u0026quot;22,151 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Solomon Slow\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment37260700_16082201\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2014-06-10 16:31:54Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Jun 10, 2014 at 16:31\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;\u0026quot;The main cause of the error in floating point division, are the division algorithms used to calculate the quotient\u0026quot; is a \u0026lt;i\u0026gt;very\u0026lt;/i\u0026gt; misleading thing to say.  For an IEEE-754 conforming division, the \u0026lt;i\u0026gt;only\u0026lt;/i\u0026gt; cause of error in floating-point division is the inability of the result to be exactly represented in the result format; the same result is computed regardless of the algorithm that is used.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/142434/stephen-canon\u0026quot; title=\u0026quot;101,076 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Stephen Canon\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment45658265_16082201\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2015-02-23 20:23:31Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Feb 23, 2015 at 20:23\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n                        \u0026lt;span title=\u0026quot;this comment was edited 1 time\u0026quot;\u0026gt;\n                            \u0026lt;svg aria-hidden=\u0026quot;true\u0026quot; class=\u0026quot;va-text-bottom o50 svg-icon iconPencilSm\u0026quot; width=\u0026quot;14\u0026quot; height=\u0026quot;14\u0026quot; viewBox=\u0026quot;0 0 14 14\u0026quot;\u0026gt;\u0026lt;path d=\u0026quot;m11.1 1.71 1.13 1.12c.2.2.2.51 0 .71L11.1 4.7 9.21 2.86l1.17-1.15c.2-.2.51-.2.71 0ZM2 10.12l6.37-6.43 1.88 1.88L3.88 12H2v-1.88Z\u0026quot;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;/svg\u0026gt;\n                        \u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;@Matt Sorry for the late response. It\u0026apos;s basically due to resource/time issues and tradeoffs. There is a way to do long division/more \u0026apos;normal\u0026apos; division, it\u0026apos;s called SRT Division with radix two. However, this repeatedly shifts and subtracts the divisor from the dividend and takes many clock cycles since it only computes one bit of the quotient per clock cycle. We use tables of reciprocals so that we can compute more bits of the quotient per cycle and make effective performance/speed tradeoffs.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/2244559/kernelpanik\u0026quot; title=\u0026quot;7,829 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;KernelPanik\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment57988540_16082201\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2016-02-01 15:33:30Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Feb 1, 2016 at 15:33\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n                        \u0026lt;span title=\u0026quot;this comment was edited 1 time\u0026quot;\u0026gt;\n                            \u0026lt;svg aria-hidden=\u0026quot;true\u0026quot; class=\u0026quot;va-text-bottom o50 svg-icon iconPencilSm\u0026quot; width=\u0026quot;14\u0026quot; height=\u0026quot;14\u0026quot; viewBox=\u0026quot;0 0 14 14\u0026quot;\u0026gt;\u0026lt;path d=\u0026quot;m11.1 1.71 1.13 1.12c.2.2.2.51 0 .71L11.1 4.7 9.21 2.86l1.17-1.15c.2-.2.51-.2.71 0ZM2 10.12l6.37-6.43 1.88 1.88L3.88 12H2v-1.88Z\u0026quot;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;/svg\u0026gt;\n                        \u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;Great and short answer. Repeating pattern looks like  0.00011001100110011001100110011001100110011001100110011...\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/487226/konstantin-chernov\u0026quot; title=\u0026quot;1,841 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Konstantin Chernov\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment14478714_588029\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2012-06-16 14:22:35Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Jun 16, 2012 at 14:22\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n                        \u0026lt;span title=\u0026quot;this comment was edited 2 times\u0026quot;\u0026gt;\n                            \u0026lt;svg aria-hidden=\u0026quot;true\u0026quot; class=\u0026quot;va-text-bottom o50 svg-icon iconPencilSm\u0026quot; width=\u0026quot;14\u0026quot; height=\u0026quot;14\u0026quot; viewBox=\u0026quot;0 0 14 14\u0026quot;\u0026gt;\u0026lt;path d=\u0026quot;m11.1 1.71 1.13 1.12c.2.2.2.51 0 .71L11.1 4.7 9.21 2.86l1.17-1.15c.2-.2.51-.2.71 0ZM2 10.12l6.37-6.43 1.88 1.88L3.88 12H2v-1.88Z\u0026quot;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;/svg\u0026gt;\n                        \u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;There ARE methods that yield exact decimal values. BCD (Binary coded decimal) or various other forms of decimal number. However, these are both slower (a LOT slower) and take more storage than using binary floating point. (as an example, packed BCD stores 2 decimal digits in a byte. That\u0026apos;s 100 possible values in a byte that can actually store 256 possible values, or 100/256, which wastes about 60% of the possible values of a byte.)\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/205185/duncan-c\u0026quot; title=\u0026quot;122,859 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Duncan C\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment63348913_588029\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2016-06-21 16:43:02Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Jun 21, 2016 at 16:43\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;@IInspectable, for floating point operations, BCD based math is hundreds  of times slower than native binary floating point.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/205185/duncan-c\u0026quot; title=\u0026quot;122,859 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Duncan C\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment66089383_588029\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2016-09-08 00:20:27Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Sep 8, 2016 at 0:20\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;@DuncanC Well, there are methods that yield exact decimal values -- for addition and subtraction. For division, multiplication, etc. they have the same issues as binary methods. That\u0026apos;s why BCD is used in accounting since that deals mostly with plus and minus and you can\u0026apos;t account for anything smaller than a penny. However something simple like \u0026lt;code\u0026gt;1/3*3 == 1\u0026lt;/code\u0026gt; fails (evaluates to false) in BCD math, just like it would fail if you used decimal division on paper.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/4691830/joooeey\u0026quot; title=\u0026quot;2,585 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Joooeey\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment88985885_588029\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2018-06-22 21:19:54Z, License: CC BY-SA 4.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Jun 22, 2018 at 21:19\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;@DuncanC: \u0026lt;i\u0026gt;\u0026quot;BCD is a lot slower than binary floating point, period.\u0026quot;\u0026lt;/i\u0026gt; - Uhm, yeah. Unless it isn\u0026apos;t. Pretty sure there are \u0026lt;a href=\u0026quot;https://en.wikipedia.org/wiki/HP_Saturn\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;architectures\u0026lt;/a\u0026gt;, where BCD math is at least as fast (or faster) than IEEE-754 floating point math. But that\u0026apos;s besides the point: If you need decimal accuracy, you cannot use IEEE-754 floating point representation. Doing so will achieve one thing only: Calculating the wrong results faster.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/1889329/iinspectable\u0026quot; title=\u0026quot;41,340 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;IInspectable\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment88994395_588029\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2018-06-23 10:30:03Z, License: CC BY-SA 4.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Jun 23, 2018 at 10:30\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;Note that there are some languages which include exact math. One example is Scheme, for example via GNU Guile. See \u0026lt;a href=\u0026quot;http://draketo.de/english/exact-math-to-the-rescue\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;draketo.de/english/exact-math-to-the-rescue\u0026lt;/a\u0026gt;  these keep the math as fractions and only slice up in the end.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/7666/arne-babenhauserheide\u0026quot; title=\u0026quot;2,333 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Arne Babenhauserheide\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment42587900_27030789\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2014-11-20 06:40:16Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Nov 20, 2014 at 6:40\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;@FloatingRock Actually, very few mainstream programming languages have rational numbers built-in. Arne is a Schemer, as I am, so these are things we get spoilt on.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/13/chris-jester-young\u0026quot; title=\u0026quot;213,722 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Chris Jester-Young\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment42763580_27030789\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2014-11-25 16:56:48Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Nov 25, 2014 at 16:56\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n                        \u0026lt;span title=\u0026quot;this comment was edited 1 time\u0026quot;\u0026gt;\n                            \u0026lt;svg aria-hidden=\u0026quot;true\u0026quot; class=\u0026quot;va-text-bottom o50 svg-icon iconPencilSm\u0026quot; width=\u0026quot;14\u0026quot; height=\u0026quot;14\u0026quot; viewBox=\u0026quot;0 0 14 14\u0026quot;\u0026gt;\u0026lt;path d=\u0026quot;m11.1 1.71 1.13 1.12c.2.2.2.51 0 .71L11.1 4.7 9.21 2.86l1.17-1.15c.2-.2.51-.2.71 0ZM2 10.12l6.37-6.43 1.88 1.88L3.88 12H2v-1.88Z\u0026quot;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;/svg\u0026gt;\n                        \u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;@ArneBabenhauserheide I think it\u0026apos;s worth adding that this will only work with rational numbers. So if you\u0026apos;re doing some math with irrational numbers like pi, you\u0026apos;d have to store it as a multiple of pi. Of course, any calculating involving pi cannot be represented as an exact decimal number.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/613176/aidiakapi\u0026quot; title=\u0026quot;5,858 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Aidiakapi\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment46223188_27030789\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2015-03-11 13:06:23Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Mar 11, 2015 at 13:06\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;@connexo Okay. How would you program your pizza rotator to get 36 degrees? What is 36 degrees? (Hint: if you are able to define this in an exact fashion, you also have a slices-an-exact-tenth pizza cutter.) In other words, you can\u0026apos;t actually have 1/360 (a degree) or 1/10 (36 degrees) with only binary floating point.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/13/chris-jester-young\u0026quot; title=\u0026quot;213,722 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Chris Jester-Young\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment51887453_27030789\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2015-08-13 14:50:56Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Aug 13, 2015 at 14:50\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n                        \u0026lt;span title=\u0026quot;this comment was edited 1 time\u0026quot;\u0026gt;\n                            \u0026lt;svg aria-hidden=\u0026quot;true\u0026quot; class=\u0026quot;va-text-bottom o50 svg-icon iconPencilSm\u0026quot; width=\u0026quot;14\u0026quot; height=\u0026quot;14\u0026quot; viewBox=\u0026quot;0 0 14 14\u0026quot;\u0026gt;\u0026lt;path d=\u0026quot;m11.1 1.71 1.13 1.12c.2.2.2.51 0 .71L11.1 4.7 9.21 2.86l1.17-1.15c.2-.2.51-.2.71 0ZM2 10.12l6.37-6.43 1.88 1.88L3.88 12H2v-1.88Z\u0026quot;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;/svg\u0026gt;\n                        \u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;@connexo Also, \u0026quot;every idiot\u0026quot; can\u0026apos;t rotate a pizza \u0026lt;i\u0026gt;exactly\u0026lt;/i\u0026gt; 36 degrees. Humans are too error-prone to do anything quite so precise.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/13/chris-jester-young\u0026quot; title=\u0026quot;213,722 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Chris Jester-Young\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment51887497_27030789\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2015-08-13 14:51:44Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Aug 13, 2015 at 14:51\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;@Pacerier Sure, they could use two unbounded-precision integers to represent a fraction, or they could use quote notation. It\u0026apos;s the specific notion of \u0026quot;binary\u0026quot; or \u0026quot;decimal\u0026quot; that makes this impossible -- the idea that you have a sequence of binary/decimal digits and, somewhere in there, a radix point.  To get precise rational results we\u0026apos;d need a better format.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/18515/devin-jeanpierre\u0026quot; title=\u0026quot;88,391 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Devin Jeanpierre\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment9474595_588019\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2011-10-15 19:45:03Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Oct 15, 2011 at 19:45\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n                        \u0026lt;span title=\u0026quot;this comment was edited 1 time\u0026quot;\u0026gt;\n                            \u0026lt;svg aria-hidden=\u0026quot;true\u0026quot; class=\u0026quot;va-text-bottom o50 svg-icon iconPencilSm\u0026quot; width=\u0026quot;14\u0026quot; height=\u0026quot;14\u0026quot; viewBox=\u0026quot;0 0 14 14\u0026quot;\u0026gt;\u0026lt;path d=\u0026quot;m11.1 1.71 1.13 1.12c.2.2.2.51 0 .71L11.1 4.7 9.21 2.86l1.17-1.15c.2-.2.51-.2.71 0ZM2 10.12l6.37-6.43 1.88 1.88L3.88 12H2v-1.88Z\u0026quot;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;/svg\u0026gt;\n                        \u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;@Pacerier: Neither binary nor decimal floating-point can precisely store 1/3 or 1/13.  Decimal floating-point types can precisely represent values of the form M/10^E, \u0026lt;i\u0026gt;but are less precise than similarly-sized binary floating-point numbers when it comes to representing most other fractions\u0026lt;/i\u0026gt;.  In many applications, it\u0026apos;s more useful to have higher precision with arbitrary fractions than to have perfect precision with a few \u0026quot;special\u0026quot; ones.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/363751/supercat\u0026quot; title=\u0026quot;73,697 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;supercat\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment35622207_588019\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2014-04-24 16:43:59Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Apr 24, 2014 at 16:43\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;@supercat In comparing precision of \u0026lt;a href=\u0026quot;https://en.wikipedia.org/wiki/Double-precision_floating-point_format\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;binary64\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026quot;https://en.wikipedia.org/wiki/Decimal64_floating-point_format\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;decimal64\u0026lt;/a\u0026gt;: the precision are fairly comparable - certainly within a factor of 10 to each other.  Granted decimal64 wobbles more than binary64.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/2410359/chux-reinstate-monica\u0026quot; title=\u0026quot;128,312 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;chux - Reinstate Monica\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment52352502_588019\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2015-08-26 19:26:18Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Aug 26, 2015 at 19:26\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;@chux: The difference in precision between binary and decimal types isn\u0026apos;t huge, but the 10:1 difference in best-case vs. worst-case precision for decimal types is far greater than the 2:1 difference with binary types.  I\u0026apos;m curious whether anyone has built hardware or written software to operate efficiently on either of the decimal types, since neither would seem amenable to efficient implementation in hardware nor software.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/363751/supercat\u0026quot; title=\u0026quot;73,697 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;supercat\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment52353188_588019\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2015-08-26 19:47:05Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Aug 26, 2015 at 19:47\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;@DevinJeanpierre I think the point is that \u0026quot;computers\u0026quot; don\u0026apos;t have a \u0026quot;specific notion of \u0026apos;binary\u0026apos; or \u0026apos;decimal\u0026apos;\u0026quot;. Pacerier\u0026apos;s point seems to be that it is \u0026lt;i\u0026gt;language\u0026lt;/i\u0026gt; designers who have decided to make the jump to \u0026quot;floating point\u0026quot; too early, when storing such numbers as \u0026quot;0.1\u0026quot;, \u0026quot;0.2\u0026quot;, and \u0026quot;0.3\u0026quot; which can not only be more accurately but also \u0026lt;b\u0026gt;more space-efficiently\u0026lt;/b\u0026gt; stored as text (BCD).\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/5379657/jeff-y\u0026quot; title=\u0026quot;2,434 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Jeff Y\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment57987036_588019\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2016-02-01 14:58:33Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Feb 1, 2016 at 14:58\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;The problem is that the conversion itself is inaccurate. 16.08 * 100 = 1607.9999999999998. Do we have to resort to splitting the number and converting separately (as in 16 * 100 + 08 = 1608)?\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/240372/jason\u0026quot; title=\u0026quot;2,676 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Jason\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment9351332_2607316\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2011-10-07 19:13:23Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Oct 7, 2011 at 19:13\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;The solution here is to do all your calculations in integer then divide by your proportion (100 in this case) and round only when presenting the data.  That will ensure that your calculations will always be precise.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/522624/just-a-guy\u0026quot; title=\u0026quot;5,642 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Just a guy\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment10428291_2607316\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2011-12-08 21:38:04Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Dec 8, 2011 at 21:38\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;Just to nitpick a little: integer arithmetic is only exact in floating-point up to a point (pun intended). If the number is larger than 0x1p53 (to use Java 7\u0026apos;s hexadecimal floating point notation, = 9007199254740992), then the ulp is 2 at that point and so 0x1p53 + 1 is rounded down to 0x1p53 (and 0x1p53 + 3 is rounded up to 0x1p53 + 4, because of round-to-even). :-D But certainly, if your number is smaller than 9 quadrillion, you should be fine. :-P\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/13/chris-jester-young\u0026quot; title=\u0026quot;213,722 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Chris Jester-Young\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment43015432_2607316\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2014-12-03 13:28:49Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Dec 3, 2014 at 13:28\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;@Mark Thank you for this Clear explanation but then the question arises why 0.1+0.4 exactly adds up to 0.5  (atleast in Python 3) . Also what is the best way to check equality when using floats in Python 3?\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/2417881/pchegoor\u0026quot; title=\u0026quot;364 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;pchegoor\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment83691241_36027429\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2018-01-20 03:15:40Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Jan 20, 2018 at 3:15\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n                        \u0026lt;span title=\u0026quot;this comment was edited 1 time\u0026quot;\u0026gt;\n                            \u0026lt;svg aria-hidden=\u0026quot;true\u0026quot; class=\u0026quot;va-text-bottom o50 svg-icon iconPencilSm\u0026quot; width=\u0026quot;14\u0026quot; height=\u0026quot;14\u0026quot; viewBox=\u0026quot;0 0 14 14\u0026quot;\u0026gt;\u0026lt;path d=\u0026quot;m11.1 1.71 1.13 1.12c.2.2.2.51 0 .71L11.1 4.7 9.21 2.86l1.17-1.15c.2-.2.51-.2.71 0ZM2 10.12l6.37-6.43 1.88 1.88L3.88 12H2v-1.88Z\u0026quot;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;/svg\u0026gt;\n                        \u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;@user2417881 IEEE floating point operations have rounding rules for every operation, and sometimes the rounding can produce an exact answer even when the two numbers are off by a little. The details are too long for a comment and I\u0026apos;m not an expert in them anyway. As you see in this answer 0.5 is one of the few decimals that can be represented in binary, but that\u0026apos;s just a coincidence. For equality testing see \u0026lt;a href=\u0026quot;https://stackoverflow.com/questions/5595425/what-is-the-best-way-to-compare-floats-for-almost-equality-in-python/33024979#33024979\u0026quot; title=\u0026quot;what is the best way to compare floats for almost equality in python\u0026quot;\u0026gt;stackoverflow.com/questions/5595425/\u0026lt;/a\u0026gt;.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/5987/mark-ransom\u0026quot; title=\u0026quot;287,628 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Mark Ransom\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment83691993_36027429\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2018-01-20 04:35:35Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Jan 20, 2018 at 4:35\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;@user2417881 your question intrigued me so I turned it into a full question and answer: \u0026lt;a href=\u0026quot;https://stackoverflow.com/q/48374522/5987\u0026quot;\u0026gt;stackoverflow.com/q/48374522/5987\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/5987/mark-ransom\u0026quot; title=\u0026quot;287,628 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Mark Ransom\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment83736404_36027429\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2018-01-22 04:27:16Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Jan 22, 2018 at 4:27\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;this made me a real headache. I sum 12 float numbers, then show sum and the average if those numbers. using toFixed() might fix the summing of 2 numbers, but when sum several numbers the leap is significant.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/4896948/nuryagdy-mustapayev\u0026quot; title=\u0026quot;529 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Nuryagdy Mustapayev\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment107434247_51723472\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2020-03-17 12:27:00Z, License: CC BY-SA 4.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Mar 17, 2020 at 12:27\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;@Nuryagdy Mustapayev I didn\u0026apos;t get your intention, as I tested before you can sum 12 float numbers, then use the floatify() function on the result, then do whatever you want on it, I observed no issue using it.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/4344976/muhammad-musavi\u0026quot; title=\u0026quot;2,226 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Muhammad Musavi\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment107476749_51723472\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2020-03-18 19:21:37Z, License: CC BY-SA 4.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Mar 18, 2020 at 19:21\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;I am just saying in my situation where I have around 20 parameters and 20 formulas where the result of each formula depends on others this solution did not help.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/4896948/nuryagdy-mustapayev\u0026quot; title=\u0026quot;529 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Nuryagdy Mustapayev\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment107520899_51723472\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2020-03-20 09:15:25Z, License: CC BY-SA 4.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Mar 20, 2020 at 9:15\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;Rounding to the nearest integer isn\u0026apos;t a safe way to solve the comparison problem in all cases.  0.4999998 and 0.500001 round to different integers, so there\u0026apos;s a \u0026quot;danger zone\u0026quot; around every rounding cut-point.  (I know those decimal strings probably aren\u0026apos;t exactly representable as IEEE binary floats.)\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/224132/peter-cordes\u0026quot; title=\u0026quot;289,881 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Peter Cordes\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment69312649_35166029\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2016-12-09 03:31:33Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Dec 9, 2016 at 3:31\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;Also, even though floating point is a \u0026quot;legacy\u0026quot; format, it\u0026apos;s very well designed.  I don\u0026apos;t know of anything that anyone would change if re-designing it now.  The more I learn about it, the more I think it\u0026apos;s really \u0026lt;i\u0026gt;well\u0026lt;/i\u0026gt; designed.  e.g. the biased exponent means consecutive binary floats have consecutive integer representations, so you can implement \u0026lt;code\u0026gt;nextafter()\u0026lt;/code\u0026gt; with an integer increment or decrement on the binary representation of an IEEE float.  Also, you can compare floats as integers and get the right answer except when they\u0026apos;re both negative (because of sign-magnitude vs. 2\u0026apos;s complement).\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/224132/peter-cordes\u0026quot; title=\u0026quot;289,881 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Peter Cordes\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment69312698_35166029\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2016-12-09 03:35:01Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Dec 9, 2016 at 3:35\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n                        \u0026lt;span title=\u0026quot;this comment was edited 2 times\u0026quot;\u0026gt;\n                            \u0026lt;svg aria-hidden=\u0026quot;true\u0026quot; class=\u0026quot;va-text-bottom o50 svg-icon iconPencilSm\u0026quot; width=\u0026quot;14\u0026quot; height=\u0026quot;14\u0026quot; viewBox=\u0026quot;0 0 14 14\u0026quot;\u0026gt;\u0026lt;path d=\u0026quot;m11.1 1.71 1.13 1.12c.2.2.2.51 0 .71L11.1 4.7 9.21 2.86l1.17-1.15c.2-.2.51-.2.71 0ZM2 10.12l6.37-6.43 1.88 1.88L3.88 12H2v-1.88Z\u0026quot;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;/svg\u0026gt;\n                        \u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;I disagree, the floats should be stored as decimals and not binary and all problems are solved.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/1071698/ronen-festinger\u0026quot; title=\u0026quot;2,183 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Ronen Festinger\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment71816147_35166029\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2017-02-19 19:32:15Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Feb 19, 2017 at 19:32\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;Shouldn\u0026apos;t \u0026quot;\u0026lt;i\u0026gt;x / (2^n + 5^n)\u0026lt;/i\u0026gt;\u0026quot; be \u0026quot;\u0026lt;i\u0026gt;x / (2^n * 5^n)\u0026lt;/i\u0026gt;\u0026quot;?\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/1364007/wai-ha-lee\u0026quot; title=\u0026quot;8,182 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Wai Ha Lee\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment84231903_35166029\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2018-02-05 07:34:25Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Feb 5, 2018 at 7:34\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n                        \u0026lt;span title=\u0026quot;this comment was edited 3 times\u0026quot;\u0026gt;\n                            \u0026lt;svg aria-hidden=\u0026quot;true\u0026quot; class=\u0026quot;va-text-bottom o50 svg-icon iconPencilSm\u0026quot; width=\u0026quot;14\u0026quot; height=\u0026quot;14\u0026quot; viewBox=\u0026quot;0 0 14 14\u0026quot;\u0026gt;\u0026lt;path d=\u0026quot;m11.1 1.71 1.13 1.12c.2.2.2.51 0 .71L11.1 4.7 9.21 2.86l1.17-1.15c.2-.2.51-.2.71 0ZM2 10.12l6.37-6.43 1.88 1.88L3.88 12H2v-1.88Z\u0026quot;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;/svg\u0026gt;\n                        \u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;@stephen c you will be able to define the precision you want at the compiler settings. But it will just round the result, like in a calculator.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/1071698/ronen-festinger\u0026quot; title=\u0026quot;2,183 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Ronen Festinger\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment90697444_35166029\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2018-08-16 08:45:46Z, License: CC BY-SA 4.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Aug 16, 2018 at 8:45\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;We may also use fixed point. For example if cents is your finest granularity, then calculations can be done with integers on number of cents instead of dollars.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/3163618/qwr\u0026quot; title=\u0026quot;8,208 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;qwr\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment111362575_32143465\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2020-07-18 21:58:58Z, License: CC BY-SA 4.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Jul 18, 2020 at 21:58\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n                        \u0026lt;span title=\u0026quot;this comment was edited 1 time\u0026quot;\u0026gt;\n                            \u0026lt;svg aria-hidden=\u0026quot;true\u0026quot; class=\u0026quot;va-text-bottom o50 svg-icon iconPencilSm\u0026quot; width=\u0026quot;14\u0026quot; height=\u0026quot;14\u0026quot; viewBox=\u0026quot;0 0 14 14\u0026quot;\u0026gt;\u0026lt;path d=\u0026quot;m11.1 1.71 1.13 1.12c.2.2.2.51 0 .71L11.1 4.7 9.21 2.86l1.17-1.15c.2-.2.51-.2.71 0ZM2 10.12l6.37-6.43 1.88 1.88L3.88 12H2v-1.88Z\u0026quot;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;/svg\u0026gt;\n                        \u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;@Nae I would translate the second paragraph as \u0026quot;The majority of fractions cannot be represented exactly in either decimal \u0026lt;i\u0026gt;or\u0026lt;/i\u0026gt; binary.  So most results will be rounded off -- although they will still be precise to the number of bits/digits inherent in the representation being used.\u0026quot;\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/3923896/steve-summit\u0026quot; title=\u0026quot;38,556 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Steve Summit\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment85396332_19365074\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2018-03-09 14:19:58Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Mar 9, 2018 at 14:19\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;To the person whose edit I just rolled back: I consider code quotes appropriate for quoting code. This answer, being language-neutral, does not contain any quoted code at all. Numbers can be used in English sentences and that does not turn them into code.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/1798593/patricia-shanahan\u0026quot; title=\u0026quot;25,513 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Patricia Shanahan\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment81833655_34394391\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2017-11-22 16:22:08Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Nov 22, 2017 at 16:22\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;\u0026lt;a href=\u0026quot;https://i.stack.imgur.com/CRFfF.png\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;This\u0026lt;/a\u0026gt; is likely why somebody formatted your numbers as code - not for formatting, but for readability.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/1364007/wai-ha-lee\u0026quot; title=\u0026quot;8,182 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Wai Ha Lee\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment83446335_34394391\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2018-01-12 18:24:54Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Jan 12, 2018 at 18:24\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;... also, the \u0026lt;i\u0026gt;round to even\u0026lt;/i\u0026gt; refers to the \u0026lt;i\u0026gt;binary\u0026lt;/i\u0026gt; representation, \u0026lt;b\u0026gt;not\u0026lt;/b\u0026gt; the \u0026lt;i\u0026gt;decimal\u0026lt;/i\u0026gt; representation. See \u0026lt;a href=\u0026quot;https://en.m.wikipedia.org/wiki/IEEE_754-1985#Rounding_floating-point_numbers\u0026quot; rel=\u0026quot;nofollow noreferrer\u0026quot;\u0026gt;this\u0026lt;/a\u0026gt; or, for instance, \u0026lt;a href=\u0026quot;https://stackoverflow.com/q/8981913/1364007\u0026quot;\u0026gt;this\u0026lt;/a\u0026gt;.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/1364007/wai-ha-lee\u0026quot; title=\u0026quot;8,182 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Wai Ha Lee\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment83448193_34394391\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2018-01-12 19:33:27Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Jan 12, 2018 at 19:33\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;@WaiHaLee I did not apply the odd/even test to any decimal numbers, only hexadecimal. A hexadecimal digit is even if, and only if, the least significant bit of its binary expansion is zero.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/1798593/patricia-shanahan\u0026quot; title=\u0026quot;25,513 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Patricia Shanahan\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment102597274_34394391\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2019-09-25 16:39:23Z, License: CC BY-SA 4.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Sep 25, 2019 at 16:39\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;Since humans use decimal numbers, I see no good reason why the floats are not represented as a decimal by default so we have accurate results.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/1071698/ronen-festinger\u0026quot; title=\u0026quot;2,183 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Ronen Festinger\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment71816044_36074188\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2017-02-19 19:27:48Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Feb 19, 2017 at 19:27\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;Humans use many bases other than base 10 (decimals), binary being the one we use most for computing.. the \u0026apos;good reason\u0026apos; is that you simply cant represent every fraction in every base..\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;span class=\u0026quot;comment-user\u0026quot;\u0026gt;user1641172\u0026lt;/span\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment71832479_36074188\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2017-02-20 08:59:52Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Feb 20, 2017 at 8:59\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;@RonenFestinger binary arithmetic is easy to implement on computers because it requires only eight basic operations with digits: say $a$, $b$ in $0,1$ all you need to know is $\\operatorname{xor}(a,b)$ and $\\operatorname{cb}(a,b)$, where xor is exclusive or and cb is the \u0026quot;carry bit\u0026quot; which is $0$ in all cases except when $a=1=b$, in which case we have one (in fact commutativity of all operations saves you $2$ cases and all you need is $6$ rules). Decimal expansion needs $10\\times 11$ (in decimal notation) cases to be stored and $10$ different states for each bit and wastes storage on the carry.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/1520820/oskar-limka\u0026quot; title=\u0026quot;133 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Oskar Limka\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment85948759_36074188\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2018-03-25 06:36:05Z, License: CC BY-SA 3.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Mar 25, 2018 at 6:36\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        ","\n            \u0026lt;div class=\u0026quot;comment-body js-comment-edit-hide\u0026quot;\u0026gt;\n                \n                \u0026lt;span class=\u0026quot;comment-copy\u0026quot;\u0026gt;@RonenFestinger - Decimal is NOT more accurate.  That is what this answer is saying.  For any base you chose, there will be rational numbers (fractions) that give an infinitely repeating digit sequences.  For the record, some of first computers \u0026lt;i\u0026gt;did\u0026lt;/i\u0026gt; use base 10 representations for numbers, but the pioneering computer hardware designers soon concluded that base 2 was much easier and more efficient to implement.\u0026lt;/span\u0026gt;\n                \n              \u0026lt;div class=\u0026quot;d-inline-flex ai-center\u0026quot;\u0026gt;\n\u0026amp;nbsp;\u0026lt;a href=\u0026quot;/users/139985/stephen-c\u0026quot; title=\u0026quot;672,235 reputation\u0026quot; class=\u0026quot;comment-user\u0026quot;\u0026gt;Stephen C\u0026lt;/a\u0026gt;\n                \u0026lt;/div\u0026gt;\n                \u0026lt;span class=\u0026quot;comment-date\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\u0026lt;a class=\u0026quot;comment-link\u0026quot; href=\u0026quot;#comment100381746_36074188\u0026quot;\u0026gt;\u0026lt;span title=\u0026quot;2019-07-07 05:26:38Z, License: CC BY-SA 4.0\u0026quot; class=\u0026quot;relativetime-clean\u0026quot;\u0026gt;Jul 7, 2019 at 5:26\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt;\n            \u0026lt;/div\u0026gt;\n        "],"id":7,"title":"Is floating point math broken?","content":"\n                \n\u0026lt;p\u0026gt;Consider the following code:\n\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;0.1 + 0.2 == 0.3  -\u0026amp;gt;  false\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;pre class=\u0026quot;lang-js s-code-block\u0026quot;\u0026gt;\u0026lt;code class=\u0026quot;hljs language-javascript\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.1\u0026lt;/span\u0026gt; + \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.2\u0026lt;/span\u0026gt;         -\u0026amp;gt;  \u0026lt;span class=\u0026quot;hljs-number\u0026quot;\u0026gt;0.30000000000000004\u0026lt;/span\u0026gt;\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;Why do these inaccuracies happen?\u0026lt;/p\u0026gt;\n    ","slug":"is-floating-point-math-broken-1657384238910","postType":"QUESTION","createdAt":"2022-07-09T16:30:38.000Z","updatedAt":"2022-07-09T16:30:38.000Z","tags":[{"id":21,"name":"math","slug":"math","createdAt":"2022-07-09T16:30:39.000Z","updatedAt":"2022-07-09T16:30:39.000Z","Questions_Tags":{"questionId":7,"tagId":21}},{"id":22,"name":"floating-accuracy","slug":"floating-accuracy","createdAt":"2022-07-09T16:30:39.000Z","updatedAt":"2022-07-09T16:30:39.000Z","Questions_Tags":{"questionId":7,"tagId":22}},{"id":23,"name":"language-agnostic","slug":"language-agnostic","createdAt":"2022-07-09T16:30:39.000Z","updatedAt":"2022-07-09T16:30:39.000Z","Questions_Tags":{"questionId":7,"tagId":23}},{"id":24,"name":"floating-point","slug":"floating-point","createdAt":"2022-07-09T16:30:39.000Z","updatedAt":"2022-07-09T16:30:39.000Z","Questions_Tags":{"questionId":7,"tagId":24}}]}},"__N_SSG":true},"page":"/questions/[slug]","query":{"slug":"is-floating-point-math-broken-1657384238910"},"buildId":"4Y8KGE-c2lolBeBmcum_I","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>