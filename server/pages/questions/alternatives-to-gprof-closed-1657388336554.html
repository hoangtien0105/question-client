<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@solutionschecker.com"/><meta name="twitter:creator" content="@solutionschecker.com"/><meta property="og:url" content="https://solutionschecker.com"/><meta property="og:type" content="website"/><meta property="og:image" content="https://solutionschecker.com/solutions-checker-banner.png"/><meta property="og:image:alt" content="Find solution for coding, HTML, CSS, JAVASCRIPT, MYSQL, PHP, PYTHON,... quickly. - solutionschecker.com"/><script type="application/ld+json">{"@context":"https://schema.org","@type":"Organization","logo":"/logo.svg","url":"https://solutionschecker.com"}</script><title>Alternatives to gprof [closed] | Solutions Checker</title><meta name="robots" content="index,follow"/><meta name="description" content="
            
        
            
                    
                        
                    
                
                    
                        As it currently stands, this question is not a good fit for our Q&amp;A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.
                        
                    
                
            
                Closed 9 years ago.
        


    

What other programs do the same thing as gprof?
    "/><meta property="og:title" content="Alternatives to gprof [closed] | Solutions Checker"/><meta property="og:description" content="
            
        
            
                    
                        
                    
                
                    
                        As it currently stands, this question is not a good fit for our Q&amp;A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.
                        
                    
                
            
                Closed 9 years ago.
        


    

What other programs do the same thing as gprof?
    "/><script type="application/ld+json">{"@context":"https://schema.org","@type":"QAPage","mainEntity":{"name":"Alternatives to gprof [closed]","text":"\n            \n        \n            \n                    \n                        \n                    \n                \n                    \n                        As it currently stands, this question is not a good fit for our Q&amp;A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.\n                        \n                    \n                \n            \n                Closed 9 years ago.\n        \n\n\n    \n\nWhat other programs do the same thing as gprof?\n    ","answerCount":7,"upVoteCount":500,"suggestedAnswer":[{"text":"gprof (read the paper) exists for historical reasons. \nIf you think it will help you find performance problems, it was never advertised as such.\nHere&apos;s what the paper says:\n\n\n  The prole can be used to compare and assess the costs of\n  various implementations.\n\n\nIt does not say it can be used to identify the various implementations to be assessed, although it does imply that it could, under special circumstances:\n\n\n  especially if small portions of the program are found to dominate its\n  execution time.\n\n\nWhat about problems that are not so localized?\nDo those not matter?\nDon&apos;t place expectations on gprof that were never claimed for it.\nIt is only a measurement tool, and only of CPU-bound operations.\n\nTry this instead.\nHere&apos;s an example of a 44x speedup.\nHere&apos;s a 730x speedup.\nHere&apos;s an 8-minute video demonstration.\nHere&apos;s an explanation of the statistics.\nHere&apos;s an answer to critiques.  \n\nThere&apos;s a simple observation about programs. In a given execution, every instruction is responsible for some fraction of the overall time (especially call instructions), in the sense that if it were not there, the time would not be spent. During that time, the instruction is on the stack **. When that is understood, you can see that -  \n\ngprof embodies certain myths about performance, such as:\n\n\nthat program counter sampling is useful.\nIt is only useful if you have an unnecessary hotspot bottleneck such as a bubble sort of a big array of scalar values. As soon as you, for example, change it into a sort using string-compare, it is still a bottleneck, but program counter sampling will not see it because now the hotspot is in string-compare. On the other hand if it were to sample the extended program counter (the call stack), the point at which the string-compare is called, the sort loop, is clearly displayed. In fact, gprof was an attempt to remedy the limitations of pc-only sampling.\nthat timing functions is more important than capturing time-consuming lines of code.\nThe reason for that myth is that gprof was not able to capture stack samples, so instead it times functions, counts their invocations, and tries to capture the call graph. However, once a costly function is identified, you still need to look inside it for the lines that are responsible for the time. If there were stack samples you would not need to look, those lines would be on the samples. (A typical function might have 100 - 1000 instructions. A function call is 1 instruction, so something that locates costly calls is 2-3 orders of magnitude more precise.)\nthat the call graph is important.\nWhat you need to know about a program is not where it spends its time, but why. When it is spending time in a function, every line of code on the stack gives one link in the chain of reasoning of why it is there. If you can only see part of the stack, you can only see part of the reason why, so you can&apos;t tell for sure if that time is actually necessary.\nWhat does the call graph tell you? Each arc tells you that some function A was in the process of calling some function B for some fraction of the time. Even if A has only one such line of code calling B, that line only gives a small part of the reason why. If you are lucky enough, maybe that line has a poor reason. Usually, you need to see multiple simultaneous lines to find a poor reason if it is there. If A calls B in more than one place, then it tells you even less.\nthat recursion is a tricky confusing issue.\nThat is only because gprof and other profilers perceive a need to generate a call-graph and then attribute times to the nodes. If one has samples of the stack, the time-cost of each line of code that appears on samples is a very simple number - the fraction of samples it is on. If there is recursion, then a given line can appear more than once on a sample. \nNo matter. Suppose samples are taken every N ms, and the line appears on F% of them (singly or not). If that line can be made to take no time (such as by deleting it or branching around it), then those samples would disappear, and the time would be reduced by F%.\nthat accuracy of time measurement (and therefore a large number of samples) is important.\nThink about it for a second. If a line of code is on 3 samples out of five, then if you could shoot it out like a light bulb, that is roughly 60% less time that would be used. Now, you know that if you had taken a different 5 samples, you might have only seen it 2 times, or as many as 4. So that 60% measurement is more like a general range from 40% to 80%. If it were only 40%, would you say the problem is not worth fixing? So what&apos;s the point of time accuracy, when what you really want is to find the problems?\n500 or 5000 samples would have measured the problem with greater precision, but would not have found it any more accurately.\nthat counting of statement or function invocations is useful.\nSuppose you know a function has been called 1000 times. Can you tell from that what fraction of time it costs? You also need to know how long it takes to run, on average, multiply it by the count, and divide by the total time. The average invocation time could vary from nanoseconds to seconds, so the count alone doesn&apos;t tell much. If there are stack samples, the cost of a routine or of any statement is just the fraction of samples it is on. That fraction of time is what could in principle be saved overall if the routine or statement could be made to take no time, so that is what has the most direct relationship to performance.\nthat samples need not be taken when blocked\nThe reasons for this myth are twofold: 1) that PC sampling is meaningless when the program is waiting, and 2) the preoccupation with accuracy of timing. However, for (1) the program may very well be waiting for something that it asked for, such as file I/O, which you need to know, and which stack samples reveal. (Obviously you want to exclude samples while waiting for user input.) For (2) if the program is waiting simply because of competition with other processes, that presumably happens in a fairly random way while it&apos;s running.\nSo while the program may be taking longer, that will not have a large effect on the statistic that matters, the percentage of time that statements are on the stack.\nthat &quot;self time&quot; matters\nSelf time only makes sense if you are measuring at the function level, not line level, and you think you need help in discerning if the function time goes into purely local computation versus in called routines. If summarizing at the line level, a line represents self time if it is at the end of the stack, otherwise it represents inclusive time. Either way, what it costs is the percentage of stack samples it is on, so that locates it for you in either case.\nthat samples have to be taken at high frequency\nThis comes from the idea that a performance problem may be fast-acting, and that samples have to be frequent in order to hit it. But, if the problem is costing, 20%, say, out of a total running time of 10 sec (or whatever), then each sample in that total time will have a 20% chance of hitting it, no matter if the problem occurs in a single piece like this\n.....XXXXXXXX...........................\n.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^ (20 samples, 4 hits)\nor in many small pieces like this\nX...X...X.X..X.........X.....X....X.....\n.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^ (20 samples, 3 hits)\nEither way, the number of hits will average about 1 in 5, no matter how many samples are taken, or how few. (Average = 20 * 0.2 = 4. Standard deviation = +/- sqrt(20 * 0.2 * 0.8) = 1.8.)\nthat you are trying to find the bottleneck\nas if there were only one. Consider the following execution timeline: vxvWvzvWvxvWvYvWvxvWv.vWvxvWvYvW\nIt consists of real useful work, represented by .. There are performance problems vWxYz taking 1/2, 1/4, 1/8, 1/16, 1/32 of the time, respectively. Sampling finds v easily. It is removed, leaving\nxWzWxWYWxW.WxWYW\nNow the program takes half as long to run, and now W takes half the time, and is found easily. It is removed, leaving\nxzxYx.xY\nThis process continues, each time removing the biggest, by percentage, performance problem, until nothing to remove can be found. Now the only thing executed is ., which executes in 1/32 of the time used by the original program. This is the magnification effect, by which removing any problem makes the remainder larger, by percent, because the denominator is reduced.\nAnother crucial point is that every single problem must be found - missing none of the 5. Any problem not found and fixed severely reduces the final speedup ratio. Just finding some, but not all, is not &quot;good enough&quot;.\n\n\nADDED: I would just like to point out one reason why gprof is popular - it is being taught,\npresumably because it&apos;s free, easy to teach, and it&apos;s been around a long time.\nA quick Google search locates some academic institutions that teach it (or appear to):\n\n\n  berkeley bu clemson\n  colorado duke earlham fsu indiana mit msu\n  ncsa.illinois ncsu nyu ou princeton psu\n  stanford ucsd umd umich utah utexas utk wustl\n\n\n** With the exception of other ways of requesting work to be done, that don&apos;t leave a trace telling why, such as by message posting.\n    ","url":"/questions/[slug]#solution1","@type":"Answer","upvoteCount":0},{"text":"Valgrind has an instruction-count profiler with a very nice visualizer called KCacheGrind.  As Mike Dunlavey recommends, Valgrind counts the fraction of instructions for which a procedure is live on the stack, although I&apos;m sorry to say it appears to become confused in the presence of mutual recursion.  But the visualizer is very nice and light years ahead of gprof.\n    ","url":"/questions/[slug]#solution2","@type":"Answer","upvoteCount":0},{"text":"Since I did&apos;t see here anything about perf which is a relatively new tool for profiling the kernel and user applications on Linux I decided to add this information.\n\nFirst of all - this is a tutorial about Linux profiling with perf \n\nYou can use perf if your Linux Kernel is greater than 2.6.32 or oprofile if it is older. Both programs don&apos;t require from you to instrument your program (like gprof requires). However in order to get call graph correctly in perf you need to build you program with -fno-omit-frame-pointer. For example: g++ -fno-omit-frame-pointer -O2 main.cpp.\n\nYou can see &quot;live&quot; analysis of your application with perf top: \n\nsudo perf top -p `pidof a.out` -K\n\n\nOr you can record performance data of a running application and analyze them after that:\n\n1) To record performance data:\n\nperf record -p `pidof a.out`\n\n\nor  to record for 10 secs:\n\nperf record -p `pidof a.out` sleep 10\n\n\nor to record with call graph ()\n\nperf record -g -p `pidof a.out` \n\n\n2) To analyze the recorded data\n\nperf report --stdio\nperf report --stdio --sort=dso -g none\nperf report --stdio -g none\nperf report --stdio -g\n\n\nOr you can record performace data of a application and analyze them after that just by launching the application in this way and waiting for it to exit:\n\nperf record ./a.out\n\n\nThis is an example of profiling a test program \n\nThe test program is in file main.cpp (I will put main.cpp at the bottom of the message):\n\nI compile it in this way:\n\ng++ -m64 -fno-omit-frame-pointer -g main.cpp -L.  -ltcmalloc_minimal -o my_test\n\n\nI use libmalloc_minimial.so since it is compiled with -fno-omit-frame-pointer while libc malloc seems to be compiled without this option.\nThen I run my test program\n\n./my_test 100000000 \n\n\nThen I record performance data of a running process:\n\nperf record -g  -p `pidof my_test` -o ./my_test.perf.data sleep 30\n\n\nThen I analyze load per module:\n\n\n  perf report --stdio  -g none --sort comm,dso  -i ./my_test.perf.data\n\n\n# Overhead  Command                 Shared Object\n# ........  .......  ............................\n#\n    70.06%  my_test  my_test\n    28.33%  my_test  libtcmalloc_minimal.so.0.1.0\n     1.61%  my_test  [kernel.kallsyms]\n\n\nThen load per function is analyzed:\n\n\n  perf report --stdio  -g none  -i ./my_test.perf.data | c++filt\n\n\n# Overhead  Command                 Shared Object                       Symbol\n# ........  .......  ............................  ...........................\n#\n    29.30%  my_test  my_test                       [.] f2(long)\n    29.14%  my_test  my_test                       [.] f1(long)\n    15.17%  my_test  libtcmalloc_minimal.so.0.1.0  [.] operator new(unsigned long)\n    13.16%  my_test  libtcmalloc_minimal.so.0.1.0  [.] operator delete(void*)\n     9.44%  my_test  my_test                       [.] process_request(long)\n     1.01%  my_test  my_test                       [.] operator delete(void*)@plt\n     0.97%  my_test  my_test                       [.] operator new(unsigned long)@plt\n     0.20%  my_test  my_test                       [.] main\n     0.19%  my_test  [kernel.kallsyms]             [k] apic_timer_interrupt\n     0.16%  my_test  [kernel.kallsyms]             [k] _spin_lock\n     0.13%  my_test  [kernel.kallsyms]             [k] native_write_msr_safe\n\n     and so on ...\n\n\nThen call chains are analyzed:\n\n\n  perf report --stdio  -g graph  -i ./my_test.perf.data | c++filt\n\n\n# Overhead  Command                 Shared Object                       Symbol\n# ........  .......  ............................  ...........................\n#\n    29.30%  my_test  my_test                       [.] f2(long)\n            |\n            --- f2(long)\n               |\n                --29.01%-- process_request(long)\n                          main\n                          __libc_start_main\n\n    29.14%  my_test  my_test                       [.] f1(long)\n            |\n            --- f1(long)\n               |\n               |--15.05%-- process_request(long)\n               |          main\n               |          __libc_start_main\n               |\n                --13.79%-- f2(long)\n                          process_request(long)\n                          main\n                          __libc_start_main\n\n    15.17%  my_test  libtcmalloc_minimal.so.0.1.0  [.] operator new(unsigned long)\n            |\n            --- operator new(unsigned long)\n               |\n               |--11.44%-- f1(long)\n               |          |\n               |          |--5.75%-- process_request(long)\n               |          |          main\n               |          |          __libc_start_main\n               |          |\n               |           --5.69%-- f2(long)\n               |                     process_request(long)\n               |                     main\n               |                     __libc_start_main\n               |\n                --3.01%-- process_request(long)\n                          main\n                          __libc_start_main\n\n    13.16%  my_test  libtcmalloc_minimal.so.0.1.0  [.] operator delete(void*)\n            |\n            --- operator delete(void*)\n               |\n               |--9.13%-- f1(long)\n               |          |\n               |          |--4.63%-- f2(long)\n               |          |          process_request(long)\n               |          |          main\n               |          |          __libc_start_main\n               |          |\n               |           --4.51%-- process_request(long)\n               |                     main\n               |                     __libc_start_main\n               |\n               |--3.05%-- process_request(long)\n               |          main\n               |          __libc_start_main\n               |\n                --0.80%-- f2(long)\n                          process_request(long)\n                          main\n                          __libc_start_main\n\n     9.44%  my_test  my_test                       [.] process_request(long)\n            |\n            --- process_request(long)\n               |\n                --9.39%-- main\n                          __libc_start_main\n\n     1.01%  my_test  my_test                       [.] operator delete(void*)@plt\n            |\n            --- operator delete(void*)@plt\n\n     0.97%  my_test  my_test                       [.] operator new(unsigned long)@plt\n            |\n            --- operator new(unsigned long)@plt\n\n     0.20%  my_test  my_test                       [.] main\n     0.19%  my_test  [kernel.kallsyms]             [k] apic_timer_interrupt\n     0.16%  my_test  [kernel.kallsyms]             [k] _spin_lock\n     and so on ...\n\n\nSo at this point you know where your program spends time.\n\nAnd this is main.cpp for the test:\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;time.h&gt;\n\ntime_t f1(time_t time_value)\n{\n  for (int j =0; j &lt; 10; ++j) {\n    ++time_value;\n    if (j%5 == 0) {\n      double *p = new double;\n      delete p;\n    }\n  }\n  return time_value;\n}\n\ntime_t f2(time_t time_value)\n{\n  for (int j =0; j &lt; 40; ++j) {\n    ++time_value;\n  }\n  time_value=f1(time_value);\n  return time_value;\n}\n\ntime_t process_request(time_t time_value)\n{\n\n  for (int j =0; j &lt; 10; ++j) {\n    int *p = new int;\n    delete p;\n    for (int m =0; m &lt; 10; ++m) {\n      ++time_value;\n    }\n  }\n  for (int i =0; i &lt; 10; ++i) {\n    time_value=f1(time_value);\n    time_value=f2(time_value);\n  }\n  return time_value;\n}\n\nint main(int argc, char* argv2[])\n{\n  int number_loops = argc &gt; 1 ? atoi(argv2[1]) : 1;\n  time_t time_value = time(0);\n  printf(&quot;number loops %d\\n&quot;, number_loops);\n  printf(&quot;time_value: %d\\n&quot;, time_value );\n\n  for (int i =0; i &lt; number_loops; ++i) {\n    time_value = process_request(time_value);\n  }\n  printf(&quot;time_value: %ld\\n&quot;, time_value );\n  return 0;\n}\n\n    ","url":"/questions/[slug]#solution3","@type":"Answer","upvoteCount":0},{"text":"Try OProfile. It is a much better tool for profiling your code. I would also suggest Intel VTune.\n\nThe two tools above can narrow down time spent in a particular line of code, annotate your code, show assembly and how much particular instruction takes.  Beside time metric, you can also query specific counters, i.e. cache hits, etc.\n\nUnlike gprof, you can profile any process/binary running on your system using either of the two.\n    ","url":"/questions/[slug]#solution4","@type":"Answer","upvoteCount":0},{"text":"Google performance tools include a simple to use profiler. CPU as well as heap profiler is available.\n    ","url":"/questions/[slug]#solution5","@type":"Answer","upvoteCount":0},{"text":"Take a look at Sysprof.\n\nYour distribution may have it already.\n    ","url":"/questions/[slug]#solution6","@type":"Answer","upvoteCount":0},{"text":"http://lttng.org/ if you want a high performance tracer\n    ","url":"/questions/[slug]#solution7","@type":"Answer","upvoteCount":0}],"@type":"Question"}}</script><meta name="next-head-count" content="16"/><link rel="preload" href="/_next/static/css/08bcc42a26fe5c92.css" as="style"/><link rel="stylesheet" href="/_next/static/css/08bcc42a26fe5c92.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-0d1b80a048d4787e.js"></script><script src="/_next/static/chunks/webpack-42cdea76c8170223.js" defer=""></script><script src="/_next/static/chunks/framework-4556c45dd113b893.js" defer=""></script><script src="/_next/static/chunks/main-25e5079ab4bd6ecd.js" defer=""></script><script src="/_next/static/chunks/pages/_app-08d1a634dea6705e.js" defer=""></script><script src="/_next/static/chunks/29107295-fbcfe2172188e46f.js" defer=""></script><script src="/_next/static/chunks/150-12e9794e898cd5f3.js" defer=""></script><script src="/_next/static/chunks/490-7f0418bb4354ac73.js" defer=""></script><script src="/_next/static/chunks/108-87de33c23337ff53.js" defer=""></script><script src="/_next/static/chunks/pages/questions/%5Bslug%5D-928c5d1eb8fb0bba.js" defer=""></script><script src="/_next/static/BnkbnjkWYxefPXjTJLVVv/_buildManifest.js" defer=""></script><script src="/_next/static/BnkbnjkWYxefPXjTJLVVv/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="wrapper"><header><nav class="bg-white border-gray-200 px-4 lg:px-6 py-2.5 dark:bg-gray-800"><div class="flex flex-wrap justify-between items-center mx-auto max-w-screen-xl"><a class="flex items-center" href="/"><img src="/logo-second.png" class="mr-3 h-6 sm:h-9" alt="Solution Checker Logo"/><h4 class="self-center text-xl font-semibold whitespace-nowrap dark:text-white">Solution Checker</h4></a><div class="flex items-center lg:order-2"><button data-collapse-toggle="mobile-menu-2" type="button" class="inline-flex items-center p-2 ml-1 text-sm text-gray-500 rounded-lg lg:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200 dark:text-gray-400 dark:hover:bg-gray-700 dark:focus:ring-gray-600" aria-controls="mobile-menu-2" aria-expanded="false"><span class="sr-only">Open main menu</span><svg class="w-6 h-6" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 10a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 15a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z" clip-rule="evenodd"></path></svg><svg class="hidden w-6 h-6" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M4.293 4.293a1 1 0 011.414 0L10 8.586l4.293-4.293a1 1 0 111.414 1.414L11.414 10l4.293 4.293a1 1 0 01-1.414 1.414L10 11.414l-4.293 4.293a1 1 0 01-1.414-1.414L8.586 10 4.293 5.707a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></button></div><div class="hidden justify-between items-center w-full lg:flex lg:w-auto lg:order-1" id="mobile-menu-2"><ul class="flex flex-col mt-4 font-medium lg:flex-row lg:space-x-8 lg:mt-0"><li><a class="block py-2 pr-4 pl-3 text-gray-700 border-b border-gray-100 hover:bg-gray-50 lg:hover:bg-transparent lg:border-0 lg:hover:text-blue-700 lg:p-0 dark:text-gray-400 lg:dark:hover:text-white dark:hover:bg-gray-700 dark:hover:text-white lg:dark:hover:bg-transparent dark:border-gray-700" aria-current="page" href="/">Home</a></li><li><a class="block py-2 pr-4 pl-3 text-gray-700 border-b border-gray-100 hover:bg-gray-50 lg:hover:bg-transparent lg:border-0 lg:hover:text-blue-700 lg:p-0 dark:text-gray-400 lg:dark:hover:text-white dark:hover:bg-gray-700 dark:hover:text-white lg:dark:hover:bg-transparent dark:border-gray-700" href="/questions?tab=news">Questions</a></li><li><a class="block py-2 pr-4 pl-3 text-gray-700 border-b border-gray-100 hover:bg-gray-50 lg:hover:bg-transparent lg:border-0 lg:hover:text-blue-700 lg:p-0 dark:text-gray-400 lg:dark:hover:text-white dark:hover:bg-gray-700 dark:hover:text-white lg:dark:hover:bg-transparent dark:border-gray-700" href="/post?tab=news">Post</a></li><li><a class="block py-2 pr-4 pl-3 text-gray-700 border-b border-gray-100 hover:bg-gray-50 lg:hover:bg-transparent lg:border-0 lg:hover:text-blue-700 lg:p-0 dark:text-gray-400 lg:dark:hover:text-white dark:hover:bg-gray-700 dark:hover:text-white lg:dark:hover:bg-transparent dark:border-gray-700" href="/questions/alternatives-to-gprof-closed-1657388336554#">Coding</a></li></ul></div></div></nav></header><div class="main-content"><div class="question my-5"><div class="flex question-header items-center m-auto justify-center"><div class="rounded-xl w-full border p-5 shadow-md bg-white"><div class="flex w-full items-center justify-between border-b pb-3"><div class="flex items-center space-x-3"><div class="text-lg font-bold text-slate-700"><a href="/questions/alternatives-to-gprof-closed-1657388336554"><h1>Alternatives to gprof [closed]</h1></a></div></div><div class="flex flex-wrap h-auto justify-end items-center space-x-8"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold" href="/questions/tag/profiler">profiler</a><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold" href="/questions/tag/gprof">gprof</a></div></div><div class="question-content mt-5">
                    <div>
            <aside class="s-notice s-notice__info post-notice js-post-notice mb16" role="status">
        <div class="d-flex fd-column fw-nowrap">
            <div class="d-flex fw-nowrap">
                    <div class="flex--item mr8">
                        <svg aria-hidden="true" class="svg-icon iconLightbulb" width="18" height="18" viewBox="0 0 18 18"><path d="M15 6.38A6.48 6.48 0 0 0 7.78.04h-.02A6.49 6.49 0 0 0 2.05 5.6a6.31 6.31 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.32 6.32 0 0 0 15 6.37ZM4.03 5.85A4.49 4.49 0 0 1 8 2.02a4.48 4.48 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.32 4.32 0 0 1-1.64-3.94ZM10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2h3Z"></path></svg>
                    </div>
                <div class="flex--item wmn0 fl1 lh-lg">
                    <div class="flex--item fl1 lh-lg">
                        As it currently stands, this question is not a good fit for our Q&amp;A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, <a href="/help/reopen-questions">visit the help center</a> for guidance.
                        
                    </div>
                </div>
            </div>
                <div class="flex--item mb0 mt8">Closed <span title="2013-01-05 06:46:45Z" class="relativetime">9 years ago</span>.</div>
        </div>
</aside>

    </div>

<p>What other programs do the same thing as gprof?</p>
    </div></div></div><div class="solution-section"><nav class="flex pagination-solution flex-col justify-end"><ul class="inline-flex -space-x-px overflow-auto"><li class="pagination-solution-item"><span data-id="#solution1" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">1</span></li><li class="pagination-solution-item"><span data-id="#solution2" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">2</span></li><li class="pagination-solution-item"><span data-id="#solution3" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">3</span></li><li class="pagination-solution-item"><span data-id="#solution4" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">4</span></li><li class="pagination-solution-item"><span data-id="#solution5" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">5</span></li><li class="pagination-solution-item"><span data-id="#solution6" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">6</span></li><li class="pagination-solution-item"><span data-id="#solution7" class="cursor-pointer py-2 px-3 leading-tight text-gray-500 bg-white border border-gray-300 hover:bg-gray-100 hover:text-gray-700 dark:bg-gray-800 dark:border-gray-700 dark:text-gray-400 dark:hover:bg-gray-700 dark:hover:text-white">7</span></li></ul></nav><div id="solution1" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 1</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiler">profiler</a><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/gprof">gprof</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p><strong>gprof</strong> <a href="http://docs.freebsd.org/44doc/psd/18.gprof/paper.pdf" rel="noreferrer">(read the paper)</a> exists for historical reasons. 
If you think it will help you find performance problems, it was never advertised as such.
Here's what the paper says:</p>

<blockquote>
  <p>The prole can be used to compare and assess the costs of
  various implementations.</p>
</blockquote>

<p>It does not say it can be used to <em>identify</em> the various implementations to be assessed, although it does <em>imply</em> that it could, under special circumstances:</p>

<blockquote>
  <p>especially if small portions of the program are found to dominate its
  execution time.</p>
</blockquote>

<p>What about problems that are not so localized?
Do those not matter?
Don't place expectations on <strong>gprof</strong> that were never claimed for it.
It is <em>only</em> a measurement tool, and only of CPU-bound operations.</p>

<p><a href="https://stackoverflow.com/questions/375913/what-can-i-use-to-profile-c-code-in-linux/378024#378024">Try this instead.</a><br>
<a href="https://stackoverflow.com/questions/926266/performance-optimization-strategies-of-last-resort/927773#927773">Here's an example of a 44x speedup.</a><br>
<a href="https://scicomp.stackexchange.com/a/1870/1262">Here's a 730x speedup.</a><br>
<a href="http://youtu.be/xPg3sRpdW1U" rel="noreferrer">Here's an 8-minute video demonstration.</a><br>
<a href="https://scicomp.stackexchange.com/a/2719/1262">Here's an explanation of the statistics.</a><br>
<a href="https://stackoverflow.com/a/18217639/23771">Here's an answer to critiques.</a>  </p>

<p>There's a simple observation about programs. In a given execution, every instruction is responsible for some fraction of the overall time (especially <code>call</code> instructions), in the sense that if it were not there, the time would not be spent. During that time, the instruction is on the stack **. When that is understood, you can see that -  </p>

<p><strong>gprof</strong> embodies certain myths about performance, such as:</p>

<ol>
<li><p><em>that program counter sampling is useful.</em><br>
It is only useful if you have an unnecessary hotspot bottleneck such as a bubble sort of a big array of scalar values. As soon as you, for example, change it into a sort using string-compare, it is still a bottleneck, but program counter sampling will not see it because now the hotspot is in string-compare. On the other hand if it were to sample the <strong>extended</strong> program counter (the call stack), the point at which the string-compare is called, the sort loop, is clearly displayed. <em>In fact, <strong>gprof</strong> was an attempt to remedy the limitations of pc-only sampling.</em></p></li>
<li><p><em>that timing functions is more important than capturing time-consuming lines of code.</em><br>
The reason for that myth is that <strong>gprof</strong> was not able to capture stack samples, so instead it times functions, counts their invocations, and tries to capture the call graph. However, once a costly function is identified, you still need to look inside it for the lines that are responsible for the time. If there were stack samples you would not need to look, those lines would be on the samples. (A typical function might have 100 - 1000 instructions. A function <em>call</em> is 1 instruction, so something that locates costly calls is 2-3 orders of magnitude more precise.)</p></li>
<li><p><em>that the call graph is important.</em><br>
What you need to know about a program is not <strong>where</strong> it spends its time, but <strong>why</strong>. When it is spending time in a function, every line of code on the stack gives one link in the chain of reasoning of why it is there. If you can only see part of the stack, you can only see part of the reason why, so you can't tell for sure if that time is actually necessary.
What does the call graph tell you? Each arc tells you that some function A was in the process of calling some function B for some fraction of the time. Even if A has only one such line of code calling B, that line only gives a small part of the reason why. If you are lucky enough, maybe that line has a poor reason. Usually, you need to see multiple simultaneous lines to find a poor reason if it is there. If A calls B in more than one place, then it tells you even less.</p></li>
<li><p><em>that recursion is a tricky confusing issue.</em><br>
That is only because <strong>gprof</strong> and other profilers perceive a need to generate a call-graph and then attribute times to the nodes. If one has samples of the stack, the time-cost of each line of code that appears on samples is a very simple number - the fraction of samples it is on. If there is recursion, then a given line can appear more than once on a sample. 
<em>No matter.</em> Suppose samples are taken every N ms, and the line appears on F% of them (singly or not). If that line can be made to take no time (such as by deleting it or branching around it), then those samples would <em>disappear</em>, and the time would be reduced by F%.</p></li>
<li><p><em>that accuracy of time measurement (and therefore a large number of samples) is important.</em><br>
Think about it for a second. If a line of code is on 3 samples out of five, then if you could shoot it out like a light bulb, that is roughly 60% less time that would be used. Now, you know that if you had taken a different 5 samples, you might have only seen it 2 times, or as many as 4. So that 60% measurement is more like a general range from 40% to 80%. If it were only 40%, would you say the problem is not worth fixing? So what's the point of time accuracy, when what you really want is to <strong>find the problems</strong>?
500 or 5000 samples would have measured the problem with greater precision, but would not have found it any more accurately.</p></li>
<li><p><em>that counting of statement or function invocations is useful.</em><br>
Suppose you know a function has been called 1000 times. Can you tell from that what fraction of time it costs? You also need to know how long it takes to run, on average, multiply it by the count, and divide by the total time. The average invocation time could vary from nanoseconds to seconds, so the count alone doesn't tell much. If there are stack samples, the cost of a routine or of any statement is just the fraction of samples it is on. That fraction of time is what could in principle be saved overall if the routine or statement could be made to take no time, so that is what has the most direct relationship to performance.</p></li>
<li><p><em>that samples need not be taken when blocked</em><br>
The reasons for this myth are twofold: 1) that PC sampling is meaningless when the program is waiting, and 2) the preoccupation with accuracy of timing. However, for (1) the program may very well be waiting for something that it asked for, such as file I/O, which you <em>need to know</em>, and which stack samples reveal. (Obviously you want to exclude samples while waiting for user input.) For (2) if the program is waiting simply because of competition with other processes, that presumably happens in a fairly random way while it's running.
So while the program may be taking longer, that will not have a large effect on the statistic that matters, the percentage of time that statements are on the stack.</p></li>
<li><p><em>that "self time" matters</em><br>
Self time only makes sense if you are measuring at the function level, not line level, and you think you need help in discerning if the function time goes into purely local computation versus in called routines. If summarizing at the line level, a line represents self time if it is at the end of the stack, otherwise it represents inclusive time. Either way, what it costs is the percentage of stack samples it is on, so that locates it for you in either case.</p></li>
<li><p><em>that samples have to be taken at high frequency</em><br>
This comes from the idea that a performance problem may be fast-acting, and that samples have to be frequent in order to hit it. But, if the problem is costing, 20%, say, out of a total running time of 10 sec (or whatever), then each sample in that total time will have a 20% chance of hitting it, no matter if the problem occurs in a single piece like this<br>
<code>.....XXXXXXXX...........................</code><br>
<code>.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^</code> (20 samples, 4 hits)<br>
or in many small pieces like this<br>
<code>X...X...X.X..X.........X.....X....X.....</code><br>
<code>.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^</code> (20 samples, 3 hits)<br>
Either way, the number of hits will average about 1 in 5, no matter how many samples are taken, or how few. (Average = 20 * 0.2 = 4. Standard deviation = +/- sqrt(20 * 0.2 * 0.8) = 1.8.)</p></li>
<li><p><em>that you are trying to find <strong>the</strong> bottleneck</em><br>
as if there were only one. Consider the following execution timeline: <code>vxvWvzvWvxvWvYvWvxvWv.vWvxvWvYvW</code><br>
It consists of real useful work, represented by <code>.</code>. There are performance problems <code>vWxYz</code> taking 1/2, 1/4, 1/8, 1/16, 1/32 of the time, respectively. Sampling finds <code>v</code> easily. It is removed, leaving<br>
<code>xWzWxWYWxW.WxWYW</code><br>
Now the program takes half as long to run, and now <code>W</code> takes half the time, and is found easily. It is removed, leaving<br>
<code>xzxYx.xY</code><br>
This process continues, each time removing the biggest, by percentage, performance problem, until nothing to remove can be found. Now the only thing executed is <code>.</code>, which executes in 1/32 of the time used by the original program. This is the <em>magnification effect</em>, by which removing any problem makes the remainder larger, by percent, because the denominator is reduced.<br>
Another crucial point is that <em>every single problem must be found</em> - missing none of the 5. Any problem not found and fixed severely reduces the final speedup ratio. Just finding some, but not all, is not "good enough".</p></li>
</ol>

<p>ADDED: I would just like to point out one reason why <em>gprof</em> is popular - it is being taught,
presumably because it's free, easy to teach, and it's been around a long time.
A quick Google search locates some academic institutions that teach it (or appear to):</p>

<blockquote>
  <p>berkeley bu clemson
  colorado duke earlham fsu indiana mit msu
  ncsa.illinois ncsu nyu ou princeton psu
  stanford ucsd umd umich utah utexas utk wustl</p>
</blockquote>

<p>** With the exception of other ways of requesting work to be done, that don't leave a trace telling <em>why</em>, such as by message posting.</p>
    </div></div></div></div><div id="solution2" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 2</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiler">profiler</a><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/gprof">gprof</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p><a href="http://en.wikipedia.org/wiki/Valgrind" rel="noreferrer">Valgrind</a> has an instruction-count profiler with a very nice visualizer called <a href="http://en.wikipedia.org/wiki/Valgrind#Tools" rel="noreferrer">KCacheGrind</a>.  As Mike Dunlavey recommends, Valgrind counts the fraction of instructions for which a procedure is live on the stack, although I'm sorry to say it appears to become confused in the presence of mutual recursion.  But the visualizer is very nice and light years ahead of <code>gprof</code>.</p>
    </div></div></div></div><div id="solution3" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 3</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiler">profiler</a><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/gprof">gprof</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Since I did't see here anything about <strong><code>perf</code></strong> which is a relatively new tool for profiling the kernel and user applications on Linux I decided to add this information.</p>

<p>First of all - this is a tutorial about <a href="https://perf.wiki.kernel.org/index.php/Tutorial" rel="noreferrer">Linux profiling with <code>perf</code> </a></p>

<p>You can use <code>perf</code> if your Linux Kernel is greater than 2.6.32 or <code>oprofile</code> if it is older. Both programs don't require from you to instrument your program (like <code>gprof</code> requires). However in order to get call graph correctly in <code>perf</code> you need to build you program with <code>-fno-omit-frame-pointer</code>. For example: <code>g++ -fno-omit-frame-pointer -O2 main.cpp</code>.</p>

<p>You can see "live" analysis of your application with <code>perf top</code>: <br></p>

<pre><code>sudo perf top -p `pidof a.out` -K
</code></pre>

<p>Or you can record performance data of a running application and analyze them after that:</p>

<p>1) To record performance data:<br></p>

<pre><code>perf record -p `pidof a.out`
</code></pre>

<p>or  to record for 10 secs:</p>

<pre><code>perf record -p `pidof a.out` sleep 10
</code></pre>

<p>or to record with call graph ()</p>

<pre><code>perf record -g -p `pidof a.out` 
</code></pre>

<p>2) To analyze the recorded data</p>

<pre><code>perf report --stdio
perf report --stdio --sort=dso -g none
perf report --stdio -g none
perf report --stdio -g
</code></pre>

<p>Or you can record performace data of a application and analyze them after that just by launching the application in this way and waiting for it to exit:</p>

<pre><code>perf record ./a.out
</code></pre>

<p><strong>This is an example of profiling a test program</strong> </p>

<p>The test program is in file main.cpp (I will put main.cpp at the bottom of the message):</p>

<p>I compile it in this way:</p>

<pre><code>g++ -m64 -fno-omit-frame-pointer -g main.cpp -L.  -ltcmalloc_minimal -o my_test
</code></pre>

<p>I use <code>libmalloc_minimial.so</code> since it is compiled with <code>-fno-omit-frame-pointer</code> while libc malloc seems to be compiled without this option.
Then I run my test program</p>

<pre><code>./my_test 100000000 
</code></pre>

<p>Then I record performance data of a running process:</p>

<pre><code>perf record -g  -p `pidof my_test` -o ./my_test.perf.data sleep 30
</code></pre>

<p>Then I analyze load per module:</p>

<blockquote>
  <p>perf report --stdio  -g none --sort comm,dso  -i ./my_test.perf.data</p>
</blockquote>

<pre><code># Overhead  Command                 Shared Object
# ........  .......  ............................
#
    70.06%  my_test  my_test
    28.33%  my_test  libtcmalloc_minimal.so.0.1.0
     1.61%  my_test  [kernel.kallsyms]
</code></pre>

<p>Then load per function is analyzed:</p>

<blockquote>
  <p>perf report --stdio  -g none  -i ./my_test.perf.data | c++filt</p>
</blockquote>

<pre><code># Overhead  Command                 Shared Object                       Symbol
# ........  .......  ............................  ...........................
#
    29.30%  my_test  my_test                       [.] f2(long)
    29.14%  my_test  my_test                       [.] f1(long)
    15.17%  my_test  libtcmalloc_minimal.so.0.1.0  [.] operator new(unsigned long)
    13.16%  my_test  libtcmalloc_minimal.so.0.1.0  [.] operator delete(void*)
     9.44%  my_test  my_test                       [.] process_request(long)
     1.01%  my_test  my_test                       [.] operator delete(void*)@plt
     0.97%  my_test  my_test                       [.] operator new(unsigned long)@plt
     0.20%  my_test  my_test                       [.] main
     0.19%  my_test  [kernel.kallsyms]             [k] apic_timer_interrupt
     0.16%  my_test  [kernel.kallsyms]             [k] _spin_lock
     0.13%  my_test  [kernel.kallsyms]             [k] native_write_msr_safe

     and so on ...
</code></pre>

<p>Then call chains are analyzed:</p>

<blockquote>
  <p>perf report --stdio  -g graph  -i ./my_test.perf.data | c++filt</p>
</blockquote>

<pre><code># Overhead  Command                 Shared Object                       Symbol
# ........  .......  ............................  ...........................
#
    29.30%  my_test  my_test                       [.] f2(long)
            |
            --- f2(long)
               |
                --29.01%-- process_request(long)
                          main
                          __libc_start_main

    29.14%  my_test  my_test                       [.] f1(long)
            |
            --- f1(long)
               |
               |--15.05%-- process_request(long)
               |          main
               |          __libc_start_main
               |
                --13.79%-- f2(long)
                          process_request(long)
                          main
                          __libc_start_main

    15.17%  my_test  libtcmalloc_minimal.so.0.1.0  [.] operator new(unsigned long)
            |
            --- operator new(unsigned long)
               |
               |--11.44%-- f1(long)
               |          |
               |          |--5.75%-- process_request(long)
               |          |          main
               |          |          __libc_start_main
               |          |
               |           --5.69%-- f2(long)
               |                     process_request(long)
               |                     main
               |                     __libc_start_main
               |
                --3.01%-- process_request(long)
                          main
                          __libc_start_main

    13.16%  my_test  libtcmalloc_minimal.so.0.1.0  [.] operator delete(void*)
            |
            --- operator delete(void*)
               |
               |--9.13%-- f1(long)
               |          |
               |          |--4.63%-- f2(long)
               |          |          process_request(long)
               |          |          main
               |          |          __libc_start_main
               |          |
               |           --4.51%-- process_request(long)
               |                     main
               |                     __libc_start_main
               |
               |--3.05%-- process_request(long)
               |          main
               |          __libc_start_main
               |
                --0.80%-- f2(long)
                          process_request(long)
                          main
                          __libc_start_main

     9.44%  my_test  my_test                       [.] process_request(long)
            |
            --- process_request(long)
               |
                --9.39%-- main
                          __libc_start_main

     1.01%  my_test  my_test                       [.] operator delete(void*)@plt
            |
            --- operator delete(void*)@plt

     0.97%  my_test  my_test                       [.] operator new(unsigned long)@plt
            |
            --- operator new(unsigned long)@plt

     0.20%  my_test  my_test                       [.] main
     0.19%  my_test  [kernel.kallsyms]             [k] apic_timer_interrupt
     0.16%  my_test  [kernel.kallsyms]             [k] _spin_lock
     and so on ...
</code></pre>

<p>So at this point you know where your program spends time.</p>

<p>And this is main.cpp for the test:</p>

<pre><code>#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;time.h&gt;

time_t f1(time_t time_value)
{
  for (int j =0; j &lt; 10; ++j) {
    ++time_value;
    if (j%5 == 0) {
      double *p = new double;
      delete p;
    }
  }
  return time_value;
}

time_t f2(time_t time_value)
{
  for (int j =0; j &lt; 40; ++j) {
    ++time_value;
  }
  time_value=f1(time_value);
  return time_value;
}

time_t process_request(time_t time_value)
{

  for (int j =0; j &lt; 10; ++j) {
    int *p = new int;
    delete p;
    for (int m =0; m &lt; 10; ++m) {
      ++time_value;
    }
  }
  for (int i =0; i &lt; 10; ++i) {
    time_value=f1(time_value);
    time_value=f2(time_value);
  }
  return time_value;
}

int main(int argc, char* argv2[])
{
  int number_loops = argc &gt; 1 ? atoi(argv2[1]) : 1;
  time_t time_value = time(0);
  printf("number loops %d\n", number_loops);
  printf("time_value: %d\n", time_value );

  for (int i =0; i &lt; number_loops; ++i) {
    time_value = process_request(time_value);
  }
  printf("time_value: %ld\n", time_value );
  return 0;
}
</code></pre>
    </div></div></div></div><div id="solution4" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 4</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiler">profiler</a><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/gprof">gprof</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Try <a href="http://oprofile.sourceforge.net/about/" rel="noreferrer">OProfile</a>. It is a much better tool for profiling your code. I would also suggest Intel <a href="http://en.wikipedia.org/wiki/VTune" rel="noreferrer">VTune</a>.</p>

<p>The two tools above can narrow down time spent in a particular line of code, annotate your code, show assembly and how much particular instruction takes.  Beside time metric, you can also query specific counters, i.e. cache hits, etc.</p>

<p>Unlike gprof, you can profile any process/binary running on your system using either of the two.</p>
    </div></div></div></div><div id="solution5" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 5</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiler">profiler</a><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/gprof">gprof</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p><a href="http://code.google.com/p/google-perftools/">Google performance tools</a> include a simple to use profiler. CPU as well as heap profiler is available.</p>
    </div></div></div></div><div id="solution6" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 6</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiler">profiler</a><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/gprof">gprof</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p>Take a look at <a href="http://sysprof.com/" rel="noreferrer">Sysprof</a>.</p>

<p>Your distribution may have it already.</p>
    </div></div></div></div><div id="solution7" class="flex mt-5 answer items-center justify-center py-5"><div class="rounded-xl solution-inner border md:px-10 md:py-10 px-2 py-10 shadow-md bg-white"><div style="display:flex;justify-content:space-between"><h4 class="text-4xl font-semibold mb-5">Solution 7</h4><div class="tags-wrap h-max space-x-8"><div class="tags"><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/profiler">profiler</a><a class="rounded-2xl border bg-neutral-100 px-3 py-1 text-sm font-semibold whitespace-nowrap" href="/questions/tag/gprof">gprof</a></div></div></div><div class=" items-center justify-between"><div class=" space-x-4 md:space-x-8">
<p><a href="http://lttng.org/" rel="nofollow">http://lttng.org/</a> if you want a high performance tracer</p>
    </div></div></div></div></div></div><div class="widget"><a href="/questions/parameterize-an-sql-in-clause-1657387536064">Parameterize an SQL IN clause</a><a href="/questions/transitions-on-the-css-display-property-1657387761601">Transitions on the CSS display property</a><a href="/questions/what-does-%22list-comprehension%22-and-similar-mean-how-does-it-work-and-how-can-i-use-it-1657387996940">What does &quot;list comprehension&quot; and similar mean? How does it work and how can I use it?</a><a href="/questions/truth-value-of-a-series-is-ambiguous.-use-a.empty-a.bool()-a.item()-a.any()-or-a.all()-1657387724259">Truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()</a><a href="/questions/using-global-variables-in-a-function-1657384796002">Using global variables in a function</a><a href="/questions/javascript-infamous-loop-issue-duplicate-1657387498530">Javascript infamous Loop issue? [duplicate]</a><a href="/questions/why-shouldn&#x27;t-i-use-mysql_*-functions-in-php-1657384260352">Why shouldn&#x27;t I use mysql_* functions in PHP?</a><a href="/questions/servlet-returns-%22http-status-404-the-requested-resource-(servlet)-is-not-available%22-1657384850661">Servlet returns &quot;HTTP Status 404 The requested resource (/servlet) is not available&quot;</a><a href="/questions/pandas-conditional-creation-of-a-seriesdataframe-column-1657387461003">Pandas conditional creation of a series/dataframe column</a><a href="/questions/can-i-mix-mysql-apis-in-php-1657384597444">Can I mix MySQL APIs in PHP?</a><a href="/questions/alternatives-to-gprof-closed-1657388336554">Alternatives to gprof [closed]</a><a href="/questions/when-to-use-virtual-destructors-1657388152135">When to use virtual destructors?</a><a href="/questions/use-dynamic-variable-names-in-javascript-1657388468075">Use dynamic variable names in JavaScript</a><a href="/questions/mysql-pivot-row-into-dynamic-number-of-columns-1657387981930">MySQL pivot row into dynamic number of columns</a><a href="/questions/how-does-the-java-&#x27;for-each&#x27;-loop-work-1657388423749">How does the Java &#x27;for each&#x27; loop work?</a><a href="/questions/how-to-filter-object-array-based-on-attributes-1657388211247">How to filter object array based on attributes?</a><a href="/questions/understanding-slicing-1657384397680">Understanding slicing</a><a href="/questions/what-does-it-mean-to-%22program-to-an-interface%22-1657384671665">What does it mean to &quot;program to an interface&quot;?</a><a href="/questions/why-not-use-double-or-float-to-represent-currency-1657387417964">Why not use Double or Float to represent currency?</a><a href="/questions/how-to-create-a-mysql-hierarchical-recursive-query-1657387662163">How to create a MySQL hierarchical recursive query?</a></div></div><span class="cursor-pointer text-lg p-2" style="position:fixed;bottom:20px;left:20px;background:#000;z-index:2000;color:white">Go go top</span></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"data":{"answer":["\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;gprof\u0026lt;/strong\u0026gt; \u0026lt;a href=\u0026quot;http://docs.freebsd.org/44doc/psd/18.gprof/paper.pdf\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;(read the paper)\u0026lt;/a\u0026gt; exists for historical reasons. \nIf you think it will help you find performance problems, it was never advertised as such.\nHere\u0026apos;s what the paper says:\u0026lt;/p\u0026gt;\n\n\u0026lt;blockquote\u0026gt;\n  \u0026lt;p\u0026gt;The prole can be used to compare and assess the costs of\n  various implementations.\u0026lt;/p\u0026gt;\n\u0026lt;/blockquote\u0026gt;\n\n\u0026lt;p\u0026gt;It does not say it can be used to \u0026lt;em\u0026gt;identify\u0026lt;/em\u0026gt; the various implementations to be assessed, although it does \u0026lt;em\u0026gt;imply\u0026lt;/em\u0026gt; that it could, under special circumstances:\u0026lt;/p\u0026gt;\n\n\u0026lt;blockquote\u0026gt;\n  \u0026lt;p\u0026gt;especially if small portions of the program are found to dominate its\n  execution time.\u0026lt;/p\u0026gt;\n\u0026lt;/blockquote\u0026gt;\n\n\u0026lt;p\u0026gt;What about problems that are not so localized?\nDo those not matter?\nDon\u0026apos;t place expectations on \u0026lt;strong\u0026gt;gprof\u0026lt;/strong\u0026gt; that were never claimed for it.\nIt is \u0026lt;em\u0026gt;only\u0026lt;/em\u0026gt; a measurement tool, and only of CPU-bound operations.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;https://stackoverflow.com/questions/375913/what-can-i-use-to-profile-c-code-in-linux/378024#378024\u0026quot;\u0026gt;Try this instead.\u0026lt;/a\u0026gt;\u0026lt;br\u0026gt;\n\u0026lt;a href=\u0026quot;https://stackoverflow.com/questions/926266/performance-optimization-strategies-of-last-resort/927773#927773\u0026quot;\u0026gt;Here\u0026apos;s an example of a 44x speedup.\u0026lt;/a\u0026gt;\u0026lt;br\u0026gt;\n\u0026lt;a href=\u0026quot;https://scicomp.stackexchange.com/a/1870/1262\u0026quot;\u0026gt;Here\u0026apos;s a 730x speedup.\u0026lt;/a\u0026gt;\u0026lt;br\u0026gt;\n\u0026lt;a href=\u0026quot;http://youtu.be/xPg3sRpdW1U\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;Here\u0026apos;s an 8-minute video demonstration.\u0026lt;/a\u0026gt;\u0026lt;br\u0026gt;\n\u0026lt;a href=\u0026quot;https://scicomp.stackexchange.com/a/2719/1262\u0026quot;\u0026gt;Here\u0026apos;s an explanation of the statistics.\u0026lt;/a\u0026gt;\u0026lt;br\u0026gt;\n\u0026lt;a href=\u0026quot;https://stackoverflow.com/a/18217639/23771\u0026quot;\u0026gt;Here\u0026apos;s an answer to critiques.\u0026lt;/a\u0026gt;  \u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;There\u0026apos;s a simple observation about programs. In a given execution, every instruction is responsible for some fraction of the overall time (especially \u0026lt;code\u0026gt;call\u0026lt;/code\u0026gt; instructions), in the sense that if it were not there, the time would not be spent. During that time, the instruction is on the stack **. When that is understood, you can see that -  \u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;gprof\u0026lt;/strong\u0026gt; embodies certain myths about performance, such as:\u0026lt;/p\u0026gt;\n\n\u0026lt;ol\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;that program counter sampling is useful.\u0026lt;/em\u0026gt;\u0026lt;br\u0026gt;\nIt is only useful if you have an unnecessary hotspot bottleneck such as a bubble sort of a big array of scalar values. As soon as you, for example, change it into a sort using string-compare, it is still a bottleneck, but program counter sampling will not see it because now the hotspot is in string-compare. On the other hand if it were to sample the \u0026lt;strong\u0026gt;extended\u0026lt;/strong\u0026gt; program counter (the call stack), the point at which the string-compare is called, the sort loop, is clearly displayed. \u0026lt;em\u0026gt;In fact, \u0026lt;strong\u0026gt;gprof\u0026lt;/strong\u0026gt; was an attempt to remedy the limitations of pc-only sampling.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;that timing functions is more important than capturing time-consuming lines of code.\u0026lt;/em\u0026gt;\u0026lt;br\u0026gt;\nThe reason for that myth is that \u0026lt;strong\u0026gt;gprof\u0026lt;/strong\u0026gt; was not able to capture stack samples, so instead it times functions, counts their invocations, and tries to capture the call graph. However, once a costly function is identified, you still need to look inside it for the lines that are responsible for the time. If there were stack samples you would not need to look, those lines would be on the samples. (A typical function might have 100 - 1000 instructions. A function \u0026lt;em\u0026gt;call\u0026lt;/em\u0026gt; is 1 instruction, so something that locates costly calls is 2-3 orders of magnitude more precise.)\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;that the call graph is important.\u0026lt;/em\u0026gt;\u0026lt;br\u0026gt;\nWhat you need to know about a program is not \u0026lt;strong\u0026gt;where\u0026lt;/strong\u0026gt; it spends its time, but \u0026lt;strong\u0026gt;why\u0026lt;/strong\u0026gt;. When it is spending time in a function, every line of code on the stack gives one link in the chain of reasoning of why it is there. If you can only see part of the stack, you can only see part of the reason why, so you can\u0026apos;t tell for sure if that time is actually necessary.\nWhat does the call graph tell you? Each arc tells you that some function A was in the process of calling some function B for some fraction of the time. Even if A has only one such line of code calling B, that line only gives a small part of the reason why. If you are lucky enough, maybe that line has a poor reason. Usually, you need to see multiple simultaneous lines to find a poor reason if it is there. If A calls B in more than one place, then it tells you even less.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;that recursion is a tricky confusing issue.\u0026lt;/em\u0026gt;\u0026lt;br\u0026gt;\nThat is only because \u0026lt;strong\u0026gt;gprof\u0026lt;/strong\u0026gt; and other profilers perceive a need to generate a call-graph and then attribute times to the nodes. If one has samples of the stack, the time-cost of each line of code that appears on samples is a very simple number - the fraction of samples it is on. If there is recursion, then a given line can appear more than once on a sample. \n\u0026lt;em\u0026gt;No matter.\u0026lt;/em\u0026gt; Suppose samples are taken every N ms, and the line appears on F% of them (singly or not). If that line can be made to take no time (such as by deleting it or branching around it), then those samples would \u0026lt;em\u0026gt;disappear\u0026lt;/em\u0026gt;, and the time would be reduced by F%.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;that accuracy of time measurement (and therefore a large number of samples) is important.\u0026lt;/em\u0026gt;\u0026lt;br\u0026gt;\nThink about it for a second. If a line of code is on 3 samples out of five, then if you could shoot it out like a light bulb, that is roughly 60% less time that would be used. Now, you know that if you had taken a different 5 samples, you might have only seen it 2 times, or as many as 4. So that 60% measurement is more like a general range from 40% to 80%. If it were only 40%, would you say the problem is not worth fixing? So what\u0026apos;s the point of time accuracy, when what you really want is to \u0026lt;strong\u0026gt;find the problems\u0026lt;/strong\u0026gt;?\n500 or 5000 samples would have measured the problem with greater precision, but would not have found it any more accurately.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;that counting of statement or function invocations is useful.\u0026lt;/em\u0026gt;\u0026lt;br\u0026gt;\nSuppose you know a function has been called 1000 times. Can you tell from that what fraction of time it costs? You also need to know how long it takes to run, on average, multiply it by the count, and divide by the total time. The average invocation time could vary from nanoseconds to seconds, so the count alone doesn\u0026apos;t tell much. If there are stack samples, the cost of a routine or of any statement is just the fraction of samples it is on. That fraction of time is what could in principle be saved overall if the routine or statement could be made to take no time, so that is what has the most direct relationship to performance.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;that samples need not be taken when blocked\u0026lt;/em\u0026gt;\u0026lt;br\u0026gt;\nThe reasons for this myth are twofold: 1) that PC sampling is meaningless when the program is waiting, and 2) the preoccupation with accuracy of timing. However, for (1) the program may very well be waiting for something that it asked for, such as file I/O, which you \u0026lt;em\u0026gt;need to know\u0026lt;/em\u0026gt;, and which stack samples reveal. (Obviously you want to exclude samples while waiting for user input.) For (2) if the program is waiting simply because of competition with other processes, that presumably happens in a fairly random way while it\u0026apos;s running.\nSo while the program may be taking longer, that will not have a large effect on the statistic that matters, the percentage of time that statements are on the stack.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;that \u0026quot;self time\u0026quot; matters\u0026lt;/em\u0026gt;\u0026lt;br\u0026gt;\nSelf time only makes sense if you are measuring at the function level, not line level, and you think you need help in discerning if the function time goes into purely local computation versus in called routines. If summarizing at the line level, a line represents self time if it is at the end of the stack, otherwise it represents inclusive time. Either way, what it costs is the percentage of stack samples it is on, so that locates it for you in either case.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;that samples have to be taken at high frequency\u0026lt;/em\u0026gt;\u0026lt;br\u0026gt;\nThis comes from the idea that a performance problem may be fast-acting, and that samples have to be frequent in order to hit it. But, if the problem is costing, 20%, say, out of a total running time of 10 sec (or whatever), then each sample in that total time will have a 20% chance of hitting it, no matter if the problem occurs in a single piece like this\u0026lt;br\u0026gt;\n\u0026lt;code\u0026gt;.....XXXXXXXX...........................\u0026lt;/code\u0026gt;\u0026lt;br\u0026gt;\n\u0026lt;code\u0026gt;.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^\u0026lt;/code\u0026gt; (20 samples, 4 hits)\u0026lt;br\u0026gt;\nor in many small pieces like this\u0026lt;br\u0026gt;\n\u0026lt;code\u0026gt;X...X...X.X..X.........X.....X....X.....\u0026lt;/code\u0026gt;\u0026lt;br\u0026gt;\n\u0026lt;code\u0026gt;.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^.^\u0026lt;/code\u0026gt; (20 samples, 3 hits)\u0026lt;br\u0026gt;\nEither way, the number of hits will average about 1 in 5, no matter how many samples are taken, or how few. (Average = 20 * 0.2 = 4. Standard deviation = +/- sqrt(20 * 0.2 * 0.8) = 1.8.)\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;that you are trying to find \u0026lt;strong\u0026gt;the\u0026lt;/strong\u0026gt; bottleneck\u0026lt;/em\u0026gt;\u0026lt;br\u0026gt;\nas if there were only one. Consider the following execution timeline: \u0026lt;code\u0026gt;vxvWvzvWvxvWvYvWvxvWv.vWvxvWvYvW\u0026lt;/code\u0026gt;\u0026lt;br\u0026gt;\nIt consists of real useful work, represented by \u0026lt;code\u0026gt;.\u0026lt;/code\u0026gt;. There are performance problems \u0026lt;code\u0026gt;vWxYz\u0026lt;/code\u0026gt; taking 1/2, 1/4, 1/8, 1/16, 1/32 of the time, respectively. Sampling finds \u0026lt;code\u0026gt;v\u0026lt;/code\u0026gt; easily. It is removed, leaving\u0026lt;br\u0026gt;\n\u0026lt;code\u0026gt;xWzWxWYWxW.WxWYW\u0026lt;/code\u0026gt;\u0026lt;br\u0026gt;\nNow the program takes half as long to run, and now \u0026lt;code\u0026gt;W\u0026lt;/code\u0026gt; takes half the time, and is found easily. It is removed, leaving\u0026lt;br\u0026gt;\n\u0026lt;code\u0026gt;xzxYx.xY\u0026lt;/code\u0026gt;\u0026lt;br\u0026gt;\nThis process continues, each time removing the biggest, by percentage, performance problem, until nothing to remove can be found. Now the only thing executed is \u0026lt;code\u0026gt;.\u0026lt;/code\u0026gt;, which executes in 1/32 of the time used by the original program. This is the \u0026lt;em\u0026gt;magnification effect\u0026lt;/em\u0026gt;, by which removing any problem makes the remainder larger, by percent, because the denominator is reduced.\u0026lt;br\u0026gt;\nAnother crucial point is that \u0026lt;em\u0026gt;every single problem must be found\u0026lt;/em\u0026gt; - missing none of the 5. Any problem not found and fixed severely reduces the final speedup ratio. Just finding some, but not all, is not \u0026quot;good enough\u0026quot;.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\n\u0026lt;/ol\u0026gt;\n\n\u0026lt;p\u0026gt;ADDED: I would just like to point out one reason why \u0026lt;em\u0026gt;gprof\u0026lt;/em\u0026gt; is popular - it is being taught,\npresumably because it\u0026apos;s free, easy to teach, and it\u0026apos;s been around a long time.\nA quick Google search locates some academic institutions that teach it (or appear to):\u0026lt;/p\u0026gt;\n\n\u0026lt;blockquote\u0026gt;\n  \u0026lt;p\u0026gt;berkeley bu clemson\n  colorado duke earlham fsu indiana mit msu\n  ncsa.illinois ncsu nyu ou princeton psu\n  stanford ucsd umd umich utah utexas utk wustl\u0026lt;/p\u0026gt;\n\u0026lt;/blockquote\u0026gt;\n\n\u0026lt;p\u0026gt;** With the exception of other ways of requesting work to be done, that don\u0026apos;t leave a trace telling \u0026lt;em\u0026gt;why\u0026lt;/em\u0026gt;, such as by message posting.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Valgrind\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;Valgrind\u0026lt;/a\u0026gt; has an instruction-count profiler with a very nice visualizer called \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/Valgrind#Tools\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;KCacheGrind\u0026lt;/a\u0026gt;.  As Mike Dunlavey recommends, Valgrind counts the fraction of instructions for which a procedure is live on the stack, although I\u0026apos;m sorry to say it appears to become confused in the presence of mutual recursion.  But the visualizer is very nice and light years ahead of \u0026lt;code\u0026gt;gprof\u0026lt;/code\u0026gt;.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Since I did\u0026apos;t see here anything about \u0026lt;strong\u0026gt;\u0026lt;code\u0026gt;perf\u0026lt;/code\u0026gt;\u0026lt;/strong\u0026gt; which is a relatively new tool for profiling the kernel and user applications on Linux I decided to add this information.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;First of all - this is a tutorial about \u0026lt;a href=\u0026quot;https://perf.wiki.kernel.org/index.php/Tutorial\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;Linux profiling with \u0026lt;code\u0026gt;perf\u0026lt;/code\u0026gt; \u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;You can use \u0026lt;code\u0026gt;perf\u0026lt;/code\u0026gt; if your Linux Kernel is greater than 2.6.32 or \u0026lt;code\u0026gt;oprofile\u0026lt;/code\u0026gt; if it is older. Both programs don\u0026apos;t require from you to instrument your program (like \u0026lt;code\u0026gt;gprof\u0026lt;/code\u0026gt; requires). However in order to get call graph correctly in \u0026lt;code\u0026gt;perf\u0026lt;/code\u0026gt; you need to build you program with \u0026lt;code\u0026gt;-fno-omit-frame-pointer\u0026lt;/code\u0026gt;. For example: \u0026lt;code\u0026gt;g++ -fno-omit-frame-pointer -O2 main.cpp\u0026lt;/code\u0026gt;.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;You can see \u0026quot;live\u0026quot; analysis of your application with \u0026lt;code\u0026gt;perf top\u0026lt;/code\u0026gt;: \u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;sudo perf top -p `pidof a.out` -K\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;Or you can record performance data of a running application and analyze them after that:\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;1) To record performance data:\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;perf record -p `pidof a.out`\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;or  to record for 10 secs:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;perf record -p `pidof a.out` sleep 10\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;or to record with call graph ()\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;perf record -g -p `pidof a.out` \n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;2) To analyze the recorded data\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;perf report --stdio\nperf report --stdio --sort=dso -g none\nperf report --stdio -g none\nperf report --stdio -g\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;Or you can record performace data of a application and analyze them after that just by launching the application in this way and waiting for it to exit:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;perf record ./a.out\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;This is an example of profiling a test program\u0026lt;/strong\u0026gt; \u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;The test program is in file main.cpp (I will put main.cpp at the bottom of the message):\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;I compile it in this way:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;g++ -m64 -fno-omit-frame-pointer -g main.cpp -L.  -ltcmalloc_minimal -o my_test\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;I use \u0026lt;code\u0026gt;libmalloc_minimial.so\u0026lt;/code\u0026gt; since it is compiled with \u0026lt;code\u0026gt;-fno-omit-frame-pointer\u0026lt;/code\u0026gt; while libc malloc seems to be compiled without this option.\nThen I run my test program\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;./my_test 100000000 \n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;Then I record performance data of a running process:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;perf record -g  -p `pidof my_test` -o ./my_test.perf.data sleep 30\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;Then I analyze load per module:\u0026lt;/p\u0026gt;\n\n\u0026lt;blockquote\u0026gt;\n  \u0026lt;p\u0026gt;perf report --stdio  -g none --sort comm,dso  -i ./my_test.perf.data\u0026lt;/p\u0026gt;\n\u0026lt;/blockquote\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;# Overhead  Command                 Shared Object\n# ........  .......  ............................\n#\n    70.06%  my_test  my_test\n    28.33%  my_test  libtcmalloc_minimal.so.0.1.0\n     1.61%  my_test  [kernel.kallsyms]\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;Then load per function is analyzed:\u0026lt;/p\u0026gt;\n\n\u0026lt;blockquote\u0026gt;\n  \u0026lt;p\u0026gt;perf report --stdio  -g none  -i ./my_test.perf.data | c++filt\u0026lt;/p\u0026gt;\n\u0026lt;/blockquote\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;# Overhead  Command                 Shared Object                       Symbol\n# ........  .......  ............................  ...........................\n#\n    29.30%  my_test  my_test                       [.] f2(long)\n    29.14%  my_test  my_test                       [.] f1(long)\n    15.17%  my_test  libtcmalloc_minimal.so.0.1.0  [.] operator new(unsigned long)\n    13.16%  my_test  libtcmalloc_minimal.so.0.1.0  [.] operator delete(void*)\n     9.44%  my_test  my_test                       [.] process_request(long)\n     1.01%  my_test  my_test                       [.] operator delete(void*)@plt\n     0.97%  my_test  my_test                       [.] operator new(unsigned long)@plt\n     0.20%  my_test  my_test                       [.] main\n     0.19%  my_test  [kernel.kallsyms]             [k] apic_timer_interrupt\n     0.16%  my_test  [kernel.kallsyms]             [k] _spin_lock\n     0.13%  my_test  [kernel.kallsyms]             [k] native_write_msr_safe\n\n     and so on ...\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;Then call chains are analyzed:\u0026lt;/p\u0026gt;\n\n\u0026lt;blockquote\u0026gt;\n  \u0026lt;p\u0026gt;perf report --stdio  -g graph  -i ./my_test.perf.data | c++filt\u0026lt;/p\u0026gt;\n\u0026lt;/blockquote\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;# Overhead  Command                 Shared Object                       Symbol\n# ........  .......  ............................  ...........................\n#\n    29.30%  my_test  my_test                       [.] f2(long)\n            |\n            --- f2(long)\n               |\n                --29.01%-- process_request(long)\n                          main\n                          __libc_start_main\n\n    29.14%  my_test  my_test                       [.] f1(long)\n            |\n            --- f1(long)\n               |\n               |--15.05%-- process_request(long)\n               |          main\n               |          __libc_start_main\n               |\n                --13.79%-- f2(long)\n                          process_request(long)\n                          main\n                          __libc_start_main\n\n    15.17%  my_test  libtcmalloc_minimal.so.0.1.0  [.] operator new(unsigned long)\n            |\n            --- operator new(unsigned long)\n               |\n               |--11.44%-- f1(long)\n               |          |\n               |          |--5.75%-- process_request(long)\n               |          |          main\n               |          |          __libc_start_main\n               |          |\n               |           --5.69%-- f2(long)\n               |                     process_request(long)\n               |                     main\n               |                     __libc_start_main\n               |\n                --3.01%-- process_request(long)\n                          main\n                          __libc_start_main\n\n    13.16%  my_test  libtcmalloc_minimal.so.0.1.0  [.] operator delete(void*)\n            |\n            --- operator delete(void*)\n               |\n               |--9.13%-- f1(long)\n               |          |\n               |          |--4.63%-- f2(long)\n               |          |          process_request(long)\n               |          |          main\n               |          |          __libc_start_main\n               |          |\n               |           --4.51%-- process_request(long)\n               |                     main\n               |                     __libc_start_main\n               |\n               |--3.05%-- process_request(long)\n               |          main\n               |          __libc_start_main\n               |\n                --0.80%-- f2(long)\n                          process_request(long)\n                          main\n                          __libc_start_main\n\n     9.44%  my_test  my_test                       [.] process_request(long)\n            |\n            --- process_request(long)\n               |\n                --9.39%-- main\n                          __libc_start_main\n\n     1.01%  my_test  my_test                       [.] operator delete(void*)@plt\n            |\n            --- operator delete(void*)@plt\n\n     0.97%  my_test  my_test                       [.] operator new(unsigned long)@plt\n            |\n            --- operator new(unsigned long)@plt\n\n     0.20%  my_test  my_test                       [.] main\n     0.19%  my_test  [kernel.kallsyms]             [k] apic_timer_interrupt\n     0.16%  my_test  [kernel.kallsyms]             [k] _spin_lock\n     and so on ...\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n\n\u0026lt;p\u0026gt;So at this point you know where your program spends time.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;And this is main.cpp for the test:\u0026lt;/p\u0026gt;\n\n\u0026lt;pre\u0026gt;\u0026lt;code\u0026gt;#include \u0026amp;lt;stdio.h\u0026amp;gt;\n#include \u0026amp;lt;stdlib.h\u0026amp;gt;\n#include \u0026amp;lt;time.h\u0026amp;gt;\n\ntime_t f1(time_t time_value)\n{\n  for (int j =0; j \u0026amp;lt; 10; ++j) {\n    ++time_value;\n    if (j%5 == 0) {\n      double *p = new double;\n      delete p;\n    }\n  }\n  return time_value;\n}\n\ntime_t f2(time_t time_value)\n{\n  for (int j =0; j \u0026amp;lt; 40; ++j) {\n    ++time_value;\n  }\n  time_value=f1(time_value);\n  return time_value;\n}\n\ntime_t process_request(time_t time_value)\n{\n\n  for (int j =0; j \u0026amp;lt; 10; ++j) {\n    int *p = new int;\n    delete p;\n    for (int m =0; m \u0026amp;lt; 10; ++m) {\n      ++time_value;\n    }\n  }\n  for (int i =0; i \u0026amp;lt; 10; ++i) {\n    time_value=f1(time_value);\n    time_value=f2(time_value);\n  }\n  return time_value;\n}\n\nint main(int argc, char* argv2[])\n{\n  int number_loops = argc \u0026amp;gt; 1 ? atoi(argv2[1]) : 1;\n  time_t time_value = time(0);\n  printf(\u0026quot;number loops %d\\n\u0026quot;, number_loops);\n  printf(\u0026quot;time_value: %d\\n\u0026quot;, time_value );\n\n  for (int i =0; i \u0026amp;lt; number_loops; ++i) {\n    time_value = process_request(time_value);\n  }\n  printf(\u0026quot;time_value: %ld\\n\u0026quot;, time_value );\n  return 0;\n}\n\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Try \u0026lt;a href=\u0026quot;http://oprofile.sourceforge.net/about/\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;OProfile\u0026lt;/a\u0026gt;. It is a much better tool for profiling your code. I would also suggest Intel \u0026lt;a href=\u0026quot;http://en.wikipedia.org/wiki/VTune\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;VTune\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;The two tools above can narrow down time spent in a particular line of code, annotate your code, show assembly and how much particular instruction takes.  Beside time metric, you can also query specific counters, i.e. cache hits, etc.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Unlike gprof, you can profile any process/binary running on your system using either of the two.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;http://code.google.com/p/google-perftools/\u0026quot;\u0026gt;Google performance tools\u0026lt;/a\u0026gt; include a simple to use profiler. CPU as well as heap profiler is available.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;Take a look at \u0026lt;a href=\u0026quot;http://sysprof.com/\u0026quot; rel=\u0026quot;noreferrer\u0026quot;\u0026gt;Sysprof\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\n\n\u0026lt;p\u0026gt;Your distribution may have it already.\u0026lt;/p\u0026gt;\n    ","\n\u0026lt;p\u0026gt;\u0026lt;a href=\u0026quot;http://lttng.org/\u0026quot; rel=\u0026quot;nofollow\u0026quot;\u0026gt;http://lttng.org/\u0026lt;/a\u0026gt; if you want a high performance tracer\u0026lt;/p\u0026gt;\n    "],"id":534,"title":"Alternatives to gprof [closed]","content":"\n                    \u0026lt;div\u0026gt;\n            \u0026lt;aside class=\u0026quot;s-notice s-notice__info post-notice js-post-notice mb16\u0026quot; role=\u0026quot;status\u0026quot;\u0026gt;\n        \u0026lt;div class=\u0026quot;d-flex fd-column fw-nowrap\u0026quot;\u0026gt;\n            \u0026lt;div class=\u0026quot;d-flex fw-nowrap\u0026quot;\u0026gt;\n                    \u0026lt;div class=\u0026quot;flex--item mr8\u0026quot;\u0026gt;\n                        \u0026lt;svg aria-hidden=\u0026quot;true\u0026quot; class=\u0026quot;svg-icon iconLightbulb\u0026quot; width=\u0026quot;18\u0026quot; height=\u0026quot;18\u0026quot; viewBox=\u0026quot;0 0 18 18\u0026quot;\u0026gt;\u0026lt;path d=\u0026quot;M15 6.38A6.48 6.48 0 0 0 7.78.04h-.02A6.49 6.49 0 0 0 2.05 5.6a6.31 6.31 0 0 0 2.39 5.75c.49.39.76.93.76 1.5v.24c0 1.07.89 1.9 1.92 1.9h2.75c1.04 0 1.92-.83 1.92-1.9v-.2c0-.6.26-1.15.7-1.48A6.32 6.32 0 0 0 15 6.37ZM4.03 5.85A4.49 4.49 0 0 1 8 2.02a4.48 4.48 0 0 1 5 4.36 4.3 4.3 0 0 1-1.72 3.44c-.98.74-1.5 1.9-1.5 3.08v.1H7.2v-.14c0-1.23-.6-2.34-1.53-3.07a4.32 4.32 0 0 1-1.64-3.94ZM10 18a1 1 0 0 0 0-2H7a1 1 0 1 0 0 2h3Z\u0026quot;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;/svg\u0026gt;\n                    \u0026lt;/div\u0026gt;\n                \u0026lt;div class=\u0026quot;flex--item wmn0 fl1 lh-lg\u0026quot;\u0026gt;\n                    \u0026lt;div class=\u0026quot;flex--item fl1 lh-lg\u0026quot;\u0026gt;\n                        As it currently stands, this question is not a good fit for our Q\u0026amp;amp;A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, \u0026lt;a href=\u0026quot;/help/reopen-questions\u0026quot;\u0026gt;visit the help center\u0026lt;/a\u0026gt; for guidance.\n                        \n                    \u0026lt;/div\u0026gt;\n                \u0026lt;/div\u0026gt;\n            \u0026lt;/div\u0026gt;\n                \u0026lt;div class=\u0026quot;flex--item mb0 mt8\u0026quot;\u0026gt;Closed \u0026lt;span title=\u0026quot;2013-01-05 06:46:45Z\u0026quot; class=\u0026quot;relativetime\u0026quot;\u0026gt;9 years ago\u0026lt;/span\u0026gt;.\u0026lt;/div\u0026gt;\n        \u0026lt;/div\u0026gt;\n\u0026lt;/aside\u0026gt;\n\n    \u0026lt;/div\u0026gt;\n\n\u0026lt;p\u0026gt;What other programs do the same thing as gprof?\u0026lt;/p\u0026gt;\n    ","slug":"alternatives-to-gprof-closed-1657388336554","postType":"QUESTION","createdAt":"2022-07-09T17:38:56.000Z","updatedAt":"2022-07-09T17:38:56.000Z","tags":[{"id":2619,"name":"profiler","slug":"profiler","createdAt":"2022-07-09T17:38:56.000Z","updatedAt":"2022-07-09T17:38:56.000Z","Questions_Tags":{"questionId":534,"tagId":2619}},{"id":2620,"name":"gprof","slug":"gprof","createdAt":"2022-07-09T17:38:56.000Z","updatedAt":"2022-07-09T17:38:56.000Z","Questions_Tags":{"questionId":534,"tagId":2620}}],"relatedQuestions":[{"title":"Alternatives to gprof [closed]","slug":"alternatives-to-gprof-closed-1657388336554","tags":[{"name":"profiler","Questions_Tags":{"questionId":534,"tagId":2619}},{"name":"gprof","Questions_Tags":{"questionId":534,"tagId":2620}}]}]},"randomQuestions":[{"title":"Parameterize an SQL IN clause","slug":"parameterize-an-sql-in-clause-1657387536064"},{"title":"Transitions on the CSS display property","slug":"transitions-on-the-css-display-property-1657387761601"},{"title":"What does \"list comprehension\" and similar mean? How does it work and how can I use it?","slug":"what-does-\"list-comprehension\"-and-similar-mean-how-does-it-work-and-how-can-i-use-it-1657387996940"},{"title":"Truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()","slug":"truth-value-of-a-series-is-ambiguous.-use-a.empty-a.bool()-a.item()-a.any()-or-a.all()-1657387724259"},{"title":"Using global variables in a function","slug":"using-global-variables-in-a-function-1657384796002"},{"title":"Javascript infamous Loop issue? [duplicate]","slug":"javascript-infamous-loop-issue-duplicate-1657387498530"},{"title":"Why shouldn't I use mysql_* functions in PHP?","slug":"why-shouldn't-i-use-mysql_*-functions-in-php-1657384260352"},{"title":"Servlet returns \"HTTP Status 404 The requested resource (/servlet) is not available\"","slug":"servlet-returns-\"http-status-404-the-requested-resource-(servlet)-is-not-available\"-1657384850661"},{"title":"Pandas conditional creation of a series/dataframe column","slug":"pandas-conditional-creation-of-a-seriesdataframe-column-1657387461003"},{"title":"Can I mix MySQL APIs in PHP?","slug":"can-i-mix-mysql-apis-in-php-1657384597444"},{"title":"Alternatives to gprof [closed]","slug":"alternatives-to-gprof-closed-1657388336554"},{"title":"When to use virtual destructors?","slug":"when-to-use-virtual-destructors-1657388152135"},{"title":"Use dynamic variable names in JavaScript","slug":"use-dynamic-variable-names-in-javascript-1657388468075"},{"title":"MySQL pivot row into dynamic number of columns","slug":"mysql-pivot-row-into-dynamic-number-of-columns-1657387981930"},{"title":"How does the Java 'for each' loop work?","slug":"how-does-the-java-'for-each'-loop-work-1657388423749"},{"title":"How to filter object array based on attributes?","slug":"how-to-filter-object-array-based-on-attributes-1657388211247"},{"title":"Understanding slicing","slug":"understanding-slicing-1657384397680"},{"title":"What does it mean to \"program to an interface\"?","slug":"what-does-it-mean-to-\"program-to-an-interface\"-1657384671665"},{"title":"Why not use Double or Float to represent currency?","slug":"why-not-use-double-or-float-to-represent-currency-1657387417964"},{"title":"How to create a MySQL hierarchical recursive query?","slug":"how-to-create-a-mysql-hierarchical-recursive-query-1657387662163"}]},"__N_SSG":true},"page":"/questions/[slug]","query":{"slug":"alternatives-to-gprof-closed-1657388336554"},"buildId":"BnkbnjkWYxefPXjTJLVVv","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>